{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfa736e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import normalize\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30aca3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_joke_df = pd.read_csv(r'data\\recsys-in-practice\\train_joke_df.csv')\n",
    "test_joke_df_nofactrating = pd.read_csv(r'data\\recsys-in-practice\\test_joke_df_nofactrating.csv')\n",
    "sample_submission = pd.read_csv(r'data\\recsys-in-practice\\sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb7ea2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>JID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18029</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3298</td>\n",
       "      <td>64</td>\n",
       "      <td>-4.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3366</td>\n",
       "      <td>58</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12735</td>\n",
       "      <td>92</td>\n",
       "      <td>3.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11365</td>\n",
       "      <td>38</td>\n",
       "      <td>-6.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448359</th>\n",
       "      <td>22604</td>\n",
       "      <td>26</td>\n",
       "      <td>2.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448360</th>\n",
       "      <td>22255</td>\n",
       "      <td>36</td>\n",
       "      <td>-1.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448361</th>\n",
       "      <td>21056</td>\n",
       "      <td>40</td>\n",
       "      <td>-9.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448362</th>\n",
       "      <td>12328</td>\n",
       "      <td>97</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448363</th>\n",
       "      <td>11528</td>\n",
       "      <td>3</td>\n",
       "      <td>9.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1448364 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           UID  JID  Rating\n",
       "0        18029    6   -1.26\n",
       "1         3298   64   -4.17\n",
       "2         3366   58    0.92\n",
       "3        12735   92    3.69\n",
       "4        11365   38   -6.60\n",
       "...        ...  ...     ...\n",
       "1448359  22604   26    2.82\n",
       "1448360  22255   36   -1.94\n",
       "1448361  21056   40   -9.56\n",
       "1448362  12328   97    0.87\n",
       "1448363  11528    3    9.32\n",
       "\n",
       "[1448364 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_joke_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96b0dcbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InteractionID</th>\n",
       "      <th>UID</th>\n",
       "      <th>JID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11228</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>21724</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>16782</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>12105</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>14427</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362086</th>\n",
       "      <td>362086</td>\n",
       "      <td>3085</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362087</th>\n",
       "      <td>362087</td>\n",
       "      <td>13765</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362088</th>\n",
       "      <td>362088</td>\n",
       "      <td>10341</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362089</th>\n",
       "      <td>362089</td>\n",
       "      <td>3553</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362090</th>\n",
       "      <td>362090</td>\n",
       "      <td>1199</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>362091 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        InteractionID    UID  JID\n",
       "0                   0  11228   39\n",
       "1                   1  21724   85\n",
       "2                   2  16782   56\n",
       "3                   3  12105   42\n",
       "4                   4  14427    2\n",
       "...               ...    ...  ...\n",
       "362086         362086   3085   66\n",
       "362087         362087  13765   31\n",
       "362088         362088  10341   29\n",
       "362089         362089   3553    8\n",
       "362090         362090   1199   67\n",
       "\n",
       "[362091 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_joke_df_nofactrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b750d0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InteractionID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362086</th>\n",
       "      <td>362086</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362087</th>\n",
       "      <td>362087</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362088</th>\n",
       "      <td>362088</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362089</th>\n",
       "      <td>362089</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362090</th>\n",
       "      <td>362090</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>362091 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        InteractionID  Rating\n",
       "0                   0     0.0\n",
       "1                   1     0.0\n",
       "2                   2     0.0\n",
       "3                   3     0.0\n",
       "4                   4     0.0\n",
       "...               ...     ...\n",
       "362086         362086     0.0\n",
       "362087         362087     0.0\n",
       "362088         362088     0.0\n",
       "362089         362089     0.0\n",
       "362090         362090     0.0\n",
       "\n",
       "[362091 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c45d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68503743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24983, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_count = np.unique(test_joke_df_nofactrating[\"UID\"]).size\n",
    "jokes_count = np.unique(test_joke_df_nofactrating[\"JID\"]).size\n",
    "users_count, jokes_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6296c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24983, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_count = np.unique(train_joke_df[\"UID\"]).size\n",
    "jokes_count = np.unique(train_joke_df[\"JID\"]).size\n",
    "users_count, jokes_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602c1ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3760c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c00ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rating = train_joke_df['Rating'].values\n",
    "#print(np.min(rating), np.max(np.abs(rating)), np.max(np.abs(rating)))\n",
    "\n",
    "#rating_norm = (rating - np.min(rating)) / (np.max(rating) - np.min(rating))\n",
    "#rating_norm = rating / np.max(np.abs(rating))\n",
    "#print(np.min(rating_norm), np.max(rating_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1ea26c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_joke_df['Rating_norm'] = rating_norm\n",
    "#X = np.zeros((users_count, jokes_count))\n",
    "\n",
    "#for row in tqdm(train_joke_df.values):\n",
    "#    user_id = int(row[0]) - 1\n",
    "#    joke_id = int(row[1]) - 1\n",
    "    \n",
    "#    rating = row[3]\n",
    "    \n",
    "#    X[user_id, joke_id] = rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6996fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1cd140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fba8a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesDataset(Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "            \n",
    "    def __getitem__(self, index: int):            \n",
    "        return torch.tensor(self.features[index], dtype=torch.float).to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3750b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d6b4969",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(train_joke_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba5458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bbb546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d3d33b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7572f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JID       28.986991\n",
       "Rating    28.986991\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(\"UID\").count().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52021dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JID       28.986991\n",
       "Rating    28.986991\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.groupby(\"UID\").count().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61495a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "686d6f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24983, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_df[\"UID\"]).size, np.unique(train_df[\"JID\"]).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9946c44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24983, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test_df[\"UID\"]).size, np.unique(test_df[\"JID\"]).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b1ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66f25ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(train_df[\"Rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3aee3147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(test_df[\"Rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df2accf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7922626e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.29, 5822),\n",
       " (2.62, 2969),\n",
       " (-0.49, 2934),\n",
       " (5.73, 2934),\n",
       " (-0.39, 2896),\n",
       " (2.82, 2890),\n",
       " (-0.53, 2871),\n",
       " (-0.44, 2868),\n",
       " (3.2, 2859),\n",
       " (4.17, 2821),\n",
       " (-0.34, 2772),\n",
       " (3.01, 2725),\n",
       " (8.83, 2704),\n",
       " (-0.24, 2693),\n",
       " (2.77, 2658),\n",
       " (2.43, 2641),\n",
       " (3.16, 2640),\n",
       " (2.96, 2640),\n",
       " (-0.58, 2631),\n",
       " (3.4, 2609),\n",
       " (2.72, 2606),\n",
       " (2.91, 2602),\n",
       " (1.07, 2596),\n",
       " (3.11, 2593),\n",
       " (3.25, 2593),\n",
       " (2.23, 2579),\n",
       " (3.59, 2571),\n",
       " (1.46, 2559),\n",
       " (2.57, 2557),\n",
       " (3.3, 2550),\n",
       " (3.64, 2544),\n",
       " (4.37, 2542),\n",
       " (3.45, 2541),\n",
       " (3.83, 2538),\n",
       " (7.28, 2532),\n",
       " (3.54, 2524),\n",
       " (3.06, 2523),\n",
       " (1.26, 2517),\n",
       " (-0.63, 2514),\n",
       " (-0.19, 2511),\n",
       " (3.35, 2509),\n",
       " (2.86, 2507),\n",
       " (3.79, 2501),\n",
       " (3.5, 2496),\n",
       " (2.38, 2494),\n",
       " (2.52, 2482),\n",
       " (9.03, 2481),\n",
       " (2.18, 2473),\n",
       " (-0.68, 2470),\n",
       " (2.67, 2460),\n",
       " (2.09, 2455),\n",
       " (2.48, 2450),\n",
       " (3.98, 2445),\n",
       " (1.65, 2436),\n",
       " (1.99, 2432),\n",
       " (1.7, 2426),\n",
       " (-0.15, 2425),\n",
       " (1.84, 2425),\n",
       " (1.21, 2423),\n",
       " (2.33, 2419),\n",
       " (3.69, 2419),\n",
       " (-0.1, 2418),\n",
       " (3.74, 2417),\n",
       " (2.28, 2416),\n",
       " (1.94, 2412),\n",
       " (2.14, 2404),\n",
       " (1.41, 2398),\n",
       " (3.88, 2395),\n",
       " (1.12, 2394),\n",
       " (3.93, 2383),\n",
       " (2.04, 2381),\n",
       " (1.5, 2370),\n",
       " (1.36, 2370),\n",
       " (1.02, 2369),\n",
       " (1.8, 2366),\n",
       " (1.17, 2359),\n",
       " (1.75, 2356),\n",
       " (8.98, 2355),\n",
       " (1.89, 2354),\n",
       " (0.97, 2353),\n",
       " (1.6, 2343),\n",
       " (0.87, 2337),\n",
       " (0.92, 2331),\n",
       " (1.31, 2331),\n",
       " (0.49, 2330),\n",
       " (5.53, 2323),\n",
       " (5.15, 2322),\n",
       " (0.78, 2321),\n",
       " (8.93, 2317),\n",
       " (0.83, 2315),\n",
       " (1.55, 2313),\n",
       " (5.49, 2305),\n",
       " (5.92, 2300),\n",
       " (-0.05, 2297),\n",
       " (5.34, 2294),\n",
       " (4.03, 2283),\n",
       " (4.13, 2280),\n",
       " (4.27, 2274),\n",
       " (0.73, 2270),\n",
       " (5.63, 2268),\n",
       " (4.22, 2267),\n",
       " (9.08, 2267),\n",
       " (6.5, 2266),\n",
       " (5.68, 2253),\n",
       " (9.13, 2252),\n",
       " (4.32, 2252),\n",
       " (4.95, 2245),\n",
       " (4.56, 2242),\n",
       " (-0.73, 2237),\n",
       " (5.05, 2235),\n",
       " (0.68, 2230),\n",
       " (8.88, 2229),\n",
       " (0.29, 2222),\n",
       " (9.17, 2219),\n",
       " (4.76, 2218),\n",
       " (6.36, 2217),\n",
       " (0.53, 2213),\n",
       " (5.58, 2208),\n",
       " (6.12, 2207),\n",
       " (5.39, 2204),\n",
       " (6.31, 2204),\n",
       " (0.1, 2199),\n",
       " (0.58, 2196),\n",
       " (5.19, 2196),\n",
       " (5.44, 2195),\n",
       " (5.78, 2193),\n",
       " (4.61, 2183),\n",
       " (5.83, 2180),\n",
       " (0.63, 2176),\n",
       " (4.66, 2164),\n",
       " (0.34, 2162),\n",
       " (5.1, 2158),\n",
       " (6.26, 2158),\n",
       " (5.29, 2153),\n",
       " (4.9, 2150),\n",
       " (6.07, 2148),\n",
       " (4.08, 2148),\n",
       " (5.0, 2145),\n",
       " (0.15, 2143),\n",
       " (0.05, 2142),\n",
       " (4.42, 2140),\n",
       " (5.87, 2140),\n",
       " (4.51, 2139),\n",
       " (0.39, 2136),\n",
       " (8.64, 2134),\n",
       " (6.02, 2129),\n",
       " (0.44, 2129),\n",
       " (5.24, 2128),\n",
       " (6.21, 2128),\n",
       " (0.0, 2127),\n",
       " (6.17, 2126),\n",
       " (5.97, 2111),\n",
       " (8.79, 2109),\n",
       " (6.41, 2104),\n",
       " (0.19, 2104),\n",
       " (4.71, 2098),\n",
       " (-0.78, 2096),\n",
       " (4.85, 2094),\n",
       " (6.46, 2083),\n",
       " (4.81, 2082),\n",
       " (6.65, 2081),\n",
       " (8.74, 2077),\n",
       " (4.47, 2073),\n",
       " (6.8, 2063),\n",
       " (0.24, 2057),\n",
       " (6.89, 2043),\n",
       " (8.69, 2038),\n",
       " (7.09, 2037),\n",
       " (-9.42, 2033),\n",
       " (6.55, 2031),\n",
       " (6.7, 2030),\n",
       " (7.14, 2019),\n",
       " (6.6, 2009),\n",
       " (7.57, 1996),\n",
       " (6.94, 1987),\n",
       " (-0.83, 1983),\n",
       " (7.33, 1982),\n",
       " (6.84, 1982),\n",
       " (9.22, 1975),\n",
       " (6.99, 1974),\n",
       " (7.04, 1974),\n",
       " (6.75, 1970),\n",
       " (7.48, 1965),\n",
       " (8.54, 1953),\n",
       " (7.67, 1942),\n",
       " (8.59, 1938),\n",
       " (7.43, 1936),\n",
       " (7.23, 1934),\n",
       " (8.45, 1925),\n",
       " (-0.87, 1912),\n",
       " (8.5, 1906),\n",
       " (7.52, 1894),\n",
       " (-0.92, 1883),\n",
       " (7.18, 1876),\n",
       " (7.62, 1867),\n",
       " (7.82, 1851),\n",
       " (8.11, 1836),\n",
       " (8.25, 1835),\n",
       " (7.96, 1834),\n",
       " (8.06, 1827),\n",
       " (7.86, 1824),\n",
       " (8.35, 1814),\n",
       " (8.3, 1811),\n",
       " (-9.61, 1808),\n",
       " (7.38, 1802),\n",
       " (7.77, 1802),\n",
       " (8.4, 1797),\n",
       " (7.91, 1785),\n",
       " (7.72, 1783),\n",
       " (-9.81, 1778),\n",
       " (8.2, 1757),\n",
       " (8.01, 1745),\n",
       " (-9.71, 1718),\n",
       " (8.16, 1708),\n",
       " (-0.97, 1705),\n",
       " (9.27, 1701),\n",
       " (-1.07, 1665),\n",
       " (-1.02, 1663),\n",
       " (-1.12, 1642),\n",
       " (-9.76, 1638),\n",
       " (-9.66, 1635),\n",
       " (-9.56, 1630),\n",
       " (-1.26, 1572),\n",
       " (-9.51, 1561),\n",
       " (-6.31, 1554),\n",
       " (-1.65, 1542),\n",
       " (-4.76, 1539),\n",
       " (-9.47, 1539),\n",
       " (-1.21, 1511),\n",
       " (-3.2, 1502),\n",
       " (-1.17, 1501),\n",
       " (-1.31, 1499),\n",
       " (-2.28, 1485),\n",
       " (-9.37, 1479),\n",
       " (-9.22, 1476),\n",
       " (-9.85, 1467),\n",
       " (-4.17, 1463),\n",
       " (-3.79, 1461),\n",
       " (-4.03, 1460),\n",
       " (-1.41, 1459),\n",
       " (-3.98, 1459),\n",
       " (-1.5, 1451),\n",
       " (-2.43, 1449),\n",
       " (-7.86, 1446),\n",
       " (-3.88, 1441),\n",
       " (-4.13, 1441),\n",
       " (-3.59, 1439),\n",
       " (-2.14, 1437),\n",
       " (-1.84, 1436),\n",
       " (-3.3, 1433),\n",
       " (-9.32, 1433),\n",
       " (-3.83, 1431),\n",
       " (-2.57, 1429),\n",
       " (-4.51, 1426),\n",
       " (-1.94, 1425),\n",
       " (-1.36, 1425),\n",
       " (-1.46, 1423),\n",
       " (-3.01, 1423),\n",
       " (-1.55, 1422),\n",
       " (-2.62, 1416),\n",
       " (-4.37, 1416),\n",
       " (-3.45, 1414),\n",
       " (-4.22, 1412),\n",
       " (-2.23, 1410),\n",
       " (-1.99, 1410),\n",
       " (-9.27, 1408),\n",
       " (-2.48, 1407),\n",
       " (-3.54, 1405),\n",
       " (-1.6, 1404),\n",
       " (-3.74, 1403),\n",
       " (-4.27, 1401),\n",
       " (-1.8, 1400),\n",
       " (-3.64, 1396),\n",
       " (-4.08, 1395),\n",
       " (-4.56, 1389),\n",
       " (-2.18, 1389),\n",
       " (-4.32, 1386),\n",
       " (-2.52, 1386),\n",
       " (-3.69, 1383),\n",
       " (-1.75, 1381),\n",
       " (-6.5, 1379),\n",
       " (-3.5, 1378),\n",
       " (-2.67, 1378),\n",
       " (-3.4, 1377),\n",
       " (-2.04, 1375),\n",
       " (-2.09, 1374),\n",
       " (-1.89, 1372),\n",
       " (-4.42, 1372),\n",
       " (-2.72, 1369),\n",
       " (-3.93, 1365),\n",
       " (-1.7, 1365),\n",
       " (-2.33, 1363),\n",
       " (-3.11, 1358),\n",
       " (-2.96, 1358),\n",
       " (-2.82, 1350),\n",
       " (-2.38, 1342),\n",
       " (-4.71, 1342),\n",
       " (9.32, 1339),\n",
       " (-4.66, 1334),\n",
       " (-9.17, 1332),\n",
       " (-3.06, 1321),\n",
       " (-2.77, 1316),\n",
       " (-3.25, 1316),\n",
       " (-4.47, 1314),\n",
       " (-4.95, 1310),\n",
       " (-2.91, 1309),\n",
       " (-3.16, 1296),\n",
       " (-2.86, 1296),\n",
       " (-6.84, 1294),\n",
       " (-3.35, 1289),\n",
       " (-7.09, 1289),\n",
       " (-4.61, 1288),\n",
       " (-6.65, 1285),\n",
       " (-9.13, 1282),\n",
       " (-9.9, 1278),\n",
       " (-6.7, 1265),\n",
       " (-6.8, 1263),\n",
       " (-6.6, 1260),\n",
       " (-6.36, 1255),\n",
       " (-4.85, 1251),\n",
       " (-6.55, 1248),\n",
       " (-9.03, 1246),\n",
       " (-6.89, 1246),\n",
       " (-9.08, 1238),\n",
       " (-5.15, 1234),\n",
       " (-6.94, 1233),\n",
       " (-8.98, 1232),\n",
       " (-7.23, 1231),\n",
       " (-6.75, 1226),\n",
       " (-6.26, 1223),\n",
       " (-4.81, 1221),\n",
       " (-6.99, 1215),\n",
       " (-7.33, 1214),\n",
       " (-7.18, 1200),\n",
       " (-8.93, 1194),\n",
       " (-7.28, 1191),\n",
       " (-6.12, 1189),\n",
       " (-5.0, 1182),\n",
       " (-6.41, 1178),\n",
       " (-7.57, 1178),\n",
       " (-7.04, 1176),\n",
       " (-8.88, 1172),\n",
       " (-7.14, 1170),\n",
       " (-4.9, 1170),\n",
       " (-8.06, 1164),\n",
       " (-6.07, 1163),\n",
       " (-6.46, 1157),\n",
       " (-7.67, 1156),\n",
       " (-7.62, 1144),\n",
       " (-7.38, 1144),\n",
       " (-7.52, 1143),\n",
       " (-7.48, 1137),\n",
       " (-5.1, 1137),\n",
       " (-6.21, 1127),\n",
       " (-7.43, 1127),\n",
       " (-7.82, 1116),\n",
       " (-7.91, 1116),\n",
       " (-6.17, 1110),\n",
       " (-8.83, 1106),\n",
       " (-5.05, 1103),\n",
       " (-8.54, 1097),\n",
       " (-8.69, 1093),\n",
       " (-5.87, 1092),\n",
       " (-8.5, 1092),\n",
       " (-6.02, 1090),\n",
       " (-5.73, 1084),\n",
       " (-7.72, 1082),\n",
       " (-5.97, 1075),\n",
       " (-8.64, 1075),\n",
       " (-7.96, 1073),\n",
       " (-8.2, 1067),\n",
       " (-5.83, 1066),\n",
       " (-8.59, 1063),\n",
       " (-5.78, 1062),\n",
       " (-5.92, 1062),\n",
       " (-8.79, 1061),\n",
       " (-5.24, 1056),\n",
       " (-5.19, 1052),\n",
       " (-5.44, 1045),\n",
       " (-8.4, 1042),\n",
       " (-8.16, 1041),\n",
       " (-7.77, 1039),\n",
       " (-8.11, 1037),\n",
       " (-5.34, 1036),\n",
       " (-5.63, 1029),\n",
       " (-8.45, 1029),\n",
       " (-5.49, 1023),\n",
       " (-5.68, 1018),\n",
       " (-8.74, 1017),\n",
       " (-8.25, 1014),\n",
       " (-8.01, 1007),\n",
       " (-5.53, 1004),\n",
       " (-5.39, 1003),\n",
       " (-5.29, 1000),\n",
       " (-5.58, 991),\n",
       " (-8.35, 987),\n",
       " (-8.3, 970),\n",
       " (-9.95, 883),\n",
       " (9.37, 821),\n",
       " (9.42, 6),\n",
       " (9.81, 4),\n",
       " (9.95, 3),\n",
       " (9.47, 2),\n",
       " (9.9, 2),\n",
       " (9.56, 2),\n",
       " (9.85, 2),\n",
       " (10.0, 2),\n",
       " (9.71, 1),\n",
       " (9.66, 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Counter(train_df[\"Rating\"]).items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8982888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.29, 5660),\n",
       " (2.62, 2963),\n",
       " (-0.39, 2962),\n",
       " (5.73, 2913),\n",
       " (-0.44, 2903),\n",
       " (-0.49, 2877),\n",
       " (-0.53, 2866),\n",
       " (2.82, 2838),\n",
       " (-0.34, 2833),\n",
       " (4.17, 2791),\n",
       " (3.01, 2755),\n",
       " (3.2, 2728),\n",
       " (8.83, 2710),\n",
       " (3.4, 2686),\n",
       " (3.35, 2674),\n",
       " (2.96, 2673),\n",
       " (-0.58, 2654),\n",
       " (3.16, 2639),\n",
       " (2.57, 2623),\n",
       " (1.07, 2613),\n",
       " (3.59, 2607),\n",
       " (2.86, 2600),\n",
       " (2.43, 2595),\n",
       " (2.38, 2582),\n",
       " (2.77, 2579),\n",
       " (3.06, 2577),\n",
       " (3.11, 2569),\n",
       " (2.23, 2566),\n",
       " (1.26, 2563),\n",
       " (-0.24, 2559),\n",
       " (2.72, 2550),\n",
       " (3.79, 2539),\n",
       " (1.46, 2538),\n",
       " (3.25, 2538),\n",
       " (3.54, 2537),\n",
       " (3.5, 2536),\n",
       " (3.45, 2528),\n",
       " (2.67, 2525),\n",
       " (7.28, 2523),\n",
       " (2.48, 2522),\n",
       " (-0.63, 2517),\n",
       " (-0.19, 2511),\n",
       " (2.52, 2510),\n",
       " (3.3, 2508),\n",
       " (1.84, 2487),\n",
       " (2.91, 2475),\n",
       " (9.03, 2474),\n",
       " (3.64, 2469),\n",
       " (1.17, 2466),\n",
       " (2.04, 2453),\n",
       " (2.33, 2453),\n",
       " (3.69, 2452),\n",
       " (-0.68, 2450),\n",
       " (1.65, 2441),\n",
       " (2.18, 2441),\n",
       " (3.74, 2439),\n",
       " (1.99, 2429),\n",
       " (2.09, 2425),\n",
       " (3.93, 2422),\n",
       " (1.12, 2412),\n",
       " (1.6, 2402),\n",
       " (1.94, 2401),\n",
       " (3.98, 2400),\n",
       " (1.8, 2394),\n",
       " (3.83, 2391),\n",
       " (0.68, 2380),\n",
       " (2.28, 2378),\n",
       " (2.14, 2377),\n",
       " (3.88, 2374),\n",
       " (0.87, 2369),\n",
       " (1.7, 2364),\n",
       " (-0.1, 2356),\n",
       " (1.41, 2335),\n",
       " (1.55, 2331),\n",
       " (-0.15, 2327),\n",
       " (1.36, 2325),\n",
       " (5.63, 2324),\n",
       " (1.21, 2324),\n",
       " (1.02, 2322),\n",
       " (5.15, 2320),\n",
       " (-0.05, 2320),\n",
       " (4.13, 2318),\n",
       " (5.34, 2317),\n",
       " (0.83, 2314),\n",
       " (1.75, 2313),\n",
       " (5.44, 2311),\n",
       " (0.63, 2308),\n",
       " (1.89, 2299),\n",
       " (0.92, 2295),\n",
       " (5.0, 2294),\n",
       " (6.31, 2294),\n",
       " (4.37, 2294),\n",
       " (9.08, 2291),\n",
       " (4.03, 2289),\n",
       " (8.93, 2289),\n",
       " (-0.73, 2287),\n",
       " (1.31, 2285),\n",
       " (4.76, 2284),\n",
       " (4.56, 2279),\n",
       " (4.22, 2279),\n",
       " (0.97, 2278),\n",
       " (5.53, 2271),\n",
       " (5.92, 2268),\n",
       " (0.29, 2262),\n",
       " (4.08, 2257),\n",
       " (4.32, 2254),\n",
       " (5.78, 2252),\n",
       " (8.88, 2251),\n",
       " (4.95, 2250),\n",
       " (9.13, 2244),\n",
       " (5.58, 2243),\n",
       " (5.29, 2239),\n",
       " (0.73, 2227),\n",
       " (1.5, 2217),\n",
       " (5.1, 2213),\n",
       " (0.58, 2212),\n",
       " (5.49, 2207),\n",
       " (8.98, 2205),\n",
       " (5.19, 2202),\n",
       " (0.49, 2202),\n",
       " (4.27, 2202),\n",
       " (6.17, 2200),\n",
       " (-0.78, 2200),\n",
       " (5.05, 2199),\n",
       " (0.78, 2190),\n",
       " (6.12, 2189),\n",
       " (0.34, 2184),\n",
       " (5.39, 2182),\n",
       " (0.53, 2178),\n",
       " (5.68, 2175),\n",
       " (4.61, 2172),\n",
       " (0.0, 2171),\n",
       " (5.24, 2167),\n",
       " (0.1, 2166),\n",
       " (5.87, 2165),\n",
       " (9.17, 2163),\n",
       " (4.81, 2160),\n",
       " (4.42, 2153),\n",
       " (6.36, 2152),\n",
       " (0.39, 2148),\n",
       " (4.47, 2139),\n",
       " (6.02, 2136),\n",
       " (6.26, 2135),\n",
       " (8.79, 2130),\n",
       " (6.5, 2130),\n",
       " (8.74, 2129),\n",
       " (4.71, 2128),\n",
       " (7.48, 2128),\n",
       " (4.51, 2125),\n",
       " (4.85, 2122),\n",
       " (6.41, 2107),\n",
       " (5.97, 2105),\n",
       " (9.22, 2105),\n",
       " (0.15, 2102),\n",
       " (4.9, 2101),\n",
       " (8.69, 2099),\n",
       " (0.24, 2090),\n",
       " (6.65, 2087),\n",
       " (-0.83, 2086),\n",
       " (0.19, 2085),\n",
       " (6.07, 2083),\n",
       " (6.7, 2080),\n",
       " (8.64, 2080),\n",
       " (6.89, 2079),\n",
       " (6.46, 2074),\n",
       " (6.75, 2073),\n",
       " (6.55, 2071),\n",
       " (0.44, 2071),\n",
       " (0.05, 2068),\n",
       " (5.83, 2058),\n",
       " (7.09, 2054),\n",
       " (6.6, 2049),\n",
       " (4.66, 2044),\n",
       " (6.21, 2038),\n",
       " (6.84, 2021),\n",
       " (7.14, 2010),\n",
       " (6.94, 1991),\n",
       " (8.54, 1979),\n",
       " (8.59, 1966),\n",
       " (-0.87, 1955),\n",
       " (7.67, 1950),\n",
       " (7.18, 1947),\n",
       " (6.99, 1941),\n",
       " (-9.42, 1941),\n",
       " (7.33, 1932),\n",
       " (7.57, 1927),\n",
       " (7.04, 1914),\n",
       " (6.8, 1913),\n",
       " (8.45, 1912),\n",
       " (8.3, 1908),\n",
       " (8.4, 1904),\n",
       " (7.43, 1904),\n",
       " (-9.61, 1891),\n",
       " (8.25, 1890),\n",
       " (7.38, 1889),\n",
       " (7.62, 1889),\n",
       " (8.5, 1888),\n",
       " (7.23, 1883),\n",
       " (7.72, 1864),\n",
       " (7.96, 1863),\n",
       " (7.86, 1848),\n",
       " (8.11, 1847),\n",
       " (7.52, 1842),\n",
       " (8.2, 1838),\n",
       " (7.77, 1827),\n",
       " (8.35, 1817),\n",
       " (8.06, 1813),\n",
       " (7.91, 1813),\n",
       " (8.01, 1813),\n",
       " (7.82, 1802),\n",
       " (9.27, 1781),\n",
       " (-9.81, 1777),\n",
       " (-0.92, 1775),\n",
       " (8.16, 1764),\n",
       " (-0.97, 1725),\n",
       " (-9.76, 1687),\n",
       " (-9.56, 1684),\n",
       " (-1.02, 1678),\n",
       " (-9.66, 1675),\n",
       " (-9.71, 1673),\n",
       " (-9.51, 1639),\n",
       " (-1.07, 1619),\n",
       " (-1.26, 1603),\n",
       " (-6.31, 1598),\n",
       " (-9.47, 1571),\n",
       " (-4.76, 1550),\n",
       " (-1.21, 1549),\n",
       " (-1.12, 1542),\n",
       " (-1.65, 1534),\n",
       " (-3.2, 1528),\n",
       " (-1.46, 1504),\n",
       " (-9.85, 1502),\n",
       " (-7.86, 1497),\n",
       " (-4.17, 1494),\n",
       " (-9.37, 1490),\n",
       " (-4.56, 1482),\n",
       " (-1.41, 1462),\n",
       " (-1.17, 1460),\n",
       " (-9.32, 1455),\n",
       " (-2.14, 1454),\n",
       " (-2.23, 1450),\n",
       " (-2.62, 1449),\n",
       " (-1.8, 1441),\n",
       " (-1.5, 1439),\n",
       " (-2.04, 1437),\n",
       " (-3.4, 1435),\n",
       " (-1.31, 1434),\n",
       " (-3.93, 1430),\n",
       " (-3.54, 1429),\n",
       " (-1.84, 1428),\n",
       " (-2.67, 1426),\n",
       " (-4.22, 1421),\n",
       " (-1.6, 1420),\n",
       " (-1.36, 1420),\n",
       " (-2.82, 1419),\n",
       " (-1.55, 1418),\n",
       " (-1.89, 1416),\n",
       " (-4.13, 1416),\n",
       " (-2.28, 1416),\n",
       " (-2.09, 1414),\n",
       " (-3.64, 1412),\n",
       " (-3.88, 1412),\n",
       " (-3.79, 1411),\n",
       " (-2.33, 1411),\n",
       " (-3.45, 1409),\n",
       " (-3.98, 1408),\n",
       " (-4.32, 1407),\n",
       " (-3.59, 1405),\n",
       " (-9.27, 1399),\n",
       " (-3.83, 1397),\n",
       " (-3.16, 1395),\n",
       " (-1.99, 1393),\n",
       " (-1.94, 1391),\n",
       " (-3.01, 1383),\n",
       " (-9.22, 1378),\n",
       " (-1.75, 1371),\n",
       " (-4.37, 1370),\n",
       " (-4.03, 1365),\n",
       " (-2.38, 1364),\n",
       " (-2.86, 1361),\n",
       " (-2.18, 1360),\n",
       " (-4.61, 1355),\n",
       " (-4.95, 1353),\n",
       " (-3.5, 1351),\n",
       " (-4.08, 1350),\n",
       " (-2.43, 1348),\n",
       " (-3.74, 1347),\n",
       " (-2.48, 1341),\n",
       " (-2.91, 1340),\n",
       " (-4.42, 1340),\n",
       " (-2.72, 1337),\n",
       " (-4.47, 1337),\n",
       " (9.32, 1335),\n",
       " (-3.3, 1333),\n",
       " (-2.57, 1329),\n",
       " (-2.52, 1329),\n",
       " (-4.51, 1326),\n",
       " (-4.27, 1325),\n",
       " (-6.7, 1322),\n",
       " (-1.7, 1316),\n",
       " (-3.11, 1316),\n",
       " (-4.66, 1311),\n",
       " (-9.08, 1309),\n",
       " (-2.96, 1306),\n",
       " (-3.69, 1306),\n",
       " (-2.77, 1303),\n",
       " (-3.25, 1300),\n",
       " (-3.06, 1297),\n",
       " (-9.17, 1297),\n",
       " (-4.85, 1292),\n",
       " (-3.35, 1292),\n",
       " (-9.13, 1275),\n",
       " (-9.9, 1274),\n",
       " (-6.65, 1273),\n",
       " (-6.75, 1273),\n",
       " (-7.18, 1267),\n",
       " (-6.5, 1262),\n",
       " (-4.71, 1257),\n",
       " (-6.6, 1250),\n",
       " (-9.03, 1247),\n",
       " (-7.09, 1244),\n",
       " (-6.8, 1237),\n",
       " (-6.89, 1234),\n",
       " (-8.98, 1232),\n",
       " (-4.9, 1230),\n",
       " (-6.99, 1226),\n",
       " (-4.81, 1223),\n",
       " (-6.36, 1218),\n",
       " (-6.94, 1217),\n",
       " (-7.04, 1211),\n",
       " (-6.55, 1205),\n",
       " (-6.17, 1198),\n",
       " (-6.41, 1197),\n",
       " (-6.46, 1197),\n",
       " (-5.15, 1191),\n",
       " (-7.33, 1188),\n",
       " (-7.14, 1185),\n",
       " (-6.84, 1179),\n",
       " (-6.07, 1177),\n",
       " (-7.67, 1177),\n",
       " (-5.05, 1175),\n",
       " (-7.28, 1173),\n",
       " (-7.57, 1170),\n",
       " (-5.92, 1169),\n",
       " (-6.26, 1168),\n",
       " (-7.23, 1164),\n",
       " (-7.48, 1159),\n",
       " (-7.43, 1158),\n",
       " (-5.34, 1156),\n",
       " (-6.21, 1150),\n",
       " (-7.38, 1150),\n",
       " (-8.93, 1149),\n",
       " (-6.12, 1146),\n",
       " (-7.82, 1142),\n",
       " (-5.0, 1141),\n",
       " (-7.72, 1130),\n",
       " (-8.25, 1128),\n",
       " (-7.52, 1126),\n",
       " (-7.62, 1126),\n",
       " (-8.83, 1124),\n",
       " (-5.1, 1122),\n",
       " (-8.06, 1106),\n",
       " (-8.88, 1103),\n",
       " (-8.79, 1100),\n",
       " (-5.78, 1099),\n",
       " (-5.97, 1094),\n",
       " (-8.54, 1092),\n",
       " (-6.02, 1084),\n",
       " (-8.64, 1080),\n",
       " (-7.77, 1071),\n",
       " (-5.73, 1065),\n",
       " (-8.01, 1063),\n",
       " (-8.69, 1062),\n",
       " (-5.53, 1061),\n",
       " (-8.3, 1058),\n",
       " (-8.59, 1054),\n",
       " (-7.96, 1054),\n",
       " (-5.63, 1053),\n",
       " (-8.16, 1050),\n",
       " (-8.2, 1047),\n",
       " (-5.49, 1043),\n",
       " (-5.68, 1041),\n",
       " (-5.58, 1040),\n",
       " (-8.74, 1032),\n",
       " (-5.39, 1031),\n",
       " (-5.19, 1029),\n",
       " (-5.29, 1028),\n",
       " (-5.24, 1020),\n",
       " (-8.5, 1019),\n",
       " (-8.11, 1018),\n",
       " (-7.91, 1017),\n",
       " (-5.87, 1017),\n",
       " (-5.83, 1015),\n",
       " (-5.44, 1014),\n",
       " (-8.45, 1011),\n",
       " (-8.35, 996),\n",
       " (-8.4, 982),\n",
       " (-9.95, 911),\n",
       " (9.37, 895),\n",
       " (9.85, 5),\n",
       " (9.56, 4),\n",
       " (9.76, 4),\n",
       " (9.42, 3),\n",
       " (9.9, 3),\n",
       " (9.61, 2),\n",
       " (9.47, 2),\n",
       " (9.81, 2),\n",
       " (9.51, 1),\n",
       " (9.71, 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Counter(test_df[\"Rating\"]).items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5498ebfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\torchvision\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(train_df['Rating'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cc6440d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\torchvision\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(test_df['Rating'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53bae23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d522d209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8523687d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f71940c87c480cb46416ba1457aea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/724182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = np.zeros((users_count, jokes_count))\n",
    "\n",
    "for row in tqdm(train_df.values):\n",
    "    user_id = int(row[0]) - 1\n",
    "    joke_id = int(row[1]) - 1\n",
    "    \n",
    "    rating = row[2]\n",
    "    \n",
    "    X_train[user_id, joke_id] = rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63e898e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebda60f469b64a989aab6337f8cda522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/724182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test = np.zeros((users_count, jokes_count))\n",
    "\n",
    "for row in tqdm(test_df.values):\n",
    "    user_id = int(row[0]) - 1\n",
    "    joke_id = int(row[1]) - 1\n",
    "    \n",
    "    rating = row[2]\n",
    "    \n",
    "    X_test[user_id, joke_id] = rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f31f17ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test = train_test_split(X, test_size=0.15, random_state=42)\n",
    "#X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5d58d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = FeaturesDataset(X_train)\n",
    "dataset_test = FeaturesDataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "232b007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d380a6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"#\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e544d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeacaae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aff2e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "def save(model, name):\n",
    "    os.mkdir(f\"artifacts2/{name}\")\n",
    "    #torch.save(model, f\"{name}/model.pkl\")\n",
    "    torch.save(model.state_dict(), f\"artifacts2/{name}/checkpoint.pth\")\n",
    "    \n",
    "def load(name):\n",
    "    return torch.load(f\"artifacts2/{name}/model.pkl\")\n",
    "\n",
    "\n",
    "def load2(name, model):\n",
    "    model.load_state_dict(torch.load(f\"artifacts2/{name}/checkpoint.pth\"))\n",
    "    \n",
    "def train_model(epoch_start, model, train_loader, val_loader, loss, optimizer, num_epochs, scheduler, loss_train_history, loss_val_history):\n",
    "    best_loss = compute_accuracy(model, val_loader, loss)\n",
    "    print('loss:', best_loss)\n",
    "    bet_model_name = None\n",
    "    for epoch in range(epoch_start, epoch_start + num_epochs):\n",
    "        t1 = time.time()\n",
    "        model.train()\n",
    "        loss_accum = 0\n",
    "        for i_step, x,in enumerate(train_loader):\n",
    "            prediction = model(x)    \n",
    "            loss_value = loss(prediction, x)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()            \n",
    "            loss_accum += loss_value\n",
    "            \n",
    "          \n",
    "        ave_loss = loss_accum / (i_step + 1)\n",
    "        loss_val = compute_accuracy(model, val_loader, loss)\n",
    "        \n",
    "        loss_train_history.append(float(ave_loss))\n",
    "        loss_val_history.append(loss_val)\n",
    "        \n",
    "        if scheduler != None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        if loss_val < best_loss:\n",
    "            best_loss = loss_val\n",
    "            bet_model_name = f'{datetime.datetime.now().strftime(\"%d.%m.%Y_%H.%M.%S.%f\")}_epoch_{epoch}_loss_{round(best_loss, 4)}'\n",
    "            save(model, bet_model_name)\n",
    "            print(f\"saved {bet_model_name}\")\n",
    "            \n",
    "        print(\"Epoch: %i lr: %f; Train loss: %f, Val loss: %f, time: %i s\" % (epoch, get_lr(optimizer), ave_loss, loss_val,\n",
    "                                                                            round(time.time() - t1)))\n",
    "    return bet_model_name\n",
    "        \n",
    "    \n",
    "def compute_accuracy(model, loader, loss):\n",
    "    \"\"\"\n",
    "    Computes accuracy on the dataset wrapped in a loader    \n",
    "    Returns: accuracy as a float value between 0 and 1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss_accum = 0\n",
    "    for i_step, x in enumerate(loader):\n",
    "        prediction = model(x)\n",
    "        loss_value = loss(prediction, x)\n",
    "        loss_accum += loss_value\n",
    "\n",
    "    ave_loss = loss_accum / (i_step + 1)         \n",
    "    return float(ave_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb1e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301835cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e701f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f106cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fdd5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eea6b2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAEModel(\n",
      "  (encode): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=64, bias=True)\n",
      "    (1): Dropout(p=0.66, inplace=False)\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "  )\n",
      "  (decode): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (1): Dropout(p=0.66, inplace=False)\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=64, out_features=100, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class VAEModel(nn.Module):\n",
    "    def __init__(self, input_shape, seed, ratings_range):\n",
    "        super().__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.y_range = ratings_range\n",
    "        \n",
    "        shape = 64\n",
    "        self.encode = nn.Sequential(\n",
    "                nn.Linear(input_shape, shape),\n",
    "                nn.Dropout(0.66),\n",
    "                nn.BatchNorm1d(shape),\n",
    "                nn.ReLU(inplace=True),    \n",
    "\n",
    "                nn.Linear(shape, shape // 2),\n",
    "                )\n",
    "\n",
    "\n",
    "        self.decode = nn.Sequential(\n",
    "                nn.Linear(shape // 2, shape),\n",
    "                nn.Dropout(0.66),\n",
    "                nn.BatchNorm1d(shape),\n",
    "                nn.ReLU(inplace=True),         \n",
    "                        \n",
    "                nn.Linear(shape, input_shape),\n",
    "                )        \n",
    "    def forward(self, x):\n",
    "        embeddings = self.encode(x)        \n",
    "        result = self.decode(embeddings)\n",
    "        return (torch.sigmoid(result) * (self.y_range[1] - self.y_range[0]) + self.y_range[0])\n",
    "    \n",
    "    \n",
    "input_shape = X_train.shape[1]\n",
    "display(input_shape)\n",
    "\n",
    "nn_model = VAEModel(input_shape, seed=1024, ratings_range=[-10, 10]).to(device)\n",
    "\n",
    "print(nn_model)\n",
    "\n",
    "loss_train_history, loss_val_history = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "963b5d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4209271a2c4873a85e244fbbe34faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 100])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in tqdm(DataLoader(dataset_train, batch_size=8)):\n",
    "    break\n",
    "print(x.shape)\n",
    "nn_model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a2faa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8c8fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE_loss(prediction, target):\n",
    "    return torch.sqrt(nn.MSELoss()(prediction, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e36598f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.954916000366211\n",
      "saved 22.04.2023_16.10.42.848087_epoch_0_loss_2.9209\n",
      "Epoch: 0 lr: 0.001000; Train loss: 3.447392, Val loss: 2.920904, time: 1 s\n",
      "saved 22.04.2023_16.10.43.268784_epoch_1_loss_2.903\n",
      "Epoch: 1 lr: 0.001000; Train loss: 3.398183, Val loss: 2.903017, time: 0 s\n",
      "saved 22.04.2023_16.10.43.795671_epoch_2_loss_2.8912\n",
      "Epoch: 2 lr: 0.001000; Train loss: 3.352556, Val loss: 2.891205, time: 1 s\n",
      "saved 22.04.2023_16.10.44.222689_epoch_3_loss_2.8826\n",
      "Epoch: 3 lr: 0.001000; Train loss: 3.311637, Val loss: 2.882556, time: 0 s\n",
      "saved 22.04.2023_16.10.44.740926_epoch_4_loss_2.8757\n",
      "Epoch: 4 lr: 0.000900; Train loss: 3.271966, Val loss: 2.875717, time: 1 s\n",
      "saved 22.04.2023_16.10.45.160093_epoch_5_loss_2.8705\n",
      "Epoch: 5 lr: 0.000900; Train loss: 3.235353, Val loss: 2.870477, time: 0 s\n",
      "saved 22.04.2023_16.10.45.667857_epoch_6_loss_2.866\n",
      "Epoch: 6 lr: 0.000900; Train loss: 3.203673, Val loss: 2.866008, time: 1 s\n",
      "saved 22.04.2023_16.10.46.089174_epoch_7_loss_2.8621\n",
      "Epoch: 7 lr: 0.000900; Train loss: 3.175586, Val loss: 2.862080, time: 0 s\n",
      "saved 22.04.2023_16.10.46.583500_epoch_8_loss_2.8585\n",
      "Epoch: 8 lr: 0.000900; Train loss: 3.150322, Val loss: 2.858532, time: 0 s\n",
      "saved 22.04.2023_16.10.47.004006_epoch_9_loss_2.8553\n",
      "Epoch: 9 lr: 0.000810; Train loss: 3.124292, Val loss: 2.855256, time: 0 s\n",
      "saved 22.04.2023_16.10.47.496610_epoch_10_loss_2.8524\n",
      "Epoch: 10 lr: 0.000810; Train loss: 3.101499, Val loss: 2.852362, time: 0 s\n",
      "saved 22.04.2023_16.10.47.913469_epoch_11_loss_2.8496\n",
      "Epoch: 11 lr: 0.000810; Train loss: 3.081133, Val loss: 2.849596, time: 0 s\n",
      "saved 22.04.2023_16.10.48.400507_epoch_12_loss_2.8469\n",
      "Epoch: 12 lr: 0.000810; Train loss: 3.061589, Val loss: 2.846914, time: 0 s\n",
      "saved 22.04.2023_16.10.48.813344_epoch_13_loss_2.8444\n",
      "Epoch: 13 lr: 0.000810; Train loss: 3.044289, Val loss: 2.844363, time: 0 s\n",
      "saved 22.04.2023_16.10.49.308556_epoch_14_loss_2.8418\n",
      "Epoch: 14 lr: 0.000729; Train loss: 3.026175, Val loss: 2.841813, time: 0 s\n",
      "saved 22.04.2023_16.10.49.796696_epoch_15_loss_2.8394\n",
      "Epoch: 15 lr: 0.000729; Train loss: 3.012487, Val loss: 2.839371, time: 0 s\n",
      "saved 22.04.2023_16.10.50.218811_epoch_16_loss_2.8369\n",
      "Epoch: 16 lr: 0.000729; Train loss: 2.999497, Val loss: 2.836924, time: 0 s\n",
      "saved 22.04.2023_16.10.50.710876_epoch_17_loss_2.8345\n",
      "Epoch: 17 lr: 0.000729; Train loss: 2.984668, Val loss: 2.834532, time: 0 s\n",
      "saved 22.04.2023_16.10.51.125842_epoch_18_loss_2.8322\n",
      "Epoch: 18 lr: 0.000729; Train loss: 2.972557, Val loss: 2.832184, time: 0 s\n",
      "saved 22.04.2023_16.10.51.610968_epoch_19_loss_2.8299\n",
      "Epoch: 19 lr: 0.000656; Train loss: 2.964160, Val loss: 2.829871, time: 0 s\n",
      "saved 22.04.2023_16.10.52.029849_epoch_20_loss_2.8276\n",
      "Epoch: 20 lr: 0.000656; Train loss: 2.952823, Val loss: 2.827595, time: 0 s\n",
      "saved 22.04.2023_16.10.52.513430_epoch_21_loss_2.8254\n",
      "Epoch: 21 lr: 0.000656; Train loss: 2.943577, Val loss: 2.825353, time: 0 s\n",
      "saved 22.04.2023_16.10.52.949534_epoch_22_loss_2.8231\n",
      "Epoch: 22 lr: 0.000656; Train loss: 2.936813, Val loss: 2.823112, time: 0 s\n",
      "saved 22.04.2023_16.10.53.449307_epoch_23_loss_2.8209\n",
      "Epoch: 23 lr: 0.000656; Train loss: 2.929049, Val loss: 2.820902, time: 0 s\n",
      "saved 22.04.2023_16.10.53.898041_epoch_24_loss_2.8187\n",
      "Epoch: 24 lr: 0.000590; Train loss: 2.920204, Val loss: 2.818734, time: 0 s\n",
      "saved 22.04.2023_16.10.54.388400_epoch_25_loss_2.8166\n",
      "Epoch: 25 lr: 0.000590; Train loss: 2.913954, Val loss: 2.816605, time: 0 s\n",
      "saved 22.04.2023_16.10.54.810759_epoch_26_loss_2.8145\n",
      "Epoch: 26 lr: 0.000590; Train loss: 2.907732, Val loss: 2.814500, time: 0 s\n",
      "saved 22.04.2023_16.10.55.313616_epoch_27_loss_2.8124\n",
      "Epoch: 27 lr: 0.000590; Train loss: 2.904006, Val loss: 2.812402, time: 1 s\n",
      "saved 22.04.2023_16.10.55.741441_epoch_28_loss_2.8103\n",
      "Epoch: 28 lr: 0.000590; Train loss: 2.898366, Val loss: 2.810305, time: 0 s\n",
      "saved 22.04.2023_16.10.56.227871_epoch_29_loss_2.8082\n",
      "Epoch: 29 lr: 0.000531; Train loss: 2.894463, Val loss: 2.808194, time: 0 s\n",
      "saved 22.04.2023_16.10.56.654308_epoch_30_loss_2.8061\n",
      "Epoch: 30 lr: 0.000531; Train loss: 2.890452, Val loss: 2.806083, time: 0 s\n",
      "saved 22.04.2023_16.10.57.151531_epoch_31_loss_2.804\n",
      "Epoch: 31 lr: 0.000531; Train loss: 2.887527, Val loss: 2.803998, time: 0 s\n",
      "saved 22.04.2023_16.10.57.581453_epoch_32_loss_2.8019\n",
      "Epoch: 32 lr: 0.000531; Train loss: 2.884387, Val loss: 2.801880, time: 0 s\n",
      "saved 22.04.2023_16.10.58.102733_epoch_33_loss_2.7997\n",
      "Epoch: 33 lr: 0.000531; Train loss: 2.880474, Val loss: 2.799717, time: 1 s\n",
      "saved 22.04.2023_16.10.58.613276_epoch_34_loss_2.7976\n",
      "Epoch: 34 lr: 0.000478; Train loss: 2.878072, Val loss: 2.797622, time: 1 s\n",
      "saved 22.04.2023_16.10.59.050924_epoch_35_loss_2.7956\n",
      "Epoch: 35 lr: 0.000478; Train loss: 2.875088, Val loss: 2.795572, time: 0 s\n",
      "saved 22.04.2023_16.10.59.542953_epoch_36_loss_2.7936\n",
      "Epoch: 36 lr: 0.000478; Train loss: 2.872141, Val loss: 2.793561, time: 0 s\n",
      "saved 22.04.2023_16.11.00.016196_epoch_37_loss_2.7915\n",
      "Epoch: 37 lr: 0.000478; Train loss: 2.871029, Val loss: 2.791494, time: 0 s\n",
      "saved 22.04.2023_16.11.00.508602_epoch_38_loss_2.7894\n",
      "Epoch: 38 lr: 0.000478; Train loss: 2.869175, Val loss: 2.789445, time: 0 s\n",
      "saved 22.04.2023_16.11.00.934123_epoch_39_loss_2.7874\n",
      "Epoch: 39 lr: 0.000430; Train loss: 2.866087, Val loss: 2.787385, time: 0 s\n",
      "saved 22.04.2023_16.11.01.431969_epoch_40_loss_2.7854\n",
      "Epoch: 40 lr: 0.000430; Train loss: 2.864319, Val loss: 2.785423, time: 0 s\n",
      "saved 22.04.2023_16.11.01.859586_epoch_41_loss_2.7835\n",
      "Epoch: 41 lr: 0.000430; Train loss: 2.862578, Val loss: 2.783491, time: 0 s\n",
      "saved 22.04.2023_16.11.02.365361_epoch_42_loss_2.7816\n",
      "Epoch: 42 lr: 0.000430; Train loss: 2.859793, Val loss: 2.781555, time: 1 s\n",
      "saved 22.04.2023_16.11.02.802104_epoch_43_loss_2.7797\n",
      "Epoch: 43 lr: 0.000430; Train loss: 2.858987, Val loss: 2.779668, time: 0 s\n",
      "saved 22.04.2023_16.11.03.309791_epoch_44_loss_2.7778\n",
      "Epoch: 44 lr: 0.000387; Train loss: 2.856755, Val loss: 2.777818, time: 1 s\n",
      "saved 22.04.2023_16.11.03.733435_epoch_45_loss_2.7761\n",
      "Epoch: 45 lr: 0.000387; Train loss: 2.856111, Val loss: 2.776095, time: 0 s\n",
      "saved 22.04.2023_16.11.04.225311_epoch_46_loss_2.7744\n",
      "Epoch: 46 lr: 0.000387; Train loss: 2.853624, Val loss: 2.774375, time: 0 s\n",
      "saved 22.04.2023_16.11.04.653079_epoch_47_loss_2.7726\n",
      "Epoch: 47 lr: 0.000387; Train loss: 2.851558, Val loss: 2.772643, time: 0 s\n",
      "saved 22.04.2023_16.11.05.183029_epoch_48_loss_2.7709\n",
      "Epoch: 48 lr: 0.000387; Train loss: 2.851359, Val loss: 2.770911, time: 1 s\n",
      "saved 22.04.2023_16.11.05.614265_epoch_49_loss_2.7692\n",
      "Epoch: 49 lr: 0.000349; Train loss: 2.848435, Val loss: 2.769248, time: 0 s\n",
      "saved 22.04.2023_16.11.06.094272_epoch_50_loss_2.7677\n",
      "Epoch: 50 lr: 0.000349; Train loss: 2.848392, Val loss: 2.767739, time: 0 s\n",
      "saved 22.04.2023_16.11.06.518340_epoch_51_loss_2.7662\n",
      "Epoch: 51 lr: 0.000349; Train loss: 2.848774, Val loss: 2.766210, time: 0 s\n",
      "saved 22.04.2023_16.11.07.038612_epoch_52_loss_2.7647\n",
      "Epoch: 52 lr: 0.000349; Train loss: 2.845151, Val loss: 2.764679, time: 1 s\n",
      "saved 22.04.2023_16.11.07.531603_epoch_53_loss_2.7632\n",
      "Epoch: 53 lr: 0.000349; Train loss: 2.843661, Val loss: 2.763187, time: 0 s\n",
      "saved 22.04.2023_16.11.07.951608_epoch_54_loss_2.7617\n",
      "Epoch: 54 lr: 0.000314; Train loss: 2.842737, Val loss: 2.761719, time: 0 s\n",
      "saved 22.04.2023_16.11.08.421915_epoch_55_loss_2.7604\n",
      "Epoch: 55 lr: 0.000314; Train loss: 2.840328, Val loss: 2.760438, time: 0 s\n",
      "saved 22.04.2023_16.11.08.833085_epoch_56_loss_2.7591\n",
      "Epoch: 56 lr: 0.000314; Train loss: 2.839137, Val loss: 2.759122, time: 0 s\n",
      "saved 22.04.2023_16.11.09.306523_epoch_57_loss_2.7578\n",
      "Epoch: 57 lr: 0.000314; Train loss: 2.839097, Val loss: 2.757808, time: 0 s\n",
      "saved 22.04.2023_16.11.09.726773_epoch_58_loss_2.7565\n",
      "Epoch: 58 lr: 0.000314; Train loss: 2.836990, Val loss: 2.756526, time: 0 s\n",
      "saved 22.04.2023_16.11.10.199244_epoch_59_loss_2.7552\n",
      "Epoch: 59 lr: 0.000282; Train loss: 2.836877, Val loss: 2.755186, time: 0 s\n",
      "saved 22.04.2023_16.11.10.611758_epoch_60_loss_2.754\n",
      "Epoch: 60 lr: 0.000282; Train loss: 2.836239, Val loss: 2.753994, time: 0 s\n",
      "saved 22.04.2023_16.11.11.090776_epoch_61_loss_2.7528\n",
      "Epoch: 61 lr: 0.000282; Train loss: 2.833528, Val loss: 2.752845, time: 0 s\n",
      "saved 22.04.2023_16.11.11.514580_epoch_62_loss_2.7516\n",
      "Epoch: 62 lr: 0.000282; Train loss: 2.833373, Val loss: 2.751649, time: 0 s\n",
      "saved 22.04.2023_16.11.11.994716_epoch_63_loss_2.7504\n",
      "Epoch: 63 lr: 0.000282; Train loss: 2.831527, Val loss: 2.750432, time: 0 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 22.04.2023_16.11.12.423827_epoch_64_loss_2.7493\n",
      "Epoch: 64 lr: 0.000254; Train loss: 2.830935, Val loss: 2.749273, time: 0 s\n",
      "saved 22.04.2023_16.11.12.914932_epoch_65_loss_2.7482\n",
      "Epoch: 65 lr: 0.000254; Train loss: 2.830649, Val loss: 2.748202, time: 0 s\n",
      "saved 22.04.2023_16.11.13.331778_epoch_66_loss_2.7473\n",
      "Epoch: 66 lr: 0.000254; Train loss: 2.827850, Val loss: 2.747251, time: 0 s\n",
      "saved 22.04.2023_16.11.13.828985_epoch_67_loss_2.7463\n",
      "Epoch: 67 lr: 0.000254; Train loss: 2.827462, Val loss: 2.746301, time: 0 s\n",
      "saved 22.04.2023_16.11.14.264658_epoch_68_loss_2.7453\n",
      "Epoch: 68 lr: 0.000254; Train loss: 2.825804, Val loss: 2.745343, time: 0 s\n",
      "saved 22.04.2023_16.11.14.770778_epoch_69_loss_2.7444\n",
      "Epoch: 69 lr: 0.000229; Train loss: 2.824990, Val loss: 2.744384, time: 1 s\n",
      "saved 22.04.2023_16.11.15.187991_epoch_70_loss_2.7435\n",
      "Epoch: 70 lr: 0.000229; Train loss: 2.825273, Val loss: 2.743518, time: 0 s\n",
      "saved 22.04.2023_16.11.15.698178_epoch_71_loss_2.7426\n",
      "Epoch: 71 lr: 0.000229; Train loss: 2.824359, Val loss: 2.742621, time: 1 s\n",
      "saved 22.04.2023_16.11.16.195095_epoch_72_loss_2.7417\n",
      "Epoch: 72 lr: 0.000229; Train loss: 2.823557, Val loss: 2.741742, time: 0 s\n",
      "saved 22.04.2023_16.11.16.630541_epoch_73_loss_2.7409\n",
      "Epoch: 73 lr: 0.000229; Train loss: 2.822770, Val loss: 2.740857, time: 0 s\n",
      "saved 22.04.2023_16.11.17.135364_epoch_74_loss_2.74\n",
      "Epoch: 74 lr: 0.000206; Train loss: 2.821831, Val loss: 2.739967, time: 1 s\n",
      "saved 22.04.2023_16.11.17.584096_epoch_75_loss_2.7392\n",
      "Epoch: 75 lr: 0.000206; Train loss: 2.819219, Val loss: 2.739236, time: 0 s\n",
      "saved 22.04.2023_16.11.18.054542_epoch_76_loss_2.7385\n",
      "Epoch: 76 lr: 0.000206; Train loss: 2.819860, Val loss: 2.738522, time: 0 s\n",
      "saved 22.04.2023_16.11.18.485677_epoch_77_loss_2.7378\n",
      "Epoch: 77 lr: 0.000206; Train loss: 2.818194, Val loss: 2.737800, time: 0 s\n",
      "saved 22.04.2023_16.11.18.966386_epoch_78_loss_2.737\n",
      "Epoch: 78 lr: 0.000206; Train loss: 2.817461, Val loss: 2.736987, time: 0 s\n",
      "saved 22.04.2023_16.11.19.369827_epoch_79_loss_2.7362\n",
      "Epoch: 79 lr: 0.000185; Train loss: 2.816942, Val loss: 2.736189, time: 0 s\n",
      "saved 22.04.2023_16.11.19.844283_epoch_80_loss_2.7355\n",
      "Epoch: 80 lr: 0.000185; Train loss: 2.815800, Val loss: 2.735521, time: 0 s\n",
      "saved 22.04.2023_16.11.20.255995_epoch_81_loss_2.7348\n",
      "Epoch: 81 lr: 0.000185; Train loss: 2.816389, Val loss: 2.734793, time: 0 s\n",
      "saved 22.04.2023_16.11.20.733078_epoch_82_loss_2.7341\n",
      "Epoch: 82 lr: 0.000185; Train loss: 2.814297, Val loss: 2.734134, time: 0 s\n",
      "saved 22.04.2023_16.11.21.141186_epoch_83_loss_2.7334\n",
      "Epoch: 83 lr: 0.000185; Train loss: 2.813950, Val loss: 2.733398, time: 0 s\n",
      "saved 22.04.2023_16.11.21.629084_epoch_84_loss_2.7327\n",
      "Epoch: 84 lr: 0.000167; Train loss: 2.813970, Val loss: 2.732653, time: 0 s\n",
      "saved 22.04.2023_16.11.22.036647_epoch_85_loss_2.732\n",
      "Epoch: 85 lr: 0.000167; Train loss: 2.814005, Val loss: 2.732029, time: 0 s\n",
      "saved 22.04.2023_16.11.22.517361_epoch_86_loss_2.7314\n",
      "Epoch: 86 lr: 0.000167; Train loss: 2.813764, Val loss: 2.731412, time: 0 s\n",
      "saved 22.04.2023_16.11.22.957119_epoch_87_loss_2.7309\n",
      "Epoch: 87 lr: 0.000167; Train loss: 2.811978, Val loss: 2.730903, time: 0 s\n",
      "saved 22.04.2023_16.11.23.439172_epoch_88_loss_2.7303\n",
      "Epoch: 88 lr: 0.000167; Train loss: 2.811509, Val loss: 2.730264, time: 0 s\n",
      "saved 22.04.2023_16.11.23.851941_epoch_89_loss_2.7296\n",
      "Epoch: 89 lr: 0.000150; Train loss: 2.810765, Val loss: 2.729620, time: 0 s\n",
      "saved 22.04.2023_16.11.24.328298_epoch_90_loss_2.7291\n",
      "Epoch: 90 lr: 0.000150; Train loss: 2.809922, Val loss: 2.729112, time: 0 s\n",
      "saved 22.04.2023_16.11.24.831118_epoch_91_loss_2.7285\n",
      "Epoch: 91 lr: 0.000150; Train loss: 2.808713, Val loss: 2.728539, time: 1 s\n",
      "saved 22.04.2023_16.11.25.270153_epoch_92_loss_2.728\n",
      "Epoch: 92 lr: 0.000150; Train loss: 2.808393, Val loss: 2.728013, time: 0 s\n",
      "saved 22.04.2023_16.11.25.793310_epoch_93_loss_2.7274\n",
      "Epoch: 93 lr: 0.000150; Train loss: 2.808282, Val loss: 2.727432, time: 1 s\n",
      "saved 22.04.2023_16.11.26.217556_epoch_94_loss_2.7269\n",
      "Epoch: 94 lr: 0.000135; Train loss: 2.807591, Val loss: 2.726876, time: 0 s\n",
      "saved 22.04.2023_16.11.26.712113_epoch_95_loss_2.7264\n",
      "Epoch: 95 lr: 0.000135; Train loss: 2.807644, Val loss: 2.726382, time: 0 s\n",
      "saved 22.04.2023_16.11.27.164923_epoch_96_loss_2.7259\n",
      "Epoch: 96 lr: 0.000135; Train loss: 2.807131, Val loss: 2.725914, time: 0 s\n",
      "saved 22.04.2023_16.11.27.693556_epoch_97_loss_2.7255\n",
      "Epoch: 97 lr: 0.000135; Train loss: 2.806120, Val loss: 2.725472, time: 1 s\n",
      "saved 22.04.2023_16.11.28.140442_epoch_98_loss_2.725\n",
      "Epoch: 98 lr: 0.000135; Train loss: 2.805503, Val loss: 2.724963, time: 0 s\n",
      "saved 22.04.2023_16.11.28.618732_epoch_99_loss_2.7244\n",
      "Epoch: 99 lr: 0.000122; Train loss: 2.806072, Val loss: 2.724449, time: 0 s\n",
      "saved 22.04.2023_16.11.29.042834_epoch_100_loss_2.724\n",
      "Epoch: 100 lr: 0.000122; Train loss: 2.803696, Val loss: 2.724019, time: 0 s\n",
      "saved 22.04.2023_16.11.29.532758_epoch_101_loss_2.7236\n",
      "Epoch: 101 lr: 0.000122; Train loss: 2.803924, Val loss: 2.723609, time: 0 s\n",
      "saved 22.04.2023_16.11.29.990655_epoch_102_loss_2.7232\n",
      "Epoch: 102 lr: 0.000122; Train loss: 2.803139, Val loss: 2.723221, time: 0 s\n",
      "saved 22.04.2023_16.11.30.464236_epoch_103_loss_2.7228\n",
      "Epoch: 103 lr: 0.000122; Train loss: 2.802875, Val loss: 2.722797, time: 0 s\n",
      "saved 22.04.2023_16.11.30.881972_epoch_104_loss_2.7224\n",
      "Epoch: 104 lr: 0.000109; Train loss: 2.802908, Val loss: 2.722356, time: 0 s\n",
      "saved 22.04.2023_16.11.31.363434_epoch_105_loss_2.722\n",
      "Epoch: 105 lr: 0.000109; Train loss: 2.801928, Val loss: 2.722000, time: 0 s\n",
      "saved 22.04.2023_16.11.31.771153_epoch_106_loss_2.7216\n",
      "Epoch: 106 lr: 0.000109; Train loss: 2.801207, Val loss: 2.721635, time: 0 s\n",
      "saved 22.04.2023_16.11.32.262527_epoch_107_loss_2.7213\n",
      "Epoch: 107 lr: 0.000109; Train loss: 2.800590, Val loss: 2.721300, time: 0 s\n",
      "saved 22.04.2023_16.11.32.684530_epoch_108_loss_2.7209\n",
      "Epoch: 108 lr: 0.000109; Train loss: 2.801320, Val loss: 2.720881, time: 0 s\n",
      "saved 22.04.2023_16.11.33.155243_epoch_109_loss_2.7204\n",
      "Epoch: 109 lr: 0.000098; Train loss: 2.799819, Val loss: 2.720446, time: 0 s\n",
      "saved 22.04.2023_16.11.33.629404_epoch_110_loss_2.7202\n",
      "Epoch: 110 lr: 0.000098; Train loss: 2.799424, Val loss: 2.720175, time: 0 s\n",
      "saved 22.04.2023_16.11.34.046054_epoch_111_loss_2.7198\n",
      "Epoch: 111 lr: 0.000098; Train loss: 2.799890, Val loss: 2.719840, time: 0 s\n",
      "saved 22.04.2023_16.11.34.519665_epoch_112_loss_2.7195\n",
      "Epoch: 112 lr: 0.000098; Train loss: 2.799415, Val loss: 2.719492, time: 0 s\n",
      "saved 22.04.2023_16.11.34.932925_epoch_113_loss_2.7192\n",
      "Epoch: 113 lr: 0.000098; Train loss: 2.798482, Val loss: 2.719154, time: 0 s\n",
      "saved 22.04.2023_16.11.35.400463_epoch_114_loss_2.7188\n",
      "Epoch: 114 lr: 0.000089; Train loss: 2.797840, Val loss: 2.718827, time: 0 s\n",
      "saved 22.04.2023_16.11.35.817227_epoch_115_loss_2.7185\n",
      "Epoch: 115 lr: 0.000089; Train loss: 2.797110, Val loss: 2.718520, time: 0 s\n",
      "saved 22.04.2023_16.11.36.294929_epoch_116_loss_2.7183\n",
      "Epoch: 116 lr: 0.000089; Train loss: 2.796925, Val loss: 2.718288, time: 0 s\n",
      "saved 22.04.2023_16.11.36.704679_epoch_117_loss_2.7179\n",
      "Epoch: 117 lr: 0.000089; Train loss: 2.796843, Val loss: 2.717949, time: 0 s\n",
      "saved 22.04.2023_16.11.37.175131_epoch_118_loss_2.7177\n",
      "Epoch: 118 lr: 0.000089; Train loss: 2.797322, Val loss: 2.717664, time: 0 s\n",
      "saved 22.04.2023_16.11.37.590548_epoch_119_loss_2.7173\n",
      "Epoch: 119 lr: 0.000080; Train loss: 2.797028, Val loss: 2.717290, time: 0 s\n",
      "saved 22.04.2023_16.11.38.077210_epoch_120_loss_2.717\n",
      "Epoch: 120 lr: 0.000080; Train loss: 2.795137, Val loss: 2.717022, time: 0 s\n",
      "saved 22.04.2023_16.11.38.497335_epoch_121_loss_2.7168\n",
      "Epoch: 121 lr: 0.000080; Train loss: 2.795605, Val loss: 2.716759, time: 0 s\n",
      "saved 22.04.2023_16.11.38.972558_epoch_122_loss_2.7165\n",
      "Epoch: 122 lr: 0.000080; Train loss: 2.795229, Val loss: 2.716479, time: 0 s\n",
      "saved 22.04.2023_16.11.39.382617_epoch_123_loss_2.7162\n",
      "Epoch: 123 lr: 0.000080; Train loss: 2.795300, Val loss: 2.716210, time: 0 s\n",
      "saved 22.04.2023_16.11.39.852465_epoch_124_loss_2.716\n",
      "Epoch: 124 lr: 0.000072; Train loss: 2.795542, Val loss: 2.715983, time: 0 s\n",
      "saved 22.04.2023_16.11.40.257976_epoch_125_loss_2.7158\n",
      "Epoch: 125 lr: 0.000072; Train loss: 2.795398, Val loss: 2.715750, time: 0 s\n",
      "saved 22.04.2023_16.11.40.755935_epoch_126_loss_2.7155\n",
      "Epoch: 126 lr: 0.000072; Train loss: 2.794291, Val loss: 2.715545, time: 0 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 22.04.2023_16.11.41.169185_epoch_127_loss_2.7153\n",
      "Epoch: 127 lr: 0.000072; Train loss: 2.794258, Val loss: 2.715323, time: 0 s\n",
      "saved 22.04.2023_16.11.41.640626_epoch_128_loss_2.7151\n",
      "Epoch: 128 lr: 0.000072; Train loss: 2.794327, Val loss: 2.715080, time: 0 s\n",
      "saved 22.04.2023_16.11.42.111107_epoch_129_loss_2.7148\n",
      "Epoch: 129 lr: 0.000065; Train loss: 2.793468, Val loss: 2.714810, time: 0 s\n",
      "saved 22.04.2023_16.11.42.521898_epoch_130_loss_2.7145\n",
      "Epoch: 130 lr: 0.000065; Train loss: 2.794171, Val loss: 2.714540, time: 0 s\n",
      "saved 22.04.2023_16.11.43.010621_epoch_131_loss_2.7143\n",
      "Epoch: 131 lr: 0.000065; Train loss: 2.794532, Val loss: 2.714257, time: 0 s\n",
      "saved 22.04.2023_16.11.43.424072_epoch_132_loss_2.714\n",
      "Epoch: 132 lr: 0.000065; Train loss: 2.793651, Val loss: 2.714041, time: 0 s\n",
      "saved 22.04.2023_16.11.43.911386_epoch_133_loss_2.7138\n",
      "Epoch: 133 lr: 0.000065; Train loss: 2.792593, Val loss: 2.713812, time: 0 s\n",
      "saved 22.04.2023_16.11.44.332935_epoch_134_loss_2.7135\n",
      "Epoch: 134 lr: 0.000058; Train loss: 2.793148, Val loss: 2.713527, time: 0 s\n",
      "saved 22.04.2023_16.11.44.806473_epoch_135_loss_2.7133\n",
      "Epoch: 135 lr: 0.000058; Train loss: 2.791120, Val loss: 2.713338, time: 0 s\n",
      "saved 22.04.2023_16.11.45.214538_epoch_136_loss_2.7131\n",
      "Epoch: 136 lr: 0.000058; Train loss: 2.792781, Val loss: 2.713079, time: 0 s\n",
      "saved 22.04.2023_16.11.45.682455_epoch_137_loss_2.7129\n",
      "Epoch: 137 lr: 0.000058; Train loss: 2.792350, Val loss: 2.712915, time: 0 s\n",
      "saved 22.04.2023_16.11.46.086141_epoch_138_loss_2.7127\n",
      "Epoch: 138 lr: 0.000058; Train loss: 2.791269, Val loss: 2.712700, time: 0 s\n",
      "saved 22.04.2023_16.11.46.574975_epoch_139_loss_2.7125\n",
      "Epoch: 139 lr: 0.000052; Train loss: 2.791893, Val loss: 2.712529, time: 0 s\n",
      "saved 22.04.2023_16.11.47.012452_epoch_140_loss_2.7124\n",
      "Epoch: 140 lr: 0.000052; Train loss: 2.791695, Val loss: 2.712410, time: 0 s\n",
      "saved 22.04.2023_16.11.47.482145_epoch_141_loss_2.7122\n",
      "Epoch: 141 lr: 0.000052; Train loss: 2.790726, Val loss: 2.712220, time: 0 s\n",
      "saved 22.04.2023_16.11.47.911691_epoch_142_loss_2.712\n",
      "Epoch: 142 lr: 0.000052; Train loss: 2.790318, Val loss: 2.712038, time: 0 s\n",
      "saved 22.04.2023_16.11.48.423953_epoch_143_loss_2.7119\n",
      "Epoch: 143 lr: 0.000052; Train loss: 2.790206, Val loss: 2.711870, time: 1 s\n",
      "saved 22.04.2023_16.11.48.868750_epoch_144_loss_2.7117\n",
      "Epoch: 144 lr: 0.000047; Train loss: 2.790753, Val loss: 2.711728, time: 0 s\n",
      "saved 22.04.2023_16.11.49.371080_epoch_145_loss_2.7116\n",
      "Epoch: 145 lr: 0.000047; Train loss: 2.790892, Val loss: 2.711561, time: 1 s\n",
      "saved 22.04.2023_16.11.49.795770_epoch_146_loss_2.7114\n",
      "Epoch: 146 lr: 0.000047; Train loss: 2.789688, Val loss: 2.711429, time: 0 s\n",
      "saved 22.04.2023_16.11.50.278764_epoch_147_loss_2.7113\n",
      "Epoch: 147 lr: 0.000047; Train loss: 2.789695, Val loss: 2.711289, time: 0 s\n",
      "saved 22.04.2023_16.11.50.771896_epoch_148_loss_2.7111\n",
      "Epoch: 148 lr: 0.000047; Train loss: 2.789959, Val loss: 2.711105, time: 0 s\n",
      "saved 22.04.2023_16.11.51.180627_epoch_149_loss_2.711\n",
      "Epoch: 149 lr: 0.000042; Train loss: 2.789592, Val loss: 2.710972, time: 0 s\n",
      "saved 22.04.2023_16.11.51.657312_epoch_150_loss_2.7109\n",
      "Epoch: 150 lr: 0.000042; Train loss: 2.788881, Val loss: 2.710869, time: 0 s\n",
      "saved 22.04.2023_16.11.52.077176_epoch_151_loss_2.7107\n",
      "Epoch: 151 lr: 0.000042; Train loss: 2.790009, Val loss: 2.710686, time: 0 s\n",
      "saved 22.04.2023_16.11.52.562939_epoch_152_loss_2.7106\n",
      "Epoch: 152 lr: 0.000042; Train loss: 2.787288, Val loss: 2.710566, time: 0 s\n",
      "saved 22.04.2023_16.11.52.988168_epoch_153_loss_2.7104\n",
      "Epoch: 153 lr: 0.000042; Train loss: 2.789536, Val loss: 2.710428, time: 0 s\n",
      "saved 22.04.2023_16.11.53.463924_epoch_154_loss_2.7103\n",
      "Epoch: 154 lr: 0.000038; Train loss: 2.788821, Val loss: 2.710263, time: 0 s\n",
      "saved 22.04.2023_16.11.53.882456_epoch_155_loss_2.7101\n",
      "Epoch: 155 lr: 0.000038; Train loss: 2.788787, Val loss: 2.710125, time: 0 s\n",
      "saved 22.04.2023_16.11.54.352677_epoch_156_loss_2.7099\n",
      "Epoch: 156 lr: 0.000038; Train loss: 2.787699, Val loss: 2.709945, time: 0 s\n",
      "saved 22.04.2023_16.11.54.761582_epoch_157_loss_2.7098\n",
      "Epoch: 157 lr: 0.000038; Train loss: 2.787081, Val loss: 2.709827, time: 0 s\n",
      "saved 22.04.2023_16.11.55.257963_epoch_158_loss_2.7098\n",
      "Epoch: 158 lr: 0.000038; Train loss: 2.787142, Val loss: 2.709761, time: 0 s\n",
      "saved 22.04.2023_16.11.55.676956_epoch_159_loss_2.7096\n",
      "Epoch: 159 lr: 0.000034; Train loss: 2.788440, Val loss: 2.709632, time: 0 s\n",
      "saved 22.04.2023_16.11.56.166804_epoch_160_loss_2.7096\n",
      "Epoch: 160 lr: 0.000034; Train loss: 2.788795, Val loss: 2.709555, time: 0 s\n",
      "saved 22.04.2023_16.11.56.613281_epoch_161_loss_2.7094\n",
      "Epoch: 161 lr: 0.000034; Train loss: 2.787958, Val loss: 2.709447, time: 0 s\n",
      "saved 22.04.2023_16.11.57.116931_epoch_162_loss_2.7093\n",
      "Epoch: 162 lr: 0.000034; Train loss: 2.788374, Val loss: 2.709252, time: 1 s\n",
      "saved 22.04.2023_16.11.57.526403_epoch_163_loss_2.7091\n",
      "Epoch: 163 lr: 0.000034; Train loss: 2.787575, Val loss: 2.709124, time: 0 s\n",
      "saved 22.04.2023_16.11.58.002180_epoch_164_loss_2.709\n",
      "Epoch: 164 lr: 0.000031; Train loss: 2.787541, Val loss: 2.709044, time: 0 s\n",
      "saved 22.04.2023_16.11.58.401569_epoch_165_loss_2.7089\n",
      "Epoch: 165 lr: 0.000031; Train loss: 2.787677, Val loss: 2.708925, time: 0 s\n",
      "saved 22.04.2023_16.11.58.902359_epoch_166_loss_2.7089\n",
      "Epoch: 166 lr: 0.000031; Train loss: 2.787054, Val loss: 2.708908, time: 1 s\n",
      "saved 22.04.2023_16.11.59.404474_epoch_167_loss_2.7088\n",
      "Epoch: 167 lr: 0.000031; Train loss: 2.786729, Val loss: 2.708818, time: 1 s\n",
      "saved 22.04.2023_16.11.59.826847_epoch_168_loss_2.7087\n",
      "Epoch: 168 lr: 0.000031; Train loss: 2.787340, Val loss: 2.708679, time: 0 s\n",
      "saved 22.04.2023_16.12.00.325754_epoch_169_loss_2.7086\n",
      "Epoch: 169 lr: 0.000028; Train loss: 2.785670, Val loss: 2.708614, time: 1 s\n",
      "saved 22.04.2023_16.12.00.745442_epoch_170_loss_2.7085\n",
      "Epoch: 170 lr: 0.000028; Train loss: 2.787791, Val loss: 2.708518, time: 0 s\n",
      "saved 22.04.2023_16.12.01.246365_epoch_171_loss_2.7084\n",
      "Epoch: 171 lr: 0.000028; Train loss: 2.786388, Val loss: 2.708403, time: 1 s\n",
      "saved 22.04.2023_16.12.01.667459_epoch_172_loss_2.7083\n",
      "Epoch: 172 lr: 0.000028; Train loss: 2.786578, Val loss: 2.708335, time: 0 s\n",
      "saved 22.04.2023_16.12.02.153486_epoch_173_loss_2.7083\n",
      "Epoch: 173 lr: 0.000028; Train loss: 2.785438, Val loss: 2.708259, time: 0 s\n",
      "saved 22.04.2023_16.12.02.561136_epoch_174_loss_2.7081\n",
      "Epoch: 174 lr: 0.000025; Train loss: 2.785569, Val loss: 2.708108, time: 0 s\n",
      "saved 22.04.2023_16.12.03.034046_epoch_175_loss_2.7081\n",
      "Epoch: 175 lr: 0.000025; Train loss: 2.786321, Val loss: 2.708071, time: 0 s\n",
      "saved 22.04.2023_16.12.03.437095_epoch_176_loss_2.708\n",
      "Epoch: 176 lr: 0.000025; Train loss: 2.785665, Val loss: 2.707967, time: 0 s\n",
      "saved 22.04.2023_16.12.03.916225_epoch_177_loss_2.7079\n",
      "Epoch: 177 lr: 0.000025; Train loss: 2.785724, Val loss: 2.707872, time: 0 s\n",
      "saved 22.04.2023_16.12.04.317312_epoch_178_loss_2.7078\n",
      "Epoch: 178 lr: 0.000025; Train loss: 2.787054, Val loss: 2.707801, time: 0 s\n",
      "saved 22.04.2023_16.12.04.789598_epoch_179_loss_2.7077\n",
      "Epoch: 179 lr: 0.000023; Train loss: 2.785151, Val loss: 2.707736, time: 0 s\n",
      "saved 22.04.2023_16.12.05.204993_epoch_180_loss_2.7077\n",
      "Epoch: 180 lr: 0.000023; Train loss: 2.784912, Val loss: 2.707679, time: 0 s\n",
      "saved 22.04.2023_16.12.05.688259_epoch_181_loss_2.7076\n",
      "Epoch: 181 lr: 0.000023; Train loss: 2.785095, Val loss: 2.707591, time: 0 s\n",
      "saved 22.04.2023_16.12.06.094183_epoch_182_loss_2.7075\n",
      "Epoch: 182 lr: 0.000023; Train loss: 2.785946, Val loss: 2.707476, time: 0 s\n",
      "saved 22.04.2023_16.12.06.564360_epoch_183_loss_2.7074\n",
      "Epoch: 183 lr: 0.000023; Train loss: 2.785015, Val loss: 2.707393, time: 0 s\n",
      "saved 22.04.2023_16.12.06.983897_epoch_184_loss_2.7073\n",
      "Epoch: 184 lr: 0.000020; Train loss: 2.784765, Val loss: 2.707312, time: 0 s\n",
      "saved 22.04.2023_16.12.07.450233_epoch_185_loss_2.7073\n",
      "Epoch: 185 lr: 0.000020; Train loss: 2.784679, Val loss: 2.707292, time: 0 s\n",
      "saved 22.04.2023_16.12.07.932915_epoch_186_loss_2.7073\n",
      "Epoch: 186 lr: 0.000020; Train loss: 2.785180, Val loss: 2.707254, time: 0 s\n",
      "saved 22.04.2023_16.12.08.350504_epoch_187_loss_2.7072\n",
      "Epoch: 187 lr: 0.000020; Train loss: 2.784927, Val loss: 2.707154, time: 0 s\n",
      "saved 22.04.2023_16.12.08.844387_epoch_188_loss_2.7071\n",
      "Epoch: 188 lr: 0.000020; Train loss: 2.784916, Val loss: 2.707110, time: 0 s\n",
      "saved 22.04.2023_16.12.09.266602_epoch_189_loss_2.707\n",
      "Epoch: 189 lr: 0.000018; Train loss: 2.786530, Val loss: 2.707025, time: 0 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 22.04.2023_16.12.09.779800_epoch_190_loss_2.707\n",
      "Epoch: 190 lr: 0.000018; Train loss: 2.784968, Val loss: 2.707000, time: 1 s\n",
      "saved 22.04.2023_16.12.10.195579_epoch_191_loss_2.7069\n",
      "Epoch: 191 lr: 0.000018; Train loss: 2.785179, Val loss: 2.706905, time: 0 s\n",
      "saved 22.04.2023_16.12.10.681832_epoch_192_loss_2.7069\n",
      "Epoch: 192 lr: 0.000018; Train loss: 2.784698, Val loss: 2.706864, time: 0 s\n",
      "saved 22.04.2023_16.12.11.109872_epoch_193_loss_2.7068\n",
      "Epoch: 193 lr: 0.000018; Train loss: 2.784525, Val loss: 2.706815, time: 0 s\n",
      "saved 22.04.2023_16.12.11.594765_epoch_194_loss_2.7067\n",
      "Epoch: 194 lr: 0.000016; Train loss: 2.785041, Val loss: 2.706749, time: 0 s\n",
      "saved 22.04.2023_16.12.12.012011_epoch_195_loss_2.7067\n",
      "Epoch: 195 lr: 0.000016; Train loss: 2.784603, Val loss: 2.706663, time: 0 s\n",
      "saved 22.04.2023_16.12.12.486547_epoch_196_loss_2.7066\n",
      "Epoch: 196 lr: 0.000016; Train loss: 2.784819, Val loss: 2.706562, time: 0 s\n",
      "saved 22.04.2023_16.12.12.911680_epoch_197_loss_2.7065\n",
      "Epoch: 197 lr: 0.000016; Train loss: 2.784922, Val loss: 2.706478, time: 0 s\n",
      "saved 22.04.2023_16.12.13.388935_epoch_198_loss_2.7064\n",
      "Epoch: 198 lr: 0.000016; Train loss: 2.784723, Val loss: 2.706428, time: 0 s\n",
      "saved 22.04.2023_16.12.13.801226_epoch_199_loss_2.7064\n",
      "Epoch: 199 lr: 0.000015; Train loss: 2.783490, Val loss: 2.706378, time: 0 s\n",
      "saved 22.04.2023_16.12.14.274958_epoch_200_loss_2.7063\n",
      "Epoch: 200 lr: 0.000015; Train loss: 2.783716, Val loss: 2.706339, time: 0 s\n",
      "saved 22.04.2023_16.12.14.678519_epoch_201_loss_2.7063\n",
      "Epoch: 201 lr: 0.000015; Train loss: 2.784216, Val loss: 2.706299, time: 0 s\n",
      "saved 22.04.2023_16.12.15.163279_epoch_202_loss_2.7063\n",
      "Epoch: 202 lr: 0.000015; Train loss: 2.783802, Val loss: 2.706263, time: 0 s\n",
      "Epoch: 203 lr: 0.000015; Train loss: 2.783684, Val loss: 2.706269, time: 0 s\n",
      "saved 22.04.2023_16.12.16.051881_epoch_204_loss_2.7062\n",
      "Epoch: 204 lr: 0.000013; Train loss: 2.784181, Val loss: 2.706151, time: 0 s\n",
      "saved 22.04.2023_16.12.16.521255_epoch_205_loss_2.7061\n",
      "Epoch: 205 lr: 0.000013; Train loss: 2.782652, Val loss: 2.706098, time: 0 s\n",
      "Epoch: 206 lr: 0.000013; Train loss: 2.784300, Val loss: 2.706121, time: 0 s\n",
      "saved 22.04.2023_16.12.17.420328_epoch_207_loss_2.7061\n",
      "Epoch: 207 lr: 0.000013; Train loss: 2.783708, Val loss: 2.706066, time: 0 s\n",
      "Epoch: 208 lr: 0.000013; Train loss: 2.783790, Val loss: 2.706068, time: 0 s\n",
      "saved 22.04.2023_16.12.18.297107_epoch_209_loss_2.706\n",
      "Epoch: 209 lr: 0.000012; Train loss: 2.783665, Val loss: 2.705986, time: 0 s\n",
      "saved 22.04.2023_16.12.18.704446_epoch_210_loss_2.706\n",
      "Epoch: 210 lr: 0.000012; Train loss: 2.784050, Val loss: 2.705955, time: 0 s\n",
      "saved 22.04.2023_16.12.19.183197_epoch_211_loss_2.7059\n",
      "Epoch: 211 lr: 0.000012; Train loss: 2.782517, Val loss: 2.705949, time: 0 s\n",
      "saved 22.04.2023_16.12.19.593273_epoch_212_loss_2.7059\n",
      "Epoch: 212 lr: 0.000012; Train loss: 2.783641, Val loss: 2.705855, time: 0 s\n",
      "saved 22.04.2023_16.12.20.076361_epoch_213_loss_2.7058\n",
      "Epoch: 213 lr: 0.000012; Train loss: 2.783922, Val loss: 2.705806, time: 0 s\n",
      "saved 22.04.2023_16.12.20.480764_epoch_214_loss_2.7057\n",
      "Epoch: 214 lr: 0.000011; Train loss: 2.783664, Val loss: 2.705750, time: 0 s\n",
      "saved 22.04.2023_16.12.20.957488_epoch_215_loss_2.7057\n",
      "Epoch: 215 lr: 0.000011; Train loss: 2.782049, Val loss: 2.705731, time: 0 s\n",
      "saved 22.04.2023_16.12.21.353841_epoch_216_loss_2.7057\n",
      "Epoch: 216 lr: 0.000011; Train loss: 2.783406, Val loss: 2.705690, time: 0 s\n",
      "saved 22.04.2023_16.12.21.829807_epoch_217_loss_2.7057\n",
      "Epoch: 217 lr: 0.000011; Train loss: 2.783175, Val loss: 2.705654, time: 0 s\n",
      "Epoch: 218 lr: 0.000011; Train loss: 2.782194, Val loss: 2.705689, time: 0 s\n",
      "Epoch: 219 lr: 0.000010; Train loss: 2.783279, Val loss: 2.705683, time: 0 s\n",
      "saved 22.04.2023_16.12.23.102554_epoch_220_loss_2.7056\n",
      "Epoch: 220 lr: 0.000010; Train loss: 2.783261, Val loss: 2.705607, time: 0 s\n",
      "Epoch: 221 lr: 0.000010; Train loss: 2.782254, Val loss: 2.705623, time: 0 s\n",
      "Epoch: 222 lr: 0.000010; Train loss: 2.783505, Val loss: 2.705609, time: 0 s\n",
      "saved 22.04.2023_16.12.24.533072_epoch_223_loss_2.7056\n",
      "Epoch: 223 lr: 0.000010; Train loss: 2.782493, Val loss: 2.705569, time: 0 s\n",
      "saved 22.04.2023_16.12.25.006842_epoch_224_loss_2.7055\n",
      "Epoch: 224 lr: 0.000009; Train loss: 2.783726, Val loss: 2.705486, time: 0 s\n",
      "saved 22.04.2023_16.12.25.472584_epoch_225_loss_2.7054\n",
      "Epoch: 225 lr: 0.000009; Train loss: 2.783336, Val loss: 2.705419, time: 0 s\n",
      "saved 22.04.2023_16.12.25.953895_epoch_226_loss_2.7054\n",
      "Epoch: 226 lr: 0.000009; Train loss: 2.782464, Val loss: 2.705391, time: 0 s\n",
      "saved 22.04.2023_16.12.26.397243_epoch_227_loss_2.7053\n",
      "Epoch: 227 lr: 0.000009; Train loss: 2.782204, Val loss: 2.705314, time: 0 s\n",
      "saved 22.04.2023_16.12.26.867820_epoch_228_loss_2.7053\n",
      "Epoch: 228 lr: 0.000009; Train loss: 2.783637, Val loss: 2.705263, time: 0 s\n",
      "saved 22.04.2023_16.12.27.281083_epoch_229_loss_2.7052\n",
      "Epoch: 229 lr: 0.000008; Train loss: 2.783307, Val loss: 2.705214, time: 0 s\n",
      "Epoch: 230 lr: 0.000008; Train loss: 2.782302, Val loss: 2.705219, time: 1 s\n",
      "saved 22.04.2023_16.12.28.213412_epoch_231_loss_2.7051\n",
      "Epoch: 231 lr: 0.000008; Train loss: 2.783326, Val loss: 2.705140, time: 0 s\n",
      "saved 22.04.2023_16.12.28.696932_epoch_232_loss_2.7051\n",
      "Epoch: 232 lr: 0.000008; Train loss: 2.782157, Val loss: 2.705130, time: 0 s\n",
      "saved 22.04.2023_16.12.29.122293_epoch_233_loss_2.7051\n",
      "Epoch: 233 lr: 0.000008; Train loss: 2.782236, Val loss: 2.705123, time: 0 s\n",
      "saved 22.04.2023_16.12.29.595081_epoch_234_loss_2.7051\n",
      "Epoch: 234 lr: 0.000007; Train loss: 2.782994, Val loss: 2.705114, time: 0 s\n",
      "saved 22.04.2023_16.12.30.055933_epoch_235_loss_2.7051\n",
      "Epoch: 235 lr: 0.000007; Train loss: 2.783114, Val loss: 2.705078, time: 0 s\n",
      "Epoch: 236 lr: 0.000007; Train loss: 2.782191, Val loss: 2.705086, time: 0 s\n",
      "Epoch: 237 lr: 0.000007; Train loss: 2.782052, Val loss: 2.705080, time: 0 s\n",
      "Epoch: 238 lr: 0.000007; Train loss: 2.782813, Val loss: 2.705079, time: 0 s\n",
      "saved 22.04.2023_16.12.31.888524_epoch_239_loss_2.705\n",
      "Epoch: 239 lr: 0.000006; Train loss: 2.781872, Val loss: 2.705009, time: 0 s\n",
      "saved 22.04.2023_16.12.32.382148_epoch_240_loss_2.7049\n",
      "Epoch: 240 lr: 0.000006; Train loss: 2.784054, Val loss: 2.704919, time: 0 s\n",
      "saved 22.04.2023_16.12.32.848314_epoch_241_loss_2.7049\n",
      "Epoch: 241 lr: 0.000006; Train loss: 2.782359, Val loss: 2.704874, time: 0 s\n",
      "saved 22.04.2023_16.12.33.319369_epoch_242_loss_2.7049\n",
      "Epoch: 242 lr: 0.000006; Train loss: 2.782622, Val loss: 2.704870, time: 0 s\n",
      "saved 22.04.2023_16.12.33.809778_epoch_243_loss_2.7048\n",
      "Epoch: 243 lr: 0.000006; Train loss: 2.782908, Val loss: 2.704792, time: 0 s\n",
      "saved 22.04.2023_16.12.34.232617_epoch_244_loss_2.7048\n",
      "Epoch: 244 lr: 0.000006; Train loss: 2.781934, Val loss: 2.704753, time: 0 s\n",
      "Epoch: 245 lr: 0.000006; Train loss: 2.781587, Val loss: 2.704771, time: 1 s\n",
      "saved 22.04.2023_16.12.35.175637_epoch_246_loss_2.7047\n",
      "Epoch: 246 lr: 0.000006; Train loss: 2.782453, Val loss: 2.704746, time: 0 s\n",
      "Epoch: 247 lr: 0.000006; Train loss: 2.782323, Val loss: 2.704797, time: 0 s\n",
      "Epoch: 248 lr: 0.000006; Train loss: 2.782632, Val loss: 2.704798, time: 0 s\n",
      "Epoch: 249 lr: 0.000005; Train loss: 2.781806, Val loss: 2.704765, time: 0 s\n",
      "saved 22.04.2023_16.12.37.025522_epoch_250_loss_2.7047\n",
      "Epoch: 250 lr: 0.000005; Train loss: 2.782143, Val loss: 2.704745, time: 0 s\n",
      "Epoch: 251 lr: 0.000005; Train loss: 2.781687, Val loss: 2.704766, time: 0 s\n",
      "saved 22.04.2023_16.12.37.918751_epoch_252_loss_2.7047\n",
      "Epoch: 252 lr: 0.000005; Train loss: 2.781628, Val loss: 2.704692, time: 0 s\n",
      "saved 22.04.2023_16.12.38.428442_epoch_253_loss_2.7047\n",
      "Epoch: 253 lr: 0.000005; Train loss: 2.781935, Val loss: 2.704659, time: 1 s\n",
      "Epoch: 254 lr: 0.000005; Train loss: 2.781656, Val loss: 2.704659, time: 0 s\n",
      "saved 22.04.2023_16.12.39.353344_epoch_255_loss_2.7046\n",
      "Epoch: 255 lr: 0.000005; Train loss: 2.781880, Val loss: 2.704627, time: 0 s\n",
      "Epoch: 256 lr: 0.000005; Train loss: 2.782192, Val loss: 2.704641, time: 0 s\n",
      "saved 22.04.2023_16.12.40.241781_epoch_257_loss_2.7046\n",
      "Epoch: 257 lr: 0.000005; Train loss: 2.782026, Val loss: 2.704593, time: 0 s\n",
      "saved 22.04.2023_16.12.40.651595_epoch_258_loss_2.7045\n",
      "Epoch: 258 lr: 0.000005; Train loss: 2.782172, Val loss: 2.704542, time: 0 s\n",
      "saved 22.04.2023_16.12.41.132988_epoch_259_loss_2.7045\n",
      "Epoch: 259 lr: 0.000004; Train loss: 2.782725, Val loss: 2.704533, time: 0 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 22.04.2023_16.12.41.565966_epoch_260_loss_2.7045\n",
      "Epoch: 260 lr: 0.000004; Train loss: 2.782854, Val loss: 2.704514, time: 0 s\n",
      "saved 22.04.2023_16.12.42.055240_epoch_261_loss_2.7045\n",
      "Epoch: 261 lr: 0.000004; Train loss: 2.782440, Val loss: 2.704500, time: 0 s\n",
      "saved 22.04.2023_16.12.42.526809_epoch_262_loss_2.7045\n",
      "Epoch: 262 lr: 0.000004; Train loss: 2.782866, Val loss: 2.704453, time: 0 s\n",
      "Epoch: 263 lr: 0.000004; Train loss: 2.781248, Val loss: 2.704464, time: 0 s\n",
      "Epoch: 264 lr: 0.000004; Train loss: 2.781375, Val loss: 2.704480, time: 0 s\n",
      "Epoch: 265 lr: 0.000004; Train loss: 2.780619, Val loss: 2.704551, time: 0 s\n",
      "Epoch: 266 lr: 0.000004; Train loss: 2.782570, Val loss: 2.704477, time: 0 s\n",
      "Epoch: 267 lr: 0.000004; Train loss: 2.781648, Val loss: 2.704466, time: 0 s\n",
      "Epoch: 268 lr: 0.000004; Train loss: 2.781392, Val loss: 2.704535, time: 0 s\n",
      "Epoch: 269 lr: 0.000003; Train loss: 2.781066, Val loss: 2.704594, time: 0 s\n",
      "Epoch: 270 lr: 0.000003; Train loss: 2.781947, Val loss: 2.704611, time: 0 s\n",
      "Epoch: 271 lr: 0.000003; Train loss: 2.781339, Val loss: 2.704600, time: 0 s\n",
      "Epoch: 272 lr: 0.000003; Train loss: 2.781175, Val loss: 2.704626, time: 1 s\n",
      "Epoch: 273 lr: 0.000003; Train loss: 2.781245, Val loss: 2.704606, time: 0 s\n",
      "Epoch: 274 lr: 0.000003; Train loss: 2.781071, Val loss: 2.704588, time: 0 s\n",
      "Epoch: 275 lr: 0.000003; Train loss: 2.781970, Val loss: 2.704545, time: 0 s\n",
      "Epoch: 276 lr: 0.000003; Train loss: 2.782776, Val loss: 2.704524, time: 0 s\n",
      "saved 22.04.2023_16.12.49.449362_epoch_277_loss_2.7044\n",
      "Epoch: 277 lr: 0.000003; Train loss: 2.781753, Val loss: 2.704445, time: 0 s\n",
      "saved 22.04.2023_16.12.49.932655_epoch_278_loss_2.7044\n",
      "Epoch: 278 lr: 0.000003; Train loss: 2.782452, Val loss: 2.704416, time: 0 s\n",
      "saved 22.04.2023_16.12.50.361691_epoch_279_loss_2.7044\n",
      "Epoch: 279 lr: 0.000003; Train loss: 2.780415, Val loss: 2.704387, time: 0 s\n",
      "saved 22.04.2023_16.12.50.869123_epoch_280_loss_2.7044\n",
      "Epoch: 280 lr: 0.000003; Train loss: 2.781940, Val loss: 2.704355, time: 1 s\n",
      "saved 22.04.2023_16.12.51.376789_epoch_281_loss_2.7043\n",
      "Epoch: 281 lr: 0.000003; Train loss: 2.781887, Val loss: 2.704338, time: 1 s\n",
      "saved 22.04.2023_16.12.51.791262_epoch_282_loss_2.7043\n",
      "Epoch: 282 lr: 0.000003; Train loss: 2.781348, Val loss: 2.704276, time: 0 s\n",
      "Epoch: 283 lr: 0.000003; Train loss: 2.781332, Val loss: 2.704305, time: 0 s\n",
      "saved 22.04.2023_16.12.52.687900_epoch_284_loss_2.7043\n",
      "Epoch: 284 lr: 0.000002; Train loss: 2.781255, Val loss: 2.704270, time: 0 s\n",
      "Epoch: 285 lr: 0.000002; Train loss: 2.782115, Val loss: 2.704270, time: 0 s\n",
      "saved 22.04.2023_16.12.53.583114_epoch_286_loss_2.7042\n",
      "Epoch: 286 lr: 0.000002; Train loss: 2.782094, Val loss: 2.704244, time: 0 s\n",
      "Epoch: 287 lr: 0.000002; Train loss: 2.780853, Val loss: 2.704269, time: 0 s\n",
      "saved 22.04.2023_16.12.54.512208_epoch_288_loss_2.7042\n",
      "Epoch: 288 lr: 0.000002; Train loss: 2.781642, Val loss: 2.704241, time: 0 s\n",
      "Epoch: 289 lr: 0.000002; Train loss: 2.781756, Val loss: 2.704242, time: 1 s\n",
      "saved 22.04.2023_16.12.55.474863_epoch_290_loss_2.7042\n",
      "Epoch: 290 lr: 0.000002; Train loss: 2.780908, Val loss: 2.704199, time: 0 s\n",
      "saved 22.04.2023_16.12.55.968793_epoch_291_loss_2.7041\n",
      "Epoch: 291 lr: 0.000002; Train loss: 2.782491, Val loss: 2.704147, time: 0 s\n",
      "Epoch: 292 lr: 0.000002; Train loss: 2.781508, Val loss: 2.704147, time: 0 s\n",
      "saved 22.04.2023_16.12.56.858692_epoch_293_loss_2.7041\n",
      "Epoch: 293 lr: 0.000002; Train loss: 2.782185, Val loss: 2.704133, time: 0 s\n",
      "saved 22.04.2023_16.12.57.289361_epoch_294_loss_2.7041\n",
      "Epoch: 294 lr: 0.000002; Train loss: 2.781410, Val loss: 2.704129, time: 0 s\n",
      "saved 22.04.2023_16.12.57.759303_epoch_295_loss_2.7041\n",
      "Epoch: 295 lr: 0.000002; Train loss: 2.781836, Val loss: 2.704126, time: 0 s\n",
      "saved 22.04.2023_16.12.58.177995_epoch_296_loss_2.7041\n",
      "Epoch: 296 lr: 0.000002; Train loss: 2.781900, Val loss: 2.704121, time: 0 s\n",
      "saved 22.04.2023_16.12.58.654785_epoch_297_loss_2.7041\n",
      "Epoch: 297 lr: 0.000002; Train loss: 2.780786, Val loss: 2.704099, time: 0 s\n",
      "saved 22.04.2023_16.12.59.078729_epoch_298_loss_2.7041\n",
      "Epoch: 298 lr: 0.000002; Train loss: 2.781638, Val loss: 2.704090, time: 0 s\n",
      "Epoch: 299 lr: 0.000002; Train loss: 2.781019, Val loss: 2.704093, time: 0 s\n",
      "Epoch: 300 lr: 0.000002; Train loss: 2.781141, Val loss: 2.704125, time: 0 s\n",
      "Epoch: 301 lr: 0.000002; Train loss: 2.782100, Val loss: 2.704125, time: 0 s\n",
      "Epoch: 302 lr: 0.000002; Train loss: 2.780484, Val loss: 2.704170, time: 1 s\n",
      "Epoch: 303 lr: 0.000002; Train loss: 2.781124, Val loss: 2.704166, time: 0 s\n",
      "Epoch: 304 lr: 0.000002; Train loss: 2.781792, Val loss: 2.704174, time: 1 s\n",
      "Epoch: 305 lr: 0.000002; Train loss: 2.780768, Val loss: 2.704224, time: 0 s\n",
      "Epoch: 306 lr: 0.000002; Train loss: 2.782470, Val loss: 2.704184, time: 1 s\n",
      "Epoch: 307 lr: 0.000002; Train loss: 2.782689, Val loss: 2.704187, time: 0 s\n",
      "Epoch: 308 lr: 0.000002; Train loss: 2.781844, Val loss: 2.704148, time: 0 s\n",
      "Epoch: 309 lr: 0.000001; Train loss: 2.781425, Val loss: 2.704141, time: 0 s\n",
      "Epoch: 310 lr: 0.000001; Train loss: 2.781577, Val loss: 2.704131, time: 0 s\n",
      "Epoch: 311 lr: 0.000001; Train loss: 2.781625, Val loss: 2.704143, time: 0 s\n",
      "Epoch: 312 lr: 0.000001; Train loss: 2.782040, Val loss: 2.704156, time: 0 s\n",
      "Epoch: 313 lr: 0.000001; Train loss: 2.781390, Val loss: 2.704095, time: 0 s\n",
      "Epoch: 314 lr: 0.000001; Train loss: 2.780868, Val loss: 2.704097, time: 0 s\n",
      "Epoch: 315 lr: 0.000001; Train loss: 2.781072, Val loss: 2.704103, time: 0 s\n",
      "saved 22.04.2023_16.13.07.347094_epoch_316_loss_2.7041\n",
      "Epoch: 316 lr: 0.000001; Train loss: 2.781324, Val loss: 2.704069, time: 0 s\n",
      "saved 22.04.2023_16.13.07.758416_epoch_317_loss_2.704\n",
      "Epoch: 317 lr: 0.000001; Train loss: 2.781757, Val loss: 2.704011, time: 0 s\n",
      "Epoch: 318 lr: 0.000001; Train loss: 2.782043, Val loss: 2.704020, time: 0 s\n",
      "Epoch: 319 lr: 0.000001; Train loss: 2.781375, Val loss: 2.704098, time: 0 s\n",
      "Epoch: 320 lr: 0.000001; Train loss: 2.781114, Val loss: 2.704086, time: 0 s\n",
      "Epoch: 321 lr: 0.000001; Train loss: 2.781443, Val loss: 2.704114, time: 0 s\n",
      "Epoch: 322 lr: 0.000001; Train loss: 2.781607, Val loss: 2.704095, time: 0 s\n",
      "Epoch: 323 lr: 0.000001; Train loss: 2.782181, Val loss: 2.704093, time: 0 s\n",
      "Epoch: 324 lr: 0.000001; Train loss: 2.781154, Val loss: 2.704098, time: 0 s\n",
      "Epoch: 325 lr: 0.000001; Train loss: 2.780417, Val loss: 2.704070, time: 0 s\n",
      "Epoch: 326 lr: 0.000001; Train loss: 2.781419, Val loss: 2.704119, time: 0 s\n",
      "Epoch: 327 lr: 0.000001; Train loss: 2.781061, Val loss: 2.704144, time: 0 s\n",
      "Epoch: 328 lr: 0.000001; Train loss: 2.781280, Val loss: 2.704109, time: 0 s\n",
      "Epoch: 329 lr: 0.000001; Train loss: 2.781346, Val loss: 2.704075, time: 0 s\n",
      "Epoch: 330 lr: 0.000001; Train loss: 2.781194, Val loss: 2.704055, time: 0 s\n",
      "saved 22.04.2023_16.13.14.206433_epoch_331_loss_2.704\n",
      "Epoch: 331 lr: 0.000001; Train loss: 2.781086, Val loss: 2.704004, time: 1 s\n",
      "saved 22.04.2023_16.13.14.633779_epoch_332_loss_2.704\n",
      "Epoch: 332 lr: 0.000001; Train loss: 2.780966, Val loss: 2.703974, time: 0 s\n",
      "Epoch: 333 lr: 0.000001; Train loss: 2.780879, Val loss: 2.703987, time: 0 s\n",
      "Epoch: 334 lr: 0.000001; Train loss: 2.780970, Val loss: 2.704018, time: 0 s\n",
      "Epoch: 335 lr: 0.000001; Train loss: 2.780951, Val loss: 2.704012, time: 0 s\n",
      "Epoch: 336 lr: 0.000001; Train loss: 2.781390, Val loss: 2.704055, time: 0 s\n",
      "Epoch: 337 lr: 0.000001; Train loss: 2.781307, Val loss: 2.704048, time: 0 s\n",
      "Epoch: 338 lr: 0.000001; Train loss: 2.781766, Val loss: 2.704016, time: 1 s\n",
      "Epoch: 339 lr: 0.000001; Train loss: 2.781456, Val loss: 2.704077, time: 0 s\n",
      "Epoch: 340 lr: 0.000001; Train loss: 2.780819, Val loss: 2.704009, time: 0 s\n",
      "Epoch: 341 lr: 0.000001; Train loss: 2.780352, Val loss: 2.704019, time: 0 s\n",
      "Epoch: 342 lr: 0.000001; Train loss: 2.781335, Val loss: 2.704014, time: 0 s\n",
      "Epoch: 343 lr: 0.000001; Train loss: 2.780319, Val loss: 2.704005, time: 0 s\n",
      "saved 22.04.2023_16.13.20.218858_epoch_344_loss_2.704\n",
      "Epoch: 344 lr: 0.000001; Train loss: 2.781423, Val loss: 2.703956, time: 1 s\n",
      "Epoch: 345 lr: 0.000001; Train loss: 2.781168, Val loss: 2.703995, time: 0 s\n",
      "Epoch: 346 lr: 0.000001; Train loss: 2.781367, Val loss: 2.704032, time: 0 s\n",
      "Epoch: 347 lr: 0.000001; Train loss: 2.781487, Val loss: 2.703973, time: 0 s\n",
      "Epoch: 348 lr: 0.000001; Train loss: 2.780816, Val loss: 2.704057, time: 0 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 349 lr: 0.000001; Train loss: 2.781022, Val loss: 2.704073, time: 0 s\n",
      "Epoch: 350 lr: 0.000001; Train loss: 2.781886, Val loss: 2.704054, time: 0 s\n",
      "Epoch: 351 lr: 0.000001; Train loss: 2.781089, Val loss: 2.704028, time: 0 s\n",
      "Epoch: 352 lr: 0.000001; Train loss: 2.781651, Val loss: 2.704079, time: 0 s\n",
      "Epoch: 353 lr: 0.000001; Train loss: 2.781134, Val loss: 2.704050, time: 0 s\n",
      "Epoch: 354 lr: 0.000001; Train loss: 2.781958, Val loss: 2.704026, time: 0 s\n",
      "Epoch: 355 lr: 0.000001; Train loss: 2.782397, Val loss: 2.704053, time: 0 s\n",
      "Epoch: 356 lr: 0.000001; Train loss: 2.780774, Val loss: 2.704002, time: 1 s\n",
      "Epoch: 357 lr: 0.000001; Train loss: 2.780928, Val loss: 2.704029, time: 0 s\n",
      "Epoch: 358 lr: 0.000001; Train loss: 2.781729, Val loss: 2.704043, time: 0 s\n",
      "Epoch: 359 lr: 0.000001; Train loss: 2.781367, Val loss: 2.704042, time: 0 s\n",
      "Epoch: 360 lr: 0.000001; Train loss: 2.781201, Val loss: 2.704011, time: 0 s\n",
      "Epoch: 361 lr: 0.000001; Train loss: 2.781631, Val loss: 2.703999, time: 0 s\n",
      "Epoch: 362 lr: 0.000001; Train loss: 2.781308, Val loss: 2.703956, time: 0 s\n",
      "saved 22.04.2023_16.13.28.834389_epoch_363_loss_2.7039\n",
      "Epoch: 363 lr: 0.000001; Train loss: 2.781822, Val loss: 2.703917, time: 0 s\n",
      "saved 22.04.2023_16.13.29.303412_epoch_364_loss_2.7039\n",
      "Epoch: 364 lr: 0.000000; Train loss: 2.780699, Val loss: 2.703912, time: 0 s\n",
      "saved 22.04.2023_16.13.29.792828_epoch_365_loss_2.7039\n",
      "Epoch: 365 lr: 0.000000; Train loss: 2.781503, Val loss: 2.703892, time: 0 s\n",
      "Epoch: 366 lr: 0.000000; Train loss: 2.780579, Val loss: 2.703949, time: 0 s\n",
      "Epoch: 367 lr: 0.000000; Train loss: 2.781008, Val loss: 2.703980, time: 0 s\n",
      "Epoch: 368 lr: 0.000000; Train loss: 2.780912, Val loss: 2.703989, time: 0 s\n",
      "Epoch: 369 lr: 0.000000; Train loss: 2.782033, Val loss: 2.703996, time: 0 s\n",
      "Epoch: 370 lr: 0.000000; Train loss: 2.781005, Val loss: 2.704042, time: 0 s\n",
      "Epoch: 371 lr: 0.000000; Train loss: 2.781791, Val loss: 2.704000, time: 0 s\n",
      "Epoch: 372 lr: 0.000000; Train loss: 2.781784, Val loss: 2.703947, time: 0 s\n",
      "Epoch: 373 lr: 0.000000; Train loss: 2.781942, Val loss: 2.703913, time: 0 s\n",
      "Epoch: 374 lr: 0.000000; Train loss: 2.780915, Val loss: 2.703961, time: 0 s\n",
      "Epoch: 375 lr: 0.000000; Train loss: 2.780900, Val loss: 2.703962, time: 0 s\n",
      "Epoch: 376 lr: 0.000000; Train loss: 2.781162, Val loss: 2.703972, time: 0 s\n",
      "Epoch: 377 lr: 0.000000; Train loss: 2.781360, Val loss: 2.703935, time: 0 s\n",
      "Epoch: 378 lr: 0.000000; Train loss: 2.780561, Val loss: 2.703892, time: 1 s\n",
      "saved 22.04.2023_16.13.36.128492_epoch_379_loss_2.7038\n",
      "Epoch: 379 lr: 0.000000; Train loss: 2.780853, Val loss: 2.703849, time: 0 s\n",
      "Epoch: 380 lr: 0.000000; Train loss: 2.781785, Val loss: 2.703884, time: 0 s\n",
      "Epoch: 381 lr: 0.000000; Train loss: 2.780711, Val loss: 2.703935, time: 0 s\n",
      "Epoch: 382 lr: 0.000000; Train loss: 2.781323, Val loss: 2.703914, time: 0 s\n",
      "Epoch: 383 lr: 0.000000; Train loss: 2.781442, Val loss: 2.703921, time: 0 s\n",
      "Epoch: 384 lr: 0.000000; Train loss: 2.779751, Val loss: 2.703984, time: 0 s\n",
      "Epoch: 385 lr: 0.000000; Train loss: 2.782441, Val loss: 2.703929, time: 0 s\n",
      "Epoch: 386 lr: 0.000000; Train loss: 2.781234, Val loss: 2.703918, time: 0 s\n",
      "Epoch: 387 lr: 0.000000; Train loss: 2.782078, Val loss: 2.703903, time: 0 s\n",
      "Epoch: 388 lr: 0.000000; Train loss: 2.781676, Val loss: 2.703863, time: 0 s\n",
      "Epoch: 389 lr: 0.000000; Train loss: 2.781873, Val loss: 2.703862, time: 0 s\n",
      "Epoch: 390 lr: 0.000000; Train loss: 2.781092, Val loss: 2.703866, time: 0 s\n",
      "Epoch: 391 lr: 0.000000; Train loss: 2.781146, Val loss: 2.703851, time: 0 s\n",
      "Epoch: 392 lr: 0.000000; Train loss: 2.781337, Val loss: 2.703855, time: 0 s\n",
      "Epoch: 393 lr: 0.000000; Train loss: 2.781525, Val loss: 2.703869, time: 0 s\n",
      "Epoch: 394 lr: 0.000000; Train loss: 2.781337, Val loss: 2.703881, time: 0 s\n",
      "Epoch: 395 lr: 0.000000; Train loss: 2.782089, Val loss: 2.703890, time: 1 s\n",
      "Epoch: 396 lr: 0.000000; Train loss: 2.781390, Val loss: 2.703898, time: 0 s\n",
      "saved 22.04.2023_16.13.44.460143_epoch_397_loss_2.7038\n",
      "Epoch: 397 lr: 0.000000; Train loss: 2.781659, Val loss: 2.703844, time: 0 s\n",
      "Epoch: 398 lr: 0.000000; Train loss: 2.780301, Val loss: 2.703893, time: 0 s\n",
      "Epoch: 399 lr: 0.000000; Train loss: 2.781961, Val loss: 2.703858, time: 1 s\n",
      "Epoch: 400 lr: 0.000000; Train loss: 2.781423, Val loss: 2.703862, time: 0 s\n",
      "saved 22.04.2023_16.13.46.355554_epoch_401_loss_2.7038\n",
      "Epoch: 401 lr: 0.000000; Train loss: 2.781070, Val loss: 2.703840, time: 1 s\n",
      "Epoch: 402 lr: 0.000000; Train loss: 2.781369, Val loss: 2.703881, time: 0 s\n",
      "Epoch: 403 lr: 0.000000; Train loss: 2.781501, Val loss: 2.703884, time: 1 s\n",
      "Epoch: 404 lr: 0.000000; Train loss: 2.781961, Val loss: 2.703898, time: 0 s\n",
      "Epoch: 405 lr: 0.000000; Train loss: 2.780952, Val loss: 2.703880, time: 0 s\n",
      "Epoch: 406 lr: 0.000000; Train loss: 2.782176, Val loss: 2.703870, time: 0 s\n",
      "Epoch: 407 lr: 0.000000; Train loss: 2.781788, Val loss: 2.703867, time: 0 s\n",
      "Epoch: 408 lr: 0.000000; Train loss: 2.780841, Val loss: 2.703872, time: 0 s\n",
      "Epoch: 409 lr: 0.000000; Train loss: 2.780873, Val loss: 2.703926, time: 0 s\n",
      "Epoch: 410 lr: 0.000000; Train loss: 2.780264, Val loss: 2.703907, time: 0 s\n",
      "Epoch: 411 lr: 0.000000; Train loss: 2.781589, Val loss: 2.703954, time: 1 s\n",
      "saved 22.04.2023_16.13.51.467869_epoch_412_loss_2.7038\n",
      "Epoch: 412 lr: 0.000000; Train loss: 2.781962, Val loss: 2.703827, time: 0 s\n",
      "saved 22.04.2023_16.13.51.979924_epoch_413_loss_2.7038\n",
      "Epoch: 413 lr: 0.000000; Train loss: 2.781355, Val loss: 2.703795, time: 1 s\n",
      "saved 22.04.2023_16.13.52.467341_epoch_414_loss_2.7038\n",
      "Epoch: 414 lr: 0.000000; Train loss: 2.781268, Val loss: 2.703757, time: 0 s\n",
      "saved 22.04.2023_16.13.52.882728_epoch_415_loss_2.7037\n",
      "Epoch: 415 lr: 0.000000; Train loss: 2.780847, Val loss: 2.703720, time: 0 s\n",
      "saved 22.04.2023_16.13.53.379737_epoch_416_loss_2.7037\n",
      "Epoch: 416 lr: 0.000000; Train loss: 2.780688, Val loss: 2.703715, time: 0 s\n",
      "saved 22.04.2023_16.13.53.794286_epoch_417_loss_2.7037\n",
      "Epoch: 417 lr: 0.000000; Train loss: 2.781070, Val loss: 2.703695, time: 0 s\n",
      "Epoch: 418 lr: 0.000000; Train loss: 2.780800, Val loss: 2.703707, time: 0 s\n",
      "Epoch: 419 lr: 0.000000; Train loss: 2.781230, Val loss: 2.703732, time: 0 s\n",
      "Epoch: 420 lr: 0.000000; Train loss: 2.780751, Val loss: 2.703757, time: 0 s\n",
      "Epoch: 421 lr: 0.000000; Train loss: 2.781440, Val loss: 2.703751, time: 0 s\n",
      "Epoch: 422 lr: 0.000000; Train loss: 2.782582, Val loss: 2.703748, time: 0 s\n",
      "Epoch: 423 lr: 0.000000; Train loss: 2.781291, Val loss: 2.703758, time: 0 s\n",
      "Epoch: 424 lr: 0.000000; Train loss: 2.781290, Val loss: 2.703723, time: 0 s\n",
      "Epoch: 425 lr: 0.000000; Train loss: 2.780560, Val loss: 2.703726, time: 0 s\n",
      "Epoch: 426 lr: 0.000000; Train loss: 2.780813, Val loss: 2.703749, time: 0 s\n",
      "Epoch: 427 lr: 0.000000; Train loss: 2.781272, Val loss: 2.703720, time: 0 s\n",
      "Epoch: 428 lr: 0.000000; Train loss: 2.782098, Val loss: 2.703750, time: 0 s\n",
      "Epoch: 429 lr: 0.000000; Train loss: 2.781440, Val loss: 2.703732, time: 0 s\n",
      "Epoch: 430 lr: 0.000000; Train loss: 2.781157, Val loss: 2.703767, time: 0 s\n",
      "Epoch: 431 lr: 0.000000; Train loss: 2.781529, Val loss: 2.703759, time: 0 s\n",
      "Epoch: 432 lr: 0.000000; Train loss: 2.780977, Val loss: 2.703772, time: 0 s\n",
      "Epoch: 433 lr: 0.000000; Train loss: 2.780456, Val loss: 2.703786, time: 0 s\n",
      "Epoch: 434 lr: 0.000000; Train loss: 2.781661, Val loss: 2.703863, time: 0 s\n",
      "Epoch: 435 lr: 0.000000; Train loss: 2.781376, Val loss: 2.703885, time: 0 s\n",
      "Epoch: 436 lr: 0.000000; Train loss: 2.780979, Val loss: 2.703861, time: 0 s\n",
      "Epoch: 437 lr: 0.000000; Train loss: 2.782577, Val loss: 2.703873, time: 0 s\n",
      "Epoch: 438 lr: 0.000000; Train loss: 2.780649, Val loss: 2.703885, time: 0 s\n",
      "Epoch: 439 lr: 0.000000; Train loss: 2.780365, Val loss: 2.703926, time: 0 s\n",
      "Epoch: 440 lr: 0.000000; Train loss: 2.780848, Val loss: 2.703898, time: 0 s\n",
      "Epoch: 441 lr: 0.000000; Train loss: 2.782048, Val loss: 2.703944, time: 0 s\n",
      "Epoch: 442 lr: 0.000000; Train loss: 2.780738, Val loss: 2.704005, time: 0 s\n",
      "Epoch: 443 lr: 0.000000; Train loss: 2.781813, Val loss: 2.704032, time: 1 s\n",
      "Epoch: 444 lr: 0.000000; Train loss: 2.781775, Val loss: 2.703984, time: 0 s\n",
      "Epoch: 445 lr: 0.000000; Train loss: 2.780397, Val loss: 2.704013, time: 0 s\n",
      "Epoch: 446 lr: 0.000000; Train loss: 2.781474, Val loss: 2.704060, time: 0 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 447 lr: 0.000000; Train loss: 2.781579, Val loss: 2.704019, time: 0 s\n",
      "Epoch: 448 lr: 0.000000; Train loss: 2.781456, Val loss: 2.703984, time: 0 s\n",
      "Epoch: 449 lr: 0.000000; Train loss: 2.781244, Val loss: 2.704011, time: 0 s\n",
      "Epoch: 450 lr: 0.000000; Train loss: 2.780865, Val loss: 2.704022, time: 0 s\n",
      "Epoch: 451 lr: 0.000000; Train loss: 2.780853, Val loss: 2.704032, time: 0 s\n",
      "Epoch: 452 lr: 0.000000; Train loss: 2.780483, Val loss: 2.704022, time: 0 s\n",
      "Epoch: 453 lr: 0.000000; Train loss: 2.781020, Val loss: 2.704014, time: 0 s\n",
      "Epoch: 454 lr: 0.000000; Train loss: 2.781480, Val loss: 2.703986, time: 0 s\n",
      "Epoch: 455 lr: 0.000000; Train loss: 2.780967, Val loss: 2.703960, time: 0 s\n",
      "Epoch: 456 lr: 0.000000; Train loss: 2.780275, Val loss: 2.703951, time: 0 s\n",
      "Epoch: 457 lr: 0.000000; Train loss: 2.780974, Val loss: 2.703954, time: 0 s\n",
      "Epoch: 458 lr: 0.000000; Train loss: 2.781597, Val loss: 2.703922, time: 0 s\n",
      "Epoch: 459 lr: 0.000000; Train loss: 2.780735, Val loss: 2.703924, time: 0 s\n",
      "Epoch: 460 lr: 0.000000; Train loss: 2.780859, Val loss: 2.703979, time: 0 s\n",
      "Epoch: 461 lr: 0.000000; Train loss: 2.781016, Val loss: 2.703995, time: 0 s\n",
      "Epoch: 462 lr: 0.000000; Train loss: 2.779875, Val loss: 2.703984, time: 0 s\n",
      "Epoch: 463 lr: 0.000000; Train loss: 2.781020, Val loss: 2.703958, time: 0 s\n",
      "Epoch: 464 lr: 0.000000; Train loss: 2.780631, Val loss: 2.703970, time: 0 s\n",
      "Epoch: 465 lr: 0.000000; Train loss: 2.781102, Val loss: 2.703942, time: 0 s\n",
      "Epoch: 466 lr: 0.000000; Train loss: 2.780661, Val loss: 2.703993, time: 0 s\n",
      "Epoch: 467 lr: 0.000000; Train loss: 2.781879, Val loss: 2.704061, time: 0 s\n",
      "Epoch: 468 lr: 0.000000; Train loss: 2.781347, Val loss: 2.704032, time: 1 s\n",
      "Epoch: 469 lr: 0.000000; Train loss: 2.780726, Val loss: 2.703981, time: 0 s\n",
      "Epoch: 470 lr: 0.000000; Train loss: 2.781758, Val loss: 2.703997, time: 0 s\n",
      "Epoch: 471 lr: 0.000000; Train loss: 2.781486, Val loss: 2.703982, time: 0 s\n",
      "Epoch: 472 lr: 0.000000; Train loss: 2.781407, Val loss: 2.703943, time: 0 s\n",
      "Epoch: 473 lr: 0.000000; Train loss: 2.781220, Val loss: 2.703960, time: 0 s\n",
      "Epoch: 474 lr: 0.000000; Train loss: 2.781048, Val loss: 2.703887, time: 0 s\n",
      "Epoch: 475 lr: 0.000000; Train loss: 2.780705, Val loss: 2.703812, time: 0 s\n",
      "Epoch: 476 lr: 0.000000; Train loss: 2.781256, Val loss: 2.703844, time: 0 s\n",
      "Epoch: 477 lr: 0.000000; Train loss: 2.781038, Val loss: 2.703823, time: 0 s\n",
      "Epoch: 478 lr: 0.000000; Train loss: 2.781205, Val loss: 2.703852, time: 0 s\n",
      "Epoch: 479 lr: 0.000000; Train loss: 2.781393, Val loss: 2.703842, time: 0 s\n",
      "Epoch: 480 lr: 0.000000; Train loss: 2.780836, Val loss: 2.703881, time: 0 s\n",
      "Epoch: 481 lr: 0.000000; Train loss: 2.780715, Val loss: 2.703819, time: 0 s\n",
      "Epoch: 482 lr: 0.000000; Train loss: 2.780936, Val loss: 2.703878, time: 0 s\n",
      "Epoch: 483 lr: 0.000000; Train loss: 2.780976, Val loss: 2.703927, time: 0 s\n",
      "Epoch: 484 lr: 0.000000; Train loss: 2.780595, Val loss: 2.703906, time: 0 s\n",
      "Epoch: 485 lr: 0.000000; Train loss: 2.780971, Val loss: 2.703884, time: 0 s\n",
      "Epoch: 486 lr: 0.000000; Train loss: 2.781656, Val loss: 2.703837, time: 0 s\n",
      "Epoch: 487 lr: 0.000000; Train loss: 2.781217, Val loss: 2.703804, time: 1 s\n",
      "Epoch: 488 lr: 0.000000; Train loss: 2.782469, Val loss: 2.703761, time: 0 s\n",
      "Epoch: 489 lr: 0.000000; Train loss: 2.780327, Val loss: 2.703759, time: 0 s\n",
      "Epoch: 490 lr: 0.000000; Train loss: 2.781355, Val loss: 2.703799, time: 0 s\n",
      "Epoch: 491 lr: 0.000000; Train loss: 2.780422, Val loss: 2.703886, time: 0 s\n",
      "Epoch: 492 lr: 0.000000; Train loss: 2.781642, Val loss: 2.703916, time: 0 s\n",
      "Epoch: 493 lr: 0.000000; Train loss: 2.781128, Val loss: 2.703883, time: 0 s\n",
      "Epoch: 494 lr: 0.000000; Train loss: 2.781692, Val loss: 2.703823, time: 0 s\n",
      "Epoch: 495 lr: 0.000000; Train loss: 2.781249, Val loss: 2.703828, time: 0 s\n",
      "Epoch: 496 lr: 0.000000; Train loss: 2.780863, Val loss: 2.703828, time: 0 s\n",
      "Epoch: 497 lr: 0.000000; Train loss: 2.780517, Val loss: 2.703843, time: 0 s\n",
      "Epoch: 498 lr: 0.000000; Train loss: 2.781361, Val loss: 2.703844, time: 0 s\n",
      "Epoch: 499 lr: 0.000000; Train loss: 2.782040, Val loss: 2.703819, time: 0 s\n",
      "Epoch: 500 lr: 0.000000; Train loss: 2.782040, Val loss: 2.703786, time: 0 s\n",
      "Epoch: 501 lr: 0.000000; Train loss: 2.780446, Val loss: 2.703828, time: 0 s\n",
      "Epoch: 502 lr: 0.000000; Train loss: 2.780790, Val loss: 2.703859, time: 0 s\n",
      "Epoch: 503 lr: 0.000000; Train loss: 2.781762, Val loss: 2.703808, time: 0 s\n",
      "Epoch: 504 lr: 0.000000; Train loss: 2.780192, Val loss: 2.703809, time: 1 s\n",
      "Epoch: 505 lr: 0.000000; Train loss: 2.781563, Val loss: 2.703773, time: 0 s\n",
      "Epoch: 506 lr: 0.000000; Train loss: 2.780991, Val loss: 2.703787, time: 0 s\n",
      "Epoch: 507 lr: 0.000000; Train loss: 2.781158, Val loss: 2.703766, time: 0 s\n",
      "Epoch: 508 lr: 0.000000; Train loss: 2.781080, Val loss: 2.703757, time: 0 s\n",
      "Epoch: 509 lr: 0.000000; Train loss: 2.780513, Val loss: 2.703782, time: 0 s\n",
      "Epoch: 510 lr: 0.000000; Train loss: 2.780184, Val loss: 2.703789, time: 0 s\n",
      "Epoch: 511 lr: 0.000000; Train loss: 2.781489, Val loss: 2.703840, time: 1 s\n",
      "Epoch: 512 lr: 0.000000; Train loss: 2.782091, Val loss: 2.703830, time: 0 s\n",
      "Epoch: 513 lr: 0.000000; Train loss: 2.780962, Val loss: 2.703905, time: 0 s\n",
      "Epoch: 514 lr: 0.000000; Train loss: 2.781019, Val loss: 2.703841, time: 0 s\n",
      "Epoch: 515 lr: 0.000000; Train loss: 2.780544, Val loss: 2.703849, time: 0 s\n",
      "Epoch: 516 lr: 0.000000; Train loss: 2.781823, Val loss: 2.703811, time: 0 s\n",
      "Epoch: 517 lr: 0.000000; Train loss: 2.780931, Val loss: 2.703809, time: 0 s\n",
      "Epoch: 518 lr: 0.000000; Train loss: 2.782618, Val loss: 2.703764, time: 0 s\n",
      "Epoch: 519 lr: 0.000000; Train loss: 2.781323, Val loss: 2.703793, time: 0 s\n",
      "Epoch: 520 lr: 0.000000; Train loss: 2.781024, Val loss: 2.703794, time: 0 s\n",
      "Epoch: 521 lr: 0.000000; Train loss: 2.781891, Val loss: 2.703817, time: 0 s\n",
      "Epoch: 522 lr: 0.000000; Train loss: 2.781229, Val loss: 2.703858, time: 0 s\n",
      "Epoch: 523 lr: 0.000000; Train loss: 2.780631, Val loss: 2.703864, time: 0 s\n",
      "Epoch: 524 lr: 0.000000; Train loss: 2.781464, Val loss: 2.703882, time: 0 s\n",
      "Epoch: 525 lr: 0.000000; Train loss: 2.781986, Val loss: 2.703875, time: 0 s\n",
      "Epoch: 526 lr: 0.000000; Train loss: 2.780761, Val loss: 2.703903, time: 0 s\n",
      "Epoch: 527 lr: 0.000000; Train loss: 2.781013, Val loss: 2.703906, time: 1 s\n",
      "Epoch: 528 lr: 0.000000; Train loss: 2.782005, Val loss: 2.703884, time: 0 s\n",
      "Epoch: 529 lr: 0.000000; Train loss: 2.781116, Val loss: 2.703939, time: 0 s\n",
      "Epoch: 530 lr: 0.000000; Train loss: 2.782057, Val loss: 2.703942, time: 1 s\n",
      "Epoch: 531 lr: 0.000000; Train loss: 2.780532, Val loss: 2.703974, time: 0 s\n",
      "Epoch: 532 lr: 0.000000; Train loss: 2.781380, Val loss: 2.704009, time: 0 s\n",
      "Epoch: 533 lr: 0.000000; Train loss: 2.780954, Val loss: 2.704090, time: 0 s\n",
      "Epoch: 534 lr: 0.000000; Train loss: 2.781063, Val loss: 2.704053, time: 0 s\n",
      "Epoch: 535 lr: 0.000000; Train loss: 2.781313, Val loss: 2.704046, time: 0 s\n",
      "Epoch: 536 lr: 0.000000; Train loss: 2.781195, Val loss: 2.703979, time: 0 s\n",
      "Epoch: 537 lr: 0.000000; Train loss: 2.780910, Val loss: 2.704002, time: 0 s\n",
      "Epoch: 538 lr: 0.000000; Train loss: 2.780445, Val loss: 2.704030, time: 0 s\n",
      "Epoch: 539 lr: 0.000000; Train loss: 2.781436, Val loss: 2.704016, time: 0 s\n",
      "Epoch: 540 lr: 0.000000; Train loss: 2.780825, Val loss: 2.703990, time: 0 s\n",
      "Epoch: 541 lr: 0.000000; Train loss: 2.780622, Val loss: 2.703958, time: 0 s\n",
      "Epoch: 542 lr: 0.000000; Train loss: 2.781457, Val loss: 2.703904, time: 0 s\n",
      "Epoch: 543 lr: 0.000000; Train loss: 2.780461, Val loss: 2.703892, time: 0 s\n",
      "Epoch: 544 lr: 0.000000; Train loss: 2.780919, Val loss: 2.703904, time: 0 s\n",
      "Epoch: 545 lr: 0.000000; Train loss: 2.780974, Val loss: 2.703919, time: 0 s\n",
      "Epoch: 546 lr: 0.000000; Train loss: 2.780822, Val loss: 2.703872, time: 0 s\n",
      "Epoch: 547 lr: 0.000000; Train loss: 2.781559, Val loss: 2.703889, time: 1 s\n",
      "Epoch: 548 lr: 0.000000; Train loss: 2.781950, Val loss: 2.703881, time: 0 s\n",
      "Epoch: 549 lr: 0.000000; Train loss: 2.781929, Val loss: 2.703875, time: 1 s\n",
      "Epoch: 550 lr: 0.000000; Train loss: 2.782191, Val loss: 2.703845, time: 0 s\n",
      "Epoch: 551 lr: 0.000000; Train loss: 2.780991, Val loss: 2.703849, time: 1 s\n",
      "Epoch: 552 lr: 0.000000; Train loss: 2.780899, Val loss: 2.703884, time: 0 s\n",
      "Epoch: 553 lr: 0.000000; Train loss: 2.781103, Val loss: 2.703894, time: 1 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 554 lr: 0.000000; Train loss: 2.780658, Val loss: 2.703895, time: 0 s\n",
      "Epoch: 555 lr: 0.000000; Train loss: 2.779769, Val loss: 2.703923, time: 0 s\n",
      "Epoch: 556 lr: 0.000000; Train loss: 2.780767, Val loss: 2.703958, time: 0 s\n",
      "Epoch: 557 lr: 0.000000; Train loss: 2.780716, Val loss: 2.703974, time: 0 s\n",
      "Epoch: 558 lr: 0.000000; Train loss: 2.780860, Val loss: 2.703970, time: 0 s\n",
      "Epoch: 559 lr: 0.000000; Train loss: 2.780939, Val loss: 2.703973, time: 0 s\n",
      "Epoch: 560 lr: 0.000000; Train loss: 2.782492, Val loss: 2.703974, time: 0 s\n",
      "Epoch: 561 lr: 0.000000; Train loss: 2.781359, Val loss: 2.703973, time: 0 s\n",
      "Epoch: 562 lr: 0.000000; Train loss: 2.781298, Val loss: 2.703938, time: 0 s\n",
      "Epoch: 563 lr: 0.000000; Train loss: 2.781240, Val loss: 2.703972, time: 0 s\n",
      "Epoch: 564 lr: 0.000000; Train loss: 2.781521, Val loss: 2.703974, time: 0 s\n",
      "Epoch: 565 lr: 0.000000; Train loss: 2.781386, Val loss: 2.703872, time: 0 s\n",
      "Epoch: 566 lr: 0.000000; Train loss: 2.782202, Val loss: 2.703873, time: 0 s\n",
      "Epoch: 567 lr: 0.000000; Train loss: 2.781471, Val loss: 2.703842, time: 0 s\n",
      "Epoch: 568 lr: 0.000000; Train loss: 2.780809, Val loss: 2.703895, time: 0 s\n",
      "Epoch: 569 lr: 0.000000; Train loss: 2.780986, Val loss: 2.703893, time: 0 s\n",
      "Epoch: 570 lr: 0.000000; Train loss: 2.781504, Val loss: 2.703865, time: 0 s\n",
      "Epoch: 571 lr: 0.000000; Train loss: 2.781998, Val loss: 2.703836, time: 0 s\n",
      "Epoch: 572 lr: 0.000000; Train loss: 2.781115, Val loss: 2.703864, time: 0 s\n",
      "Epoch: 573 lr: 0.000000; Train loss: 2.781258, Val loss: 2.703845, time: 0 s\n",
      "Epoch: 574 lr: 0.000000; Train loss: 2.782025, Val loss: 2.703847, time: 0 s\n",
      "Epoch: 575 lr: 0.000000; Train loss: 2.780712, Val loss: 2.703866, time: 0 s\n",
      "Epoch: 576 lr: 0.000000; Train loss: 2.781210, Val loss: 2.703834, time: 0 s\n",
      "Epoch: 577 lr: 0.000000; Train loss: 2.780884, Val loss: 2.703865, time: 0 s\n",
      "Epoch: 578 lr: 0.000000; Train loss: 2.781660, Val loss: 2.703863, time: 0 s\n",
      "Epoch: 579 lr: 0.000000; Train loss: 2.780856, Val loss: 2.703876, time: 0 s\n",
      "Epoch: 580 lr: 0.000000; Train loss: 2.781715, Val loss: 2.703807, time: 1 s\n",
      "Epoch: 581 lr: 0.000000; Train loss: 2.780966, Val loss: 2.703825, time: 0 s\n",
      "Epoch: 582 lr: 0.000000; Train loss: 2.780925, Val loss: 2.703857, time: 0 s\n",
      "Epoch: 583 lr: 0.000000; Train loss: 2.782032, Val loss: 2.703824, time: 0 s\n",
      "Epoch: 584 lr: 0.000000; Train loss: 2.780582, Val loss: 2.703779, time: 0 s\n",
      "Epoch: 585 lr: 0.000000; Train loss: 2.779983, Val loss: 2.703797, time: 0 s\n",
      "Epoch: 586 lr: 0.000000; Train loss: 2.781444, Val loss: 2.703824, time: 0 s\n",
      "Epoch: 587 lr: 0.000000; Train loss: 2.780679, Val loss: 2.703830, time: 1 s\n",
      "Epoch: 588 lr: 0.000000; Train loss: 2.779753, Val loss: 2.703879, time: 0 s\n",
      "Epoch: 589 lr: 0.000000; Train loss: 2.781384, Val loss: 2.703809, time: 0 s\n",
      "Epoch: 590 lr: 0.000000; Train loss: 2.781649, Val loss: 2.703820, time: 0 s\n",
      "Epoch: 591 lr: 0.000000; Train loss: 2.781548, Val loss: 2.703785, time: 1 s\n",
      "Epoch: 592 lr: 0.000000; Train loss: 2.781087, Val loss: 2.703791, time: 0 s\n",
      "Epoch: 593 lr: 0.000000; Train loss: 2.781338, Val loss: 2.703792, time: 1 s\n",
      "Epoch: 594 lr: 0.000000; Train loss: 2.781348, Val loss: 2.703798, time: 0 s\n",
      "Epoch: 595 lr: 0.000000; Train loss: 2.780549, Val loss: 2.703854, time: 0 s\n",
      "Epoch: 596 lr: 0.000000; Train loss: 2.781153, Val loss: 2.703856, time: 0 s\n",
      "Epoch: 597 lr: 0.000000; Train loss: 2.781919, Val loss: 2.703872, time: 0 s\n",
      "Epoch: 598 lr: 0.000000; Train loss: 2.781215, Val loss: 2.703910, time: 0 s\n",
      "Epoch: 599 lr: 0.000000; Train loss: 2.780845, Val loss: 2.703939, time: 0 s\n",
      "Epoch: 600 lr: 0.000000; Train loss: 2.781431, Val loss: 2.703951, time: 0 s\n",
      "Epoch: 601 lr: 0.000000; Train loss: 2.780852, Val loss: 2.703946, time: 0 s\n",
      "Epoch: 602 lr: 0.000000; Train loss: 2.781743, Val loss: 2.703897, time: 0 s\n",
      "Epoch: 603 lr: 0.000000; Train loss: 2.781092, Val loss: 2.703918, time: 0 s\n",
      "Epoch: 604 lr: 0.000000; Train loss: 2.780663, Val loss: 2.703963, time: 0 s\n",
      "Epoch: 605 lr: 0.000000; Train loss: 2.781623, Val loss: 2.703956, time: 0 s\n",
      "Epoch: 606 lr: 0.000000; Train loss: 2.780187, Val loss: 2.703910, time: 1 s\n",
      "Epoch: 607 lr: 0.000000; Train loss: 2.780866, Val loss: 2.703929, time: 0 s\n",
      "Epoch: 608 lr: 0.000000; Train loss: 2.781882, Val loss: 2.703909, time: 1 s\n",
      "Epoch: 609 lr: 0.000000; Train loss: 2.782134, Val loss: 2.703911, time: 0 s\n",
      "Epoch: 610 lr: 0.000000; Train loss: 2.780777, Val loss: 2.703896, time: 0 s\n",
      "Epoch: 611 lr: 0.000000; Train loss: 2.780564, Val loss: 2.703877, time: 0 s\n",
      "Epoch: 612 lr: 0.000000; Train loss: 2.779838, Val loss: 2.703878, time: 0 s\n",
      "Epoch: 613 lr: 0.000000; Train loss: 2.781288, Val loss: 2.703841, time: 0 s\n",
      "Epoch: 614 lr: 0.000000; Train loss: 2.780984, Val loss: 2.703859, time: 0 s\n",
      "Epoch: 615 lr: 0.000000; Train loss: 2.781423, Val loss: 2.703861, time: 0 s\n",
      "Epoch: 616 lr: 0.000000; Train loss: 2.780936, Val loss: 2.703902, time: 1 s\n",
      "Epoch: 617 lr: 0.000000; Train loss: 2.782080, Val loss: 2.703911, time: 0 s\n",
      "Epoch: 618 lr: 0.000000; Train loss: 2.781013, Val loss: 2.703836, time: 1 s\n",
      "Epoch: 619 lr: 0.000000; Train loss: 2.781593, Val loss: 2.703787, time: 0 s\n",
      "Epoch: 620 lr: 0.000000; Train loss: 2.781413, Val loss: 2.703797, time: 0 s\n",
      "Epoch: 621 lr: 0.000000; Train loss: 2.781352, Val loss: 2.703784, time: 0 s\n",
      "Epoch: 622 lr: 0.000000; Train loss: 2.781186, Val loss: 2.703807, time: 0 s\n",
      "Epoch: 623 lr: 0.000000; Train loss: 2.780187, Val loss: 2.703830, time: 0 s\n",
      "Epoch: 624 lr: 0.000000; Train loss: 2.781197, Val loss: 2.703818, time: 0 s\n",
      "Epoch: 625 lr: 0.000000; Train loss: 2.781931, Val loss: 2.703826, time: 1 s\n",
      "Epoch: 626 lr: 0.000000; Train loss: 2.780192, Val loss: 2.703831, time: 0 s\n",
      "Epoch: 627 lr: 0.000000; Train loss: 2.780565, Val loss: 2.703855, time: 0 s\n",
      "Epoch: 628 lr: 0.000000; Train loss: 2.781036, Val loss: 2.703869, time: 0 s\n",
      "Epoch: 629 lr: 0.000000; Train loss: 2.781730, Val loss: 2.703850, time: 1 s\n",
      "Epoch: 630 lr: 0.000000; Train loss: 2.781621, Val loss: 2.703874, time: 0 s\n",
      "Epoch: 631 lr: 0.000000; Train loss: 2.780460, Val loss: 2.703924, time: 0 s\n",
      "Epoch: 632 lr: 0.000000; Train loss: 2.782078, Val loss: 2.703903, time: 0 s\n",
      "Epoch: 633 lr: 0.000000; Train loss: 2.781439, Val loss: 2.703944, time: 0 s\n",
      "Epoch: 634 lr: 0.000000; Train loss: 2.781010, Val loss: 2.703942, time: 0 s\n",
      "Epoch: 635 lr: 0.000000; Train loss: 2.781355, Val loss: 2.703899, time: 0 s\n",
      "Epoch: 636 lr: 0.000000; Train loss: 2.780063, Val loss: 2.703942, time: 0 s\n",
      "Epoch: 637 lr: 0.000000; Train loss: 2.780971, Val loss: 2.703948, time: 1 s\n",
      "Epoch: 638 lr: 0.000000; Train loss: 2.780797, Val loss: 2.703983, time: 0 s\n",
      "Epoch: 639 lr: 0.000000; Train loss: 2.780329, Val loss: 2.703979, time: 1 s\n",
      "Epoch: 640 lr: 0.000000; Train loss: 2.781574, Val loss: 2.703997, time: 0 s\n",
      "Epoch: 641 lr: 0.000000; Train loss: 2.782296, Val loss: 2.703997, time: 0 s\n",
      "Epoch: 642 lr: 0.000000; Train loss: 2.780709, Val loss: 2.704009, time: 0 s\n",
      "Epoch: 643 lr: 0.000000; Train loss: 2.781655, Val loss: 2.704026, time: 0 s\n",
      "Epoch: 644 lr: 0.000000; Train loss: 2.781596, Val loss: 2.704019, time: 0 s\n",
      "Epoch: 645 lr: 0.000000; Train loss: 2.780574, Val loss: 2.703987, time: 0 s\n",
      "Epoch: 646 lr: 0.000000; Train loss: 2.781520, Val loss: 2.703928, time: 0 s\n",
      "Epoch: 647 lr: 0.000000; Train loss: 2.781179, Val loss: 2.703973, time: 0 s\n",
      "Epoch: 648 lr: 0.000000; Train loss: 2.781903, Val loss: 2.703948, time: 0 s\n",
      "Epoch: 649 lr: 0.000000; Train loss: 2.780894, Val loss: 2.703962, time: 0 s\n",
      "Epoch: 650 lr: 0.000000; Train loss: 2.780906, Val loss: 2.703985, time: 0 s\n",
      "Epoch: 651 lr: 0.000000; Train loss: 2.780905, Val loss: 2.703990, time: 0 s\n",
      "Epoch: 652 lr: 0.000000; Train loss: 2.781471, Val loss: 2.703972, time: 0 s\n",
      "Epoch: 653 lr: 0.000000; Train loss: 2.781512, Val loss: 2.703951, time: 0 s\n",
      "Epoch: 654 lr: 0.000000; Train loss: 2.782235, Val loss: 2.703916, time: 0 s\n",
      "Epoch: 655 lr: 0.000000; Train loss: 2.780596, Val loss: 2.703866, time: 0 s\n",
      "Epoch: 656 lr: 0.000000; Train loss: 2.781600, Val loss: 2.703900, time: 0 s\n",
      "Epoch: 657 lr: 0.000000; Train loss: 2.781777, Val loss: 2.703907, time: 0 s\n",
      "Epoch: 658 lr: 0.000000; Train loss: 2.781000, Val loss: 2.703915, time: 0 s\n",
      "Epoch: 659 lr: 0.000000; Train loss: 2.780914, Val loss: 2.703892, time: 0 s\n",
      "Epoch: 660 lr: 0.000000; Train loss: 2.781021, Val loss: 2.703918, time: 1 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 661 lr: 0.000000; Train loss: 2.781516, Val loss: 2.703933, time: 1 s\n",
      "Epoch: 662 lr: 0.000000; Train loss: 2.781552, Val loss: 2.703913, time: 0 s\n",
      "Epoch: 663 lr: 0.000000; Train loss: 2.781397, Val loss: 2.703858, time: 1 s\n",
      "Epoch: 664 lr: 0.000000; Train loss: 2.781870, Val loss: 2.703869, time: 0 s\n",
      "Epoch: 665 lr: 0.000000; Train loss: 2.781009, Val loss: 2.703828, time: 1 s\n",
      "Epoch: 666 lr: 0.000000; Train loss: 2.780993, Val loss: 2.703800, time: 0 s\n",
      "Epoch: 667 lr: 0.000000; Train loss: 2.781298, Val loss: 2.703845, time: 1 s\n",
      "Epoch: 668 lr: 0.000000; Train loss: 2.781523, Val loss: 2.703830, time: 0 s\n",
      "Epoch: 669 lr: 0.000000; Train loss: 2.780971, Val loss: 2.703789, time: 1 s\n",
      "Epoch: 670 lr: 0.000000; Train loss: 2.782106, Val loss: 2.703803, time: 0 s\n",
      "Epoch: 671 lr: 0.000000; Train loss: 2.781283, Val loss: 2.703839, time: 0 s\n",
      "Epoch: 672 lr: 0.000000; Train loss: 2.781166, Val loss: 2.703860, time: 0 s\n",
      "Epoch: 673 lr: 0.000000; Train loss: 2.781191, Val loss: 2.703898, time: 1 s\n",
      "Epoch: 674 lr: 0.000000; Train loss: 2.781022, Val loss: 2.703867, time: 0 s\n",
      "Epoch: 675 lr: 0.000000; Train loss: 2.781388, Val loss: 2.703830, time: 0 s\n",
      "Epoch: 676 lr: 0.000000; Train loss: 2.780645, Val loss: 2.703839, time: 0 s\n",
      "Epoch: 677 lr: 0.000000; Train loss: 2.779866, Val loss: 2.703898, time: 0 s\n",
      "Epoch: 678 lr: 0.000000; Train loss: 2.781220, Val loss: 2.703830, time: 0 s\n",
      "Epoch: 679 lr: 0.000000; Train loss: 2.781050, Val loss: 2.703803, time: 0 s\n",
      "Epoch: 680 lr: 0.000000; Train loss: 2.780174, Val loss: 2.703801, time: 0 s\n",
      "Epoch: 681 lr: 0.000000; Train loss: 2.781313, Val loss: 2.703826, time: 0 s\n",
      "Epoch: 682 lr: 0.000000; Train loss: 2.781754, Val loss: 2.703844, time: 0 s\n",
      "Epoch: 683 lr: 0.000000; Train loss: 2.781502, Val loss: 2.703851, time: 0 s\n",
      "Epoch: 684 lr: 0.000000; Train loss: 2.781659, Val loss: 2.703894, time: 0 s\n",
      "Epoch: 685 lr: 0.000000; Train loss: 2.780344, Val loss: 2.703926, time: 0 s\n",
      "Epoch: 686 lr: 0.000000; Train loss: 2.781037, Val loss: 2.703975, time: 0 s\n",
      "Epoch: 687 lr: 0.000000; Train loss: 2.781484, Val loss: 2.703968, time: 0 s\n",
      "Epoch: 688 lr: 0.000000; Train loss: 2.780760, Val loss: 2.704024, time: 0 s\n",
      "Epoch: 689 lr: 0.000000; Train loss: 2.780801, Val loss: 2.703991, time: 0 s\n",
      "Epoch: 690 lr: 0.000000; Train loss: 2.780857, Val loss: 2.703964, time: 0 s\n",
      "Epoch: 691 lr: 0.000000; Train loss: 2.780340, Val loss: 2.703931, time: 0 s\n",
      "Epoch: 692 lr: 0.000000; Train loss: 2.781768, Val loss: 2.703885, time: 0 s\n",
      "Epoch: 693 lr: 0.000000; Train loss: 2.780443, Val loss: 2.703904, time: 0 s\n",
      "Epoch: 694 lr: 0.000000; Train loss: 2.780751, Val loss: 2.703940, time: 0 s\n",
      "Epoch: 695 lr: 0.000000; Train loss: 2.781941, Val loss: 2.703950, time: 0 s\n",
      "Epoch: 696 lr: 0.000000; Train loss: 2.781170, Val loss: 2.703914, time: 1 s\n",
      "Epoch: 697 lr: 0.000000; Train loss: 2.781249, Val loss: 2.703881, time: 0 s\n",
      "Epoch: 698 lr: 0.000000; Train loss: 2.781385, Val loss: 2.703884, time: 1 s\n",
      "Epoch: 699 lr: 0.000000; Train loss: 2.780762, Val loss: 2.703915, time: 0 s\n",
      "Epoch: 700 lr: 0.000000; Train loss: 2.781528, Val loss: 2.703925, time: 0 s\n",
      "Epoch: 701 lr: 0.000000; Train loss: 2.781166, Val loss: 2.703913, time: 0 s\n",
      "Epoch: 702 lr: 0.000000; Train loss: 2.782476, Val loss: 2.703875, time: 0 s\n",
      "Epoch: 703 lr: 0.000000; Train loss: 2.780383, Val loss: 2.703938, time: 1 s\n",
      "Epoch: 704 lr: 0.000000; Train loss: 2.780918, Val loss: 2.703881, time: 0 s\n",
      "Epoch: 705 lr: 0.000000; Train loss: 2.780640, Val loss: 2.703907, time: 0 s\n",
      "Epoch: 706 lr: 0.000000; Train loss: 2.781283, Val loss: 2.703920, time: 0 s\n",
      "Epoch: 707 lr: 0.000000; Train loss: 2.780319, Val loss: 2.703944, time: 0 s\n",
      "Epoch: 708 lr: 0.000000; Train loss: 2.781581, Val loss: 2.703949, time: 0 s\n",
      "Epoch: 709 lr: 0.000000; Train loss: 2.780220, Val loss: 2.703959, time: 0 s\n",
      "Epoch: 710 lr: 0.000000; Train loss: 2.780951, Val loss: 2.703975, time: 0 s\n",
      "Epoch: 711 lr: 0.000000; Train loss: 2.781137, Val loss: 2.703928, time: 0 s\n",
      "Epoch: 712 lr: 0.000000; Train loss: 2.781262, Val loss: 2.703915, time: 0 s\n",
      "Epoch: 713 lr: 0.000000; Train loss: 2.782023, Val loss: 2.703845, time: 0 s\n",
      "Epoch: 714 lr: 0.000000; Train loss: 2.780829, Val loss: 2.703876, time: 0 s\n",
      "Epoch: 715 lr: 0.000000; Train loss: 2.781303, Val loss: 2.703901, time: 0 s\n",
      "Epoch: 716 lr: 0.000000; Train loss: 2.779804, Val loss: 2.703937, time: 0 s\n",
      "Epoch: 717 lr: 0.000000; Train loss: 2.781167, Val loss: 2.703928, time: 0 s\n",
      "Epoch: 718 lr: 0.000000; Train loss: 2.781389, Val loss: 2.703915, time: 0 s\n",
      "Epoch: 719 lr: 0.000000; Train loss: 2.781550, Val loss: 2.703903, time: 0 s\n",
      "Epoch: 720 lr: 0.000000; Train loss: 2.780740, Val loss: 2.703894, time: 0 s\n",
      "Epoch: 721 lr: 0.000000; Train loss: 2.782212, Val loss: 2.703883, time: 0 s\n",
      "Epoch: 722 lr: 0.000000; Train loss: 2.780328, Val loss: 2.703851, time: 0 s\n",
      "Epoch: 723 lr: 0.000000; Train loss: 2.782266, Val loss: 2.703865, time: 0 s\n",
      "Epoch: 724 lr: 0.000000; Train loss: 2.781599, Val loss: 2.703860, time: 0 s\n",
      "Epoch: 725 lr: 0.000000; Train loss: 2.781476, Val loss: 2.703871, time: 0 s\n",
      "Epoch: 726 lr: 0.000000; Train loss: 2.781655, Val loss: 2.703875, time: 1 s\n",
      "Epoch: 727 lr: 0.000000; Train loss: 2.780720, Val loss: 2.703871, time: 0 s\n",
      "Epoch: 728 lr: 0.000000; Train loss: 2.780824, Val loss: 2.703883, time: 1 s\n",
      "Epoch: 729 lr: 0.000000; Train loss: 2.781594, Val loss: 2.703876, time: 0 s\n",
      "Epoch: 730 lr: 0.000000; Train loss: 2.781197, Val loss: 2.703848, time: 0 s\n",
      "Epoch: 731 lr: 0.000000; Train loss: 2.780926, Val loss: 2.703865, time: 0 s\n",
      "Epoch: 732 lr: 0.000000; Train loss: 2.782109, Val loss: 2.703888, time: 0 s\n",
      "Epoch: 733 lr: 0.000000; Train loss: 2.781333, Val loss: 2.703890, time: 0 s\n",
      "Epoch: 734 lr: 0.000000; Train loss: 2.781124, Val loss: 2.703872, time: 0 s\n",
      "Epoch: 735 lr: 0.000000; Train loss: 2.781294, Val loss: 2.703862, time: 0 s\n",
      "Epoch: 736 lr: 0.000000; Train loss: 2.781024, Val loss: 2.703879, time: 1 s\n",
      "Epoch: 737 lr: 0.000000; Train loss: 2.781811, Val loss: 2.703893, time: 0 s\n",
      "Epoch: 738 lr: 0.000000; Train loss: 2.781283, Val loss: 2.703891, time: 0 s\n",
      "Epoch: 739 lr: 0.000000; Train loss: 2.781181, Val loss: 2.703885, time: 0 s\n",
      "Epoch: 740 lr: 0.000000; Train loss: 2.779739, Val loss: 2.703901, time: 0 s\n",
      "Epoch: 741 lr: 0.000000; Train loss: 2.780946, Val loss: 2.703888, time: 0 s\n",
      "Epoch: 742 lr: 0.000000; Train loss: 2.781332, Val loss: 2.703819, time: 0 s\n",
      "Epoch: 743 lr: 0.000000; Train loss: 2.781859, Val loss: 2.703823, time: 0 s\n",
      "Epoch: 744 lr: 0.000000; Train loss: 2.781962, Val loss: 2.703816, time: 0 s\n",
      "Epoch: 745 lr: 0.000000; Train loss: 2.780936, Val loss: 2.703814, time: 0 s\n",
      "Epoch: 746 lr: 0.000000; Train loss: 2.780654, Val loss: 2.703839, time: 0 s\n",
      "Epoch: 747 lr: 0.000000; Train loss: 2.780612, Val loss: 2.703882, time: 0 s\n",
      "Epoch: 748 lr: 0.000000; Train loss: 2.781177, Val loss: 2.703875, time: 0 s\n",
      "Epoch: 749 lr: 0.000000; Train loss: 2.779339, Val loss: 2.703905, time: 0 s\n",
      "Epoch: 750 lr: 0.000000; Train loss: 2.782722, Val loss: 2.703876, time: 0 s\n",
      "Epoch: 751 lr: 0.000000; Train loss: 2.781808, Val loss: 2.703841, time: 0 s\n",
      "Epoch: 752 lr: 0.000000; Train loss: 2.780748, Val loss: 2.703841, time: 0 s\n",
      "Epoch: 753 lr: 0.000000; Train loss: 2.780949, Val loss: 2.703902, time: 0 s\n",
      "Epoch: 754 lr: 0.000000; Train loss: 2.781343, Val loss: 2.703938, time: 0 s\n",
      "Epoch: 755 lr: 0.000000; Train loss: 2.780360, Val loss: 2.703925, time: 0 s\n",
      "Epoch: 756 lr: 0.000000; Train loss: 2.781307, Val loss: 2.703933, time: 0 s\n",
      "Epoch: 757 lr: 0.000000; Train loss: 2.780272, Val loss: 2.703903, time: 0 s\n",
      "Epoch: 758 lr: 0.000000; Train loss: 2.781469, Val loss: 2.703900, time: 0 s\n",
      "Epoch: 759 lr: 0.000000; Train loss: 2.780945, Val loss: 2.703928, time: 0 s\n",
      "Epoch: 760 lr: 0.000000; Train loss: 2.780222, Val loss: 2.703965, time: 0 s\n",
      "Epoch: 761 lr: 0.000000; Train loss: 2.782014, Val loss: 2.703970, time: 0 s\n",
      "Epoch: 762 lr: 0.000000; Train loss: 2.782077, Val loss: 2.703938, time: 0 s\n",
      "Epoch: 763 lr: 0.000000; Train loss: 2.781296, Val loss: 2.703913, time: 0 s\n",
      "Epoch: 764 lr: 0.000000; Train loss: 2.781771, Val loss: 2.703897, time: 0 s\n",
      "Epoch: 765 lr: 0.000000; Train loss: 2.780947, Val loss: 2.703885, time: 0 s\n",
      "Epoch: 766 lr: 0.000000; Train loss: 2.781229, Val loss: 2.703880, time: 1 s\n",
      "Epoch: 767 lr: 0.000000; Train loss: 2.780761, Val loss: 2.703920, time: 0 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 768 lr: 0.000000; Train loss: 2.780795, Val loss: 2.703913, time: 0 s\n",
      "Epoch: 769 lr: 0.000000; Train loss: 2.782008, Val loss: 2.703884, time: 0 s\n",
      "Epoch: 770 lr: 0.000000; Train loss: 2.780413, Val loss: 2.703909, time: 1 s\n",
      "Epoch: 771 lr: 0.000000; Train loss: 2.780704, Val loss: 2.703932, time: 0 s\n",
      "Epoch: 772 lr: 0.000000; Train loss: 2.780751, Val loss: 2.703909, time: 0 s\n",
      "Epoch: 773 lr: 0.000000; Train loss: 2.781606, Val loss: 2.703872, time: 0 s\n",
      "Epoch: 774 lr: 0.000000; Train loss: 2.780519, Val loss: 2.703871, time: 0 s\n",
      "Epoch: 775 lr: 0.000000; Train loss: 2.781207, Val loss: 2.703895, time: 0 s\n",
      "Epoch: 776 lr: 0.000000; Train loss: 2.781180, Val loss: 2.703907, time: 0 s\n",
      "Epoch: 777 lr: 0.000000; Train loss: 2.781103, Val loss: 2.703956, time: 0 s\n",
      "Epoch: 778 lr: 0.000000; Train loss: 2.780839, Val loss: 2.703943, time: 0 s\n",
      "Epoch: 779 lr: 0.000000; Train loss: 2.780616, Val loss: 2.703929, time: 1 s\n",
      "Epoch: 780 lr: 0.000000; Train loss: 2.781440, Val loss: 2.703912, time: 0 s\n",
      "Epoch: 781 lr: 0.000000; Train loss: 2.781301, Val loss: 2.703893, time: 0 s\n",
      "Epoch: 782 lr: 0.000000; Train loss: 2.781688, Val loss: 2.703891, time: 0 s\n",
      "Epoch: 783 lr: 0.000000; Train loss: 2.781770, Val loss: 2.703855, time: 1 s\n",
      "Epoch: 784 lr: 0.000000; Train loss: 2.781299, Val loss: 2.703861, time: 0 s\n",
      "Epoch: 785 lr: 0.000000; Train loss: 2.780501, Val loss: 2.703877, time: 0 s\n",
      "Epoch: 786 lr: 0.000000; Train loss: 2.781927, Val loss: 2.703900, time: 0 s\n",
      "Epoch: 787 lr: 0.000000; Train loss: 2.781556, Val loss: 2.703884, time: 0 s\n",
      "Epoch: 788 lr: 0.000000; Train loss: 2.781214, Val loss: 2.703839, time: 0 s\n",
      "Epoch: 789 lr: 0.000000; Train loss: 2.782092, Val loss: 2.703834, time: 0 s\n",
      "Epoch: 790 lr: 0.000000; Train loss: 2.781151, Val loss: 2.703844, time: 0 s\n",
      "Epoch: 791 lr: 0.000000; Train loss: 2.781552, Val loss: 2.703840, time: 0 s\n",
      "Epoch: 792 lr: 0.000000; Train loss: 2.781655, Val loss: 2.703807, time: 0 s\n",
      "Epoch: 793 lr: 0.000000; Train loss: 2.780933, Val loss: 2.703845, time: 0 s\n",
      "Epoch: 794 lr: 0.000000; Train loss: 2.780646, Val loss: 2.703873, time: 0 s\n",
      "Epoch: 795 lr: 0.000000; Train loss: 2.780527, Val loss: 2.703854, time: 0 s\n",
      "Epoch: 796 lr: 0.000000; Train loss: 2.780676, Val loss: 2.703862, time: 1 s\n",
      "Epoch: 797 lr: 0.000000; Train loss: 2.780351, Val loss: 2.703916, time: 0 s\n",
      "Epoch: 798 lr: 0.000000; Train loss: 2.781722, Val loss: 2.703906, time: 0 s\n",
      "Epoch: 799 lr: 0.000000; Train loss: 2.781692, Val loss: 2.703897, time: 0 s\n",
      "Epoch: 800 lr: 0.000000; Train loss: 2.781245, Val loss: 2.703942, time: 0 s\n",
      "Epoch: 801 lr: 0.000000; Train loss: 2.781682, Val loss: 2.703907, time: 0 s\n",
      "Epoch: 802 lr: 0.000000; Train loss: 2.780986, Val loss: 2.703891, time: 0 s\n",
      "Epoch: 803 lr: 0.000000; Train loss: 2.781089, Val loss: 2.703836, time: 0 s\n",
      "Epoch: 804 lr: 0.000000; Train loss: 2.781240, Val loss: 2.703857, time: 0 s\n",
      "Epoch: 805 lr: 0.000000; Train loss: 2.780559, Val loss: 2.703873, time: 0 s\n",
      "Epoch: 806 lr: 0.000000; Train loss: 2.781881, Val loss: 2.703884, time: 1 s\n",
      "Epoch: 807 lr: 0.000000; Train loss: 2.781831, Val loss: 2.703808, time: 0 s\n",
      "Epoch: 808 lr: 0.000000; Train loss: 2.782483, Val loss: 2.703824, time: 0 s\n",
      "Epoch: 809 lr: 0.000000; Train loss: 2.781183, Val loss: 2.703844, time: 0 s\n",
      "Epoch: 810 lr: 0.000000; Train loss: 2.781162, Val loss: 2.703796, time: 0 s\n",
      "Epoch: 811 lr: 0.000000; Train loss: 2.780797, Val loss: 2.703801, time: 0 s\n",
      "Epoch: 812 lr: 0.000000; Train loss: 2.780160, Val loss: 2.703763, time: 0 s\n",
      "Epoch: 813 lr: 0.000000; Train loss: 2.780989, Val loss: 2.703754, time: 0 s\n",
      "Epoch: 814 lr: 0.000000; Train loss: 2.780627, Val loss: 2.703701, time: 0 s\n",
      "Epoch: 815 lr: 0.000000; Train loss: 2.780685, Val loss: 2.703727, time: 0 s\n",
      "Epoch: 816 lr: 0.000000; Train loss: 2.780909, Val loss: 2.703748, time: 0 s\n",
      "Epoch: 817 lr: 0.000000; Train loss: 2.781542, Val loss: 2.703718, time: 0 s\n",
      "saved 22.04.2023_16.16.57.134564_epoch_818_loss_2.7037\n",
      "Epoch: 818 lr: 0.000000; Train loss: 2.781852, Val loss: 2.703669, time: 0 s\n",
      "Epoch: 819 lr: 0.000000; Train loss: 2.780901, Val loss: 2.703697, time: 0 s\n",
      "Epoch: 820 lr: 0.000000; Train loss: 2.781009, Val loss: 2.703739, time: 0 s\n",
      "Epoch: 821 lr: 0.000000; Train loss: 2.781484, Val loss: 2.703787, time: 0 s\n",
      "Epoch: 822 lr: 0.000000; Train loss: 2.781625, Val loss: 2.703780, time: 0 s\n",
      "Epoch: 823 lr: 0.000000; Train loss: 2.780747, Val loss: 2.703837, time: 0 s\n",
      "Epoch: 824 lr: 0.000000; Train loss: 2.781060, Val loss: 2.703871, time: 0 s\n",
      "Epoch: 825 lr: 0.000000; Train loss: 2.781359, Val loss: 2.703889, time: 0 s\n",
      "Epoch: 826 lr: 0.000000; Train loss: 2.780751, Val loss: 2.703824, time: 0 s\n",
      "Epoch: 827 lr: 0.000000; Train loss: 2.781448, Val loss: 2.703839, time: 0 s\n",
      "Epoch: 828 lr: 0.000000; Train loss: 2.780211, Val loss: 2.703864, time: 0 s\n",
      "Epoch: 829 lr: 0.000000; Train loss: 2.780277, Val loss: 2.703875, time: 0 s\n",
      "Epoch: 830 lr: 0.000000; Train loss: 2.780871, Val loss: 2.703922, time: 0 s\n",
      "Epoch: 831 lr: 0.000000; Train loss: 2.781394, Val loss: 2.703917, time: 0 s\n",
      "Epoch: 832 lr: 0.000000; Train loss: 2.780848, Val loss: 2.703902, time: 0 s\n",
      "Epoch: 833 lr: 0.000000; Train loss: 2.780985, Val loss: 2.703913, time: 0 s\n",
      "Epoch: 834 lr: 0.000000; Train loss: 2.781462, Val loss: 2.703944, time: 0 s\n",
      "Epoch: 835 lr: 0.000000; Train loss: 2.780724, Val loss: 2.703909, time: 0 s\n",
      "Epoch: 836 lr: 0.000000; Train loss: 2.781447, Val loss: 2.703837, time: 0 s\n",
      "Epoch: 837 lr: 0.000000; Train loss: 2.780372, Val loss: 2.703824, time: 0 s\n",
      "Epoch: 838 lr: 0.000000; Train loss: 2.781624, Val loss: 2.703851, time: 0 s\n",
      "Epoch: 839 lr: 0.000000; Train loss: 2.780764, Val loss: 2.703856, time: 0 s\n",
      "Epoch: 840 lr: 0.000000; Train loss: 2.782617, Val loss: 2.703795, time: 0 s\n",
      "Epoch: 841 lr: 0.000000; Train loss: 2.781037, Val loss: 2.703796, time: 0 s\n",
      "Epoch: 842 lr: 0.000000; Train loss: 2.781778, Val loss: 2.703800, time: 1 s\n",
      "Epoch: 843 lr: 0.000000; Train loss: 2.780979, Val loss: 2.703858, time: 0 s\n",
      "Epoch: 844 lr: 0.000000; Train loss: 2.780759, Val loss: 2.703877, time: 0 s\n",
      "Epoch: 845 lr: 0.000000; Train loss: 2.781669, Val loss: 2.703828, time: 0 s\n",
      "Epoch: 846 lr: 0.000000; Train loss: 2.781713, Val loss: 2.703831, time: 0 s\n",
      "Epoch: 847 lr: 0.000000; Train loss: 2.780886, Val loss: 2.703876, time: 0 s\n",
      "Epoch: 848 lr: 0.000000; Train loss: 2.780748, Val loss: 2.703887, time: 0 s\n",
      "Epoch: 849 lr: 0.000000; Train loss: 2.781519, Val loss: 2.703891, time: 0 s\n",
      "Epoch: 850 lr: 0.000000; Train loss: 2.780511, Val loss: 2.703894, time: 0 s\n",
      "Epoch: 851 lr: 0.000000; Train loss: 2.781616, Val loss: 2.703899, time: 0 s\n",
      "Epoch: 852 lr: 0.000000; Train loss: 2.781235, Val loss: 2.703891, time: 0 s\n",
      "Epoch: 853 lr: 0.000000; Train loss: 2.781426, Val loss: 2.703878, time: 0 s\n",
      "Epoch: 854 lr: 0.000000; Train loss: 2.782353, Val loss: 2.703837, time: 0 s\n",
      "Epoch: 855 lr: 0.000000; Train loss: 2.781504, Val loss: 2.703899, time: 0 s\n",
      "Epoch: 856 lr: 0.000000; Train loss: 2.781385, Val loss: 2.703858, time: 0 s\n",
      "Epoch: 857 lr: 0.000000; Train loss: 2.781245, Val loss: 2.703902, time: 0 s\n",
      "Epoch: 858 lr: 0.000000; Train loss: 2.780890, Val loss: 2.703905, time: 0 s\n",
      "Epoch: 859 lr: 0.000000; Train loss: 2.780303, Val loss: 2.703907, time: 1 s\n",
      "Epoch: 860 lr: 0.000000; Train loss: 2.782016, Val loss: 2.703884, time: 0 s\n",
      "Epoch: 861 lr: 0.000000; Train loss: 2.781980, Val loss: 2.703858, time: 0 s\n",
      "Epoch: 862 lr: 0.000000; Train loss: 2.780810, Val loss: 2.703831, time: 0 s\n",
      "Epoch: 863 lr: 0.000000; Train loss: 2.781762, Val loss: 2.703830, time: 1 s\n",
      "Epoch: 864 lr: 0.000000; Train loss: 2.781070, Val loss: 2.703855, time: 1 s\n",
      "Epoch: 865 lr: 0.000000; Train loss: 2.781379, Val loss: 2.703833, time: 1 s\n",
      "Epoch: 866 lr: 0.000000; Train loss: 2.781432, Val loss: 2.703832, time: 0 s\n",
      "Epoch: 867 lr: 0.000000; Train loss: 2.781626, Val loss: 2.703852, time: 1 s\n",
      "Epoch: 868 lr: 0.000000; Train loss: 2.782251, Val loss: 2.703815, time: 0 s\n",
      "Epoch: 869 lr: 0.000000; Train loss: 2.780870, Val loss: 2.703801, time: 1 s\n",
      "Epoch: 870 lr: 0.000000; Train loss: 2.781098, Val loss: 2.703773, time: 1 s\n",
      "Epoch: 871 lr: 0.000000; Train loss: 2.781222, Val loss: 2.703780, time: 0 s\n",
      "Epoch: 872 lr: 0.000000; Train loss: 2.780931, Val loss: 2.703797, time: 1 s\n",
      "Epoch: 873 lr: 0.000000; Train loss: 2.781342, Val loss: 2.703889, time: 0 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 874 lr: 0.000000; Train loss: 2.782279, Val loss: 2.703851, time: 0 s\n",
      "Epoch: 875 lr: 0.000000; Train loss: 2.780542, Val loss: 2.703843, time: 0 s\n",
      "Epoch: 876 lr: 0.000000; Train loss: 2.780006, Val loss: 2.703885, time: 0 s\n",
      "Epoch: 877 lr: 0.000000; Train loss: 2.780617, Val loss: 2.703882, time: 0 s\n",
      "Epoch: 878 lr: 0.000000; Train loss: 2.780962, Val loss: 2.703894, time: 0 s\n",
      "Epoch: 879 lr: 0.000000; Train loss: 2.780428, Val loss: 2.703902, time: 0 s\n",
      "Epoch: 880 lr: 0.000000; Train loss: 2.779968, Val loss: 2.703886, time: 0 s\n",
      "Epoch: 881 lr: 0.000000; Train loss: 2.781130, Val loss: 2.703924, time: 0 s\n",
      "Epoch: 882 lr: 0.000000; Train loss: 2.781114, Val loss: 2.703885, time: 0 s\n",
      "Epoch: 883 lr: 0.000000; Train loss: 2.781507, Val loss: 2.703851, time: 0 s\n",
      "Epoch: 884 lr: 0.000000; Train loss: 2.781290, Val loss: 2.703812, time: 0 s\n",
      "Epoch: 885 lr: 0.000000; Train loss: 2.782047, Val loss: 2.703798, time: 0 s\n",
      "Epoch: 886 lr: 0.000000; Train loss: 2.781635, Val loss: 2.703832, time: 0 s\n",
      "Epoch: 887 lr: 0.000000; Train loss: 2.782149, Val loss: 2.703840, time: 0 s\n",
      "Epoch: 888 lr: 0.000000; Train loss: 2.781469, Val loss: 2.703893, time: 0 s\n",
      "Epoch: 889 lr: 0.000000; Train loss: 2.781436, Val loss: 2.703850, time: 0 s\n",
      "Epoch: 890 lr: 0.000000; Train loss: 2.780499, Val loss: 2.703831, time: 0 s\n",
      "Epoch: 891 lr: 0.000000; Train loss: 2.780871, Val loss: 2.703837, time: 0 s\n",
      "Epoch: 892 lr: 0.000000; Train loss: 2.780632, Val loss: 2.703840, time: 0 s\n",
      "Epoch: 893 lr: 0.000000; Train loss: 2.781367, Val loss: 2.703862, time: 0 s\n",
      "Epoch: 894 lr: 0.000000; Train loss: 2.781883, Val loss: 2.703851, time: 0 s\n",
      "Epoch: 895 lr: 0.000000; Train loss: 2.780956, Val loss: 2.703812, time: 0 s\n",
      "Epoch: 896 lr: 0.000000; Train loss: 2.780954, Val loss: 2.703785, time: 0 s\n",
      "Epoch: 897 lr: 0.000000; Train loss: 2.782393, Val loss: 2.703746, time: 0 s\n",
      "Epoch: 898 lr: 0.000000; Train loss: 2.779975, Val loss: 2.703756, time: 0 s\n",
      "Epoch: 899 lr: 0.000000; Train loss: 2.781277, Val loss: 2.703810, time: 0 s\n",
      "Epoch: 900 lr: 0.000000; Train loss: 2.781180, Val loss: 2.703778, time: 0 s\n",
      "Epoch: 901 lr: 0.000000; Train loss: 2.781370, Val loss: 2.703799, time: 0 s\n",
      "Epoch: 902 lr: 0.000000; Train loss: 2.781361, Val loss: 2.703806, time: 0 s\n",
      "Epoch: 903 lr: 0.000000; Train loss: 2.781368, Val loss: 2.703782, time: 0 s\n",
      "Epoch: 904 lr: 0.000000; Train loss: 2.780785, Val loss: 2.703744, time: 0 s\n",
      "Epoch: 905 lr: 0.000000; Train loss: 2.780605, Val loss: 2.703757, time: 0 s\n",
      "Epoch: 906 lr: 0.000000; Train loss: 2.780639, Val loss: 2.703754, time: 0 s\n",
      "Epoch: 907 lr: 0.000000; Train loss: 2.780474, Val loss: 2.703783, time: 0 s\n",
      "Epoch: 908 lr: 0.000000; Train loss: 2.780597, Val loss: 2.703771, time: 0 s\n",
      "Epoch: 909 lr: 0.000000; Train loss: 2.782136, Val loss: 2.703754, time: 0 s\n",
      "Epoch: 910 lr: 0.000000; Train loss: 2.781098, Val loss: 2.703850, time: 0 s\n",
      "Epoch: 911 lr: 0.000000; Train loss: 2.781838, Val loss: 2.703822, time: 0 s\n",
      "Epoch: 912 lr: 0.000000; Train loss: 2.780015, Val loss: 2.703822, time: 0 s\n",
      "Epoch: 913 lr: 0.000000; Train loss: 2.779871, Val loss: 2.703811, time: 0 s\n",
      "Epoch: 914 lr: 0.000000; Train loss: 2.780313, Val loss: 2.703840, time: 0 s\n",
      "Epoch: 915 lr: 0.000000; Train loss: 2.781887, Val loss: 2.703786, time: 0 s\n",
      "Epoch: 916 lr: 0.000000; Train loss: 2.782011, Val loss: 2.703737, time: 0 s\n",
      "Epoch: 917 lr: 0.000000; Train loss: 2.782031, Val loss: 2.703792, time: 0 s\n",
      "Epoch: 918 lr: 0.000000; Train loss: 2.780859, Val loss: 2.703809, time: 0 s\n",
      "Epoch: 919 lr: 0.000000; Train loss: 2.780698, Val loss: 2.703853, time: 0 s\n",
      "Epoch: 920 lr: 0.000000; Train loss: 2.781034, Val loss: 2.703840, time: 0 s\n",
      "Epoch: 921 lr: 0.000000; Train loss: 2.780745, Val loss: 2.703924, time: 0 s\n",
      "Epoch: 922 lr: 0.000000; Train loss: 2.781081, Val loss: 2.703920, time: 1 s\n",
      "Epoch: 923 lr: 0.000000; Train loss: 2.780817, Val loss: 2.703909, time: 0 s\n",
      "Epoch: 924 lr: 0.000000; Train loss: 2.781282, Val loss: 2.703942, time: 0 s\n",
      "Epoch: 925 lr: 0.000000; Train loss: 2.779949, Val loss: 2.703975, time: 0 s\n",
      "Epoch: 926 lr: 0.000000; Train loss: 2.780387, Val loss: 2.703999, time: 1 s\n",
      "Epoch: 927 lr: 0.000000; Train loss: 2.780613, Val loss: 2.703946, time: 0 s\n",
      "Epoch: 928 lr: 0.000000; Train loss: 2.780995, Val loss: 2.703963, time: 0 s\n",
      "Epoch: 929 lr: 0.000000; Train loss: 2.781085, Val loss: 2.703935, time: 0 s\n",
      "Epoch: 930 lr: 0.000000; Train loss: 2.781596, Val loss: 2.703903, time: 0 s\n",
      "Epoch: 931 lr: 0.000000; Train loss: 2.781982, Val loss: 2.703910, time: 0 s\n",
      "Epoch: 932 lr: 0.000000; Train loss: 2.781645, Val loss: 2.703809, time: 0 s\n",
      "Epoch: 933 lr: 0.000000; Train loss: 2.781906, Val loss: 2.703729, time: 0 s\n",
      "Epoch: 934 lr: 0.000000; Train loss: 2.780643, Val loss: 2.703783, time: 0 s\n",
      "Epoch: 935 lr: 0.000000; Train loss: 2.781234, Val loss: 2.703767, time: 0 s\n",
      "Epoch: 936 lr: 0.000000; Train loss: 2.780268, Val loss: 2.703843, time: 0 s\n",
      "Epoch: 937 lr: 0.000000; Train loss: 2.780921, Val loss: 2.703891, time: 0 s\n",
      "Epoch: 938 lr: 0.000000; Train loss: 2.781303, Val loss: 2.703913, time: 0 s\n",
      "Epoch: 939 lr: 0.000000; Train loss: 2.780810, Val loss: 2.703921, time: 0 s\n",
      "Epoch: 940 lr: 0.000000; Train loss: 2.780746, Val loss: 2.703907, time: 0 s\n",
      "Epoch: 941 lr: 0.000000; Train loss: 2.781278, Val loss: 2.703947, time: 0 s\n",
      "Epoch: 942 lr: 0.000000; Train loss: 2.780918, Val loss: 2.703920, time: 0 s\n",
      "Epoch: 943 lr: 0.000000; Train loss: 2.781292, Val loss: 2.703952, time: 0 s\n",
      "Epoch: 944 lr: 0.000000; Train loss: 2.780842, Val loss: 2.703927, time: 0 s\n",
      "Epoch: 945 lr: 0.000000; Train loss: 2.781490, Val loss: 2.703893, time: 0 s\n",
      "Epoch: 946 lr: 0.000000; Train loss: 2.780102, Val loss: 2.703929, time: 0 s\n",
      "Epoch: 947 lr: 0.000000; Train loss: 2.780635, Val loss: 2.703937, time: 0 s\n",
      "Epoch: 948 lr: 0.000000; Train loss: 2.780888, Val loss: 2.703919, time: 0 s\n",
      "Epoch: 949 lr: 0.000000; Train loss: 2.781983, Val loss: 2.703900, time: 0 s\n",
      "Epoch: 950 lr: 0.000000; Train loss: 2.782227, Val loss: 2.703901, time: 0 s\n",
      "Epoch: 951 lr: 0.000000; Train loss: 2.780973, Val loss: 2.703896, time: 0 s\n",
      "Epoch: 952 lr: 0.000000; Train loss: 2.781628, Val loss: 2.703817, time: 0 s\n",
      "Epoch: 953 lr: 0.000000; Train loss: 2.781225, Val loss: 2.703843, time: 0 s\n",
      "Epoch: 954 lr: 0.000000; Train loss: 2.781162, Val loss: 2.703824, time: 0 s\n",
      "Epoch: 955 lr: 0.000000; Train loss: 2.781061, Val loss: 2.703833, time: 0 s\n",
      "Epoch: 956 lr: 0.000000; Train loss: 2.782264, Val loss: 2.703818, time: 0 s\n",
      "Epoch: 957 lr: 0.000000; Train loss: 2.781510, Val loss: 2.703756, time: 0 s\n",
      "Epoch: 958 lr: 0.000000; Train loss: 2.780193, Val loss: 2.703817, time: 1 s\n",
      "Epoch: 959 lr: 0.000000; Train loss: 2.780715, Val loss: 2.703835, time: 0 s\n",
      "Epoch: 960 lr: 0.000000; Train loss: 2.781007, Val loss: 2.703765, time: 1 s\n",
      "Epoch: 961 lr: 0.000000; Train loss: 2.781173, Val loss: 2.703791, time: 0 s\n",
      "Epoch: 962 lr: 0.000000; Train loss: 2.781066, Val loss: 2.703807, time: 0 s\n",
      "Epoch: 963 lr: 0.000000; Train loss: 2.780267, Val loss: 2.703856, time: 0 s\n",
      "Epoch: 964 lr: 0.000000; Train loss: 2.780994, Val loss: 2.703851, time: 0 s\n",
      "Epoch: 965 lr: 0.000000; Train loss: 2.781025, Val loss: 2.703827, time: 0 s\n",
      "Epoch: 966 lr: 0.000000; Train loss: 2.781206, Val loss: 2.703817, time: 0 s\n",
      "Epoch: 967 lr: 0.000000; Train loss: 2.781025, Val loss: 2.703786, time: 1 s\n",
      "Epoch: 968 lr: 0.000000; Train loss: 2.781176, Val loss: 2.703779, time: 0 s\n",
      "Epoch: 969 lr: 0.000000; Train loss: 2.781643, Val loss: 2.703813, time: 0 s\n",
      "Epoch: 970 lr: 0.000000; Train loss: 2.780729, Val loss: 2.703777, time: 0 s\n",
      "Epoch: 971 lr: 0.000000; Train loss: 2.780742, Val loss: 2.703818, time: 0 s\n",
      "Epoch: 972 lr: 0.000000; Train loss: 2.781631, Val loss: 2.703784, time: 0 s\n",
      "Epoch: 973 lr: 0.000000; Train loss: 2.780797, Val loss: 2.703809, time: 0 s\n",
      "Epoch: 974 lr: 0.000000; Train loss: 2.781762, Val loss: 2.703847, time: 0 s\n",
      "Epoch: 975 lr: 0.000000; Train loss: 2.781952, Val loss: 2.703892, time: 0 s\n",
      "Epoch: 976 lr: 0.000000; Train loss: 2.779696, Val loss: 2.703941, time: 0 s\n",
      "Epoch: 977 lr: 0.000000; Train loss: 2.780468, Val loss: 2.703959, time: 1 s\n",
      "Epoch: 978 lr: 0.000000; Train loss: 2.782201, Val loss: 2.703953, time: 0 s\n",
      "Epoch: 979 lr: 0.000000; Train loss: 2.780560, Val loss: 2.703910, time: 1 s\n",
      "Epoch: 980 lr: 0.000000; Train loss: 2.780869, Val loss: 2.703904, time: 0 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 981 lr: 0.000000; Train loss: 2.781690, Val loss: 2.703940, time: 0 s\n",
      "Epoch: 982 lr: 0.000000; Train loss: 2.782317, Val loss: 2.703967, time: 0 s\n",
      "Epoch: 983 lr: 0.000000; Train loss: 2.781117, Val loss: 2.703956, time: 0 s\n",
      "Epoch: 984 lr: 0.000000; Train loss: 2.780621, Val loss: 2.703990, time: 0 s\n",
      "Epoch: 985 lr: 0.000000; Train loss: 2.781952, Val loss: 2.703960, time: 0 s\n",
      "Epoch: 986 lr: 0.000000; Train loss: 2.780439, Val loss: 2.704021, time: 0 s\n",
      "Epoch: 987 lr: 0.000000; Train loss: 2.781010, Val loss: 2.703997, time: 0 s\n",
      "Epoch: 988 lr: 0.000000; Train loss: 2.782502, Val loss: 2.703928, time: 1 s\n",
      "Epoch: 989 lr: 0.000000; Train loss: 2.780481, Val loss: 2.703894, time: 0 s\n",
      "Epoch: 990 lr: 0.000000; Train loss: 2.781449, Val loss: 2.703899, time: 0 s\n",
      "Epoch: 991 lr: 0.000000; Train loss: 2.781580, Val loss: 2.703876, time: 0 s\n",
      "Epoch: 992 lr: 0.000000; Train loss: 2.781034, Val loss: 2.703891, time: 0 s\n",
      "Epoch: 993 lr: 0.000000; Train loss: 2.781375, Val loss: 2.703834, time: 0 s\n",
      "Epoch: 994 lr: 0.000000; Train loss: 2.780620, Val loss: 2.703888, time: 0 s\n",
      "Epoch: 995 lr: 0.000000; Train loss: 2.781440, Val loss: 2.703884, time: 0 s\n",
      "Epoch: 996 lr: 0.000000; Train loss: 2.781237, Val loss: 2.703858, time: 0 s\n",
      "Epoch: 997 lr: 0.000000; Train loss: 2.781754, Val loss: 2.703827, time: 0 s\n",
      "Epoch: 998 lr: 0.000000; Train loss: 2.780831, Val loss: 2.703870, time: 0 s\n",
      "Epoch: 999 lr: 0.000000; Train loss: 2.781884, Val loss: 2.703880, time: 0 s\n",
      "end!\n",
      "22.04.2023_16.16.57.134564_epoch_818_loss_2.7037\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
    "\n",
    "\n",
    "bet_model_name = train_model(0,\n",
    "    nn_model, \n",
    "    DataLoader(dataset_train, batch_size=100000),\n",
    "    DataLoader(dataset_test, batch_size=100000),\n",
    "    RMSE_loss, optimizer, 1000, scheduler, loss_train_history, loss_val_history)\n",
    "print('end!')\n",
    "print(bet_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a47b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfae57ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 8))    \n",
    "plt.xlabel(\"#iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_train_history, label='train loss')\n",
    "plt.plot(loss_val_history, label='val loss')\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a96b83e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05676cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e517776",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FeaturesDataset(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb5f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
    "\n",
    "loss_train_history, loss_val_history, best_model_name = train_model(\n",
    "    nn_model, \n",
    "    DataLoader(dataset, batch_size=1000),\n",
    "    DataLoader(dataset_test, batch_size=300),\n",
    "    loss_func, optimizer, 100, scheduler)\n",
    "print('end!')\n",
    "print(best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e15ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))    \n",
    "plt.xlabel(\"#iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_train_history, label='train loss')\n",
    "plt.plot(loss_val_history, label='val loss')\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c2f71a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85be58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f425b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699404c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62771c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c0126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6118d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = VAEModel(input_shape, seed=1024).to(device)\n",
    "print(compute_accuracy(model2, DataLoader(dataset_test, batch_size=300), loss_func))\n",
    "model2.load_state_dict(torch.load(\"checkpoint.pth\"))\n",
    "print(compute_accuracy(model2, DataLoader(dataset_test, batch_size=300), loss_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a9c82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = VAEModel(input_shape, seed=1024).to(device)\n",
    "print(compute_accuracy(model2, DataLoader(dataset_test, batch_size=300), loss_func))\n",
    "save(nn_model, '111')\n",
    "\n",
    "load2('111', model2)\n",
    "#model2.load_state_dict(torch.load(\"checkpoint.pth\"))\n",
    "print(compute_accuracy(model2, DataLoader(dataset_test, batch_size=300), loss_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c19a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed18849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model_name = '21.04.2023_17.40.39.101646_epoch_88_loss_0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c56a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = VAEModel(input_shape, seed=1024).to(device)\n",
    "print(compute_accuracy(best_model, DataLoader(dataset_test, batch_size=300), loss_func))\n",
    "\n",
    "load2(best_model_name, best_model)\n",
    "print(compute_accuracy(best_model, DataLoader(dataset_test, batch_size=300), loss_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1454e845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ece1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5c308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_ds = FeaturesDataset(torch.diag(torch.ones(cols.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bccd634",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_ds[:3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5020d539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9838f1de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da355c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in tqdm(DataLoader(emb_ds, batch_size=len(emb_ds))):\n",
    "    artist_emb = nn_model.encode(x).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de097832",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354daae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582f320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a7e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be8403d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    if isinstance(color, str): color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: pl.show(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee022af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a29e7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd698624",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_emb_tsne = TSNE(2).fit_transform(artist_emb)\n",
    "artist_emb_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d346db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors(artist_emb_tsne[:, 0], artist_emb_tsne[:, 1], token=artists_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef05ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = KMeans(148).fit_predict(artist_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e4028",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors(artist_emb_tsne[:, 0], artist_emb_tsne[:, 1], color=[_colors[l] for l in labels],\n",
    "             token=artists_arr,\n",
    "             radius=20,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf218ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df['label'] = labels\n",
    "titles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfa8ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df[titles_df['artist_name'] == 'Nirvana']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbab844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df[titles_df['label'] == 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dea8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(labels)\n",
    "sorted(count.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda804f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9333b88f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d0344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d03df38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe40b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pca = PCA(2)\n",
    "scaler = StandardScaler()\n",
    "artist_emb_pca = pca.fit_transform(artist_emb)\n",
    "artist_emb_pca\n",
    "#word_vectors_pca = scaler.fit_transform(word_vectors_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_emb_pca = scaler.fit_transform(artist_emb_pca)\n",
    "artist_emb_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c62838",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_arr = list(titles_dict.values())\n",
    "artists_arr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d30c6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors(artist_emb_pca[:, 0], artist_emb_pca[:, 1], token=artists_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc123eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cdb2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27604665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c95cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = umap.UMAP(n_neighbors=5).fit_transform(artist_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f182a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors(embedding[:, 0], embedding[:, 1], token=artists_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d4700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(data_vectors, query_vector, k=10):\n",
    "    \"\"\"\n",
    "    given text line (query), return k most similar lines from data, sorted from most to least similar\n",
    "    similarity should be measured as cosine between query and line embedding vectors\n",
    "    hint: it's okay to use global variables: data and data_vectors. see also: np.argpartition, np.argsort\n",
    "    \"\"\"\n",
    "    dists = data_vectors.dot(query_vector[:, None])[:, 0] / ((norms+1e-16)*np.linalg.norm(query_vector))\n",
    "    nearest_elements = dists.argsort(axis=0)[-k:][::-1]\n",
    "    out = [data[i] for i in nearest_elements]\n",
    "    return out# <YOUR CODE: top-k lines starting from most similar>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0fcf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f624879",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcolors.CSS4_COLORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a399f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mcolors.CSS4_COLORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b32afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81496228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a99b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = KMeans(148).fit_predict(artist_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f8e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_colors = list(mcolors.CSS4_COLORS.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e356233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors(artist_emb_pca[:, 0], artist_emb_pca[:, 1], color=[_colors[l] for l in labels],\n",
    "             token=artists_arr,\n",
    "             radius=20,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3258e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dbscan = DBSCAN(eps=0.66).fit_predict(artist_emb)\n",
    "display(np.unique(labels_dbscan))\n",
    "count = Counter(labels_dbscan)\n",
    "sorted(count.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf7cee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df['label'] = labels_dbscan\n",
    "titles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1808da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df[titles_df['label'] == 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d634cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8352ca8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e604f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c63e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors(artist_emb_pca[:, 0], artist_emb_pca[:, 1], color=[_colors[l] for l in labels_dbscan],\n",
    "             token=artists_arr,\n",
    "             radius=20,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba99c943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5717c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd8bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = KMeans(300).fit_predict(artist_emb)\n",
    "\n",
    "titles_df['label'] = labels\n",
    "titles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d850e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2d5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b8859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb19f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df[titles_df['artist_name'] == 'Nirvana']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60100a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df[titles_df['label'] == 225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd790d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f0d522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37c524d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ee9f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
