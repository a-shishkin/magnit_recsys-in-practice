{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dcbea09",
   "metadata": {},
   "source": [
    "# Comparing matrix factorization with transformers for MovieLens recommendations using PyTorch-accelerated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e4220e",
   "metadata": {},
   "source": [
    "By Chris Hughes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d206280",
   "metadata": {},
   "source": [
    "The package versions used are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7145f3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5666bf1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e77708af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca536465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ffeec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d72ab90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06045473",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_joke_df = pd.read_csv(r'data\\recsys-in-practice\\train_joke_df.csv')\n",
    "sample_submission = pd.read_csv(r'data\\recsys-in-practice\\sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae91343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d7fdcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eadcceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(train_joke_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b569c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f717bdff",
   "metadata": {},
   "source": [
    "Even when considering model benchmarks on the same dataset, to have a fair comparison, it is important to understand how the data has been split and to make sure that the approaches taken are consistent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f180563",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_lookup = {v: i+1 for i, v in enumerate(train_joke_df[\"UID\"].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99971bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_lookup = {v: i+1 for i, v in enumerate(train_joke_df[\"JID\"].unique())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aca6eda",
   "metadata": {},
   "source": [
    "Now that we can encode our features, as we are using PyTorch, we need to define a Dataset to wrap our DataFrame and return the user-item ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d8e1a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9909b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class UserItemRatingDataset(Dataset):\n",
    "    def __init__(self, df, movie_lookup, user_lookup):\n",
    "        self.df = df\n",
    "        self.movie_lookup = movie_lookup\n",
    "        self.user_lookup = user_lookup\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        user_id = torch.tensor(self.user_lookup[row.UID]).to(device)\n",
    "        movie_id = torch.tensor(self.movie_lookup[row.JID]).to(device)\n",
    "        \n",
    "        rating = torch.tensor(row.Rating, dtype=torch.float32).to(device)\n",
    "        \n",
    "        return (user_id, movie_id), rating\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38640a64",
   "metadata": {},
   "source": [
    "We can now use this to create our training and validation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df035b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = UserItemRatingDataset(train_df, movie_lookup, user_lookup)\n",
    "valid_dataset = UserItemRatingDataset(valid_df, movie_lookup, user_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3933d4a3",
   "metadata": {},
   "source": [
    "Next, let's define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8f95c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MfDotBias(nn.Module):\n",
    "\n",
    "    def __init__(self, n_factors, n_users, n_items, ratings_range=None, use_biases=True):\n",
    "        super().__init__()\n",
    "        self.bias = use_biases\n",
    "        self.y_range = ratings_range\n",
    "        self.user_embedding = nn.Embedding(n_users+1, n_factors, padding_idx=0)\n",
    "        self.item_embedding = nn.Embedding(n_items+1, n_factors, padding_idx=0)\n",
    "\n",
    "        if use_biases:\n",
    "            self.user_bias = nn.Embedding(n_users+1, 1, padding_idx=0)\n",
    "            self.item_bias = nn.Embedding(n_items+1, 1, padding_idx=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        users, items = inputs\n",
    "        dot = self.user_embedding(users) * self.item_embedding(items)\n",
    "        result = dot.sum(1)\n",
    "        if self.bias:\n",
    "            result = (result + self.user_bias(users).squeeze() + self.item_bias(items).squeeze())\n",
    "\n",
    "        if self.y_range is None:\n",
    "            return result\n",
    "        else:\n",
    "            return (torch.sigmoid(result) * (self.y_range[1] - self.y_range[0]) + self.y_range[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7f7c68",
   "metadata": {},
   "source": [
    "As we can see, this is very simple to define. Note that because an embedding layer is simply a lookup table, it is important that when we specify the size of the embedding layer, it must contain any value that will be seen during training and evaluation. Because of this, we will use the number of unique items observed in the full dataset to do this, not just the training set. We have also specified a padding embedding at index 0, which can be used for any unknown values. PyTorch handles this by setting this entry to a zero-vector, which is not updated during training.\n",
    "\n",
    "Additionally, as this is a regression task, the range that the model could predict is potentially unbounded. While the model can learn to restrict the output values to between 1 and 5, we can make this easier for the model by modifying the architecture to restrict this range prior to training. We have done this by applying the sigmoid function to the model's output - which restricts the range to between 0 and 1 - and then scaling this to within a range that we can define."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541d17da",
   "metadata": {},
   "source": [
    "### Train with PyTorch accelerated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc03b619",
   "metadata": {},
   "source": [
    "At this point, we would usually start writing the training loop; however, as we are using pytorch-accelerated, this will largely be taken care of for us. However, as pytorch-accelerated tracks only the training and validation losses by default, let's create a callback to track our metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae85e7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33053722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad2a4ff4",
   "metadata": {},
   "source": [
    "Let's create a callback to track our metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc26c9d",
   "metadata": {},
   "source": [
    "Now, all that is left to do is to train the model. PyTorch-accelerated provides a notebook_launcher function, which enables us to run multi-GPU training runs from within a notebook. To use this, all we need to do is to define a training function that instantiates our Trainer object and calls the train method.\n",
    "\n",
    "Components such as the model and dataset can be defined anywhere in the notebook, but it is important that the trainer is only ever instantiated within a training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d114276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def save(model, name):\n",
    "    os.mkdir(f\"artifacts_wonderfund_v1/{name}\")\n",
    "    #torch.save(model, f\"{name}/model.pkl\")\n",
    "    torch.save(model.state_dict(), f\"artifacts_wonderfund_v1/{name}/checkpoint.pth\")\n",
    "    \n",
    "def load(name):\n",
    "    return torch.load(f\"artifacts_wonderfund_v1/{name}/model.pkl\")\n",
    "\n",
    "def load2(name, model):\n",
    "    model.load_state_dict(torch.load(f\"artifacts_wonderfund_v1/{name}/checkpoint.pth\"))\n",
    "    \n",
    "def train_model(epoch_start, model, train_loader, val_loader, loss, optimizer, num_epochs, scheduler, loss_train_history, loss_val_history):   \n",
    "    bet_model_name = None\n",
    "    best_loss = compute_accuracy(model, val_loader, loss)\n",
    "    print('loss:', best_loss)\n",
    "    for epoch in range(epoch_start, epoch_start + num_epochs):     \n",
    "        t1 = time.time()\n",
    "        model.train()   \n",
    "        loss_accum = 0\n",
    "        for i_step, (x, y) in enumerate(train_loader):\n",
    "            prediction = model(x)    \n",
    "            loss_value = loss(prediction, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()            \n",
    "            loss_accum += loss_value\n",
    "\n",
    "            \n",
    "        \n",
    "        ave_loss = loss_accum / (i_step + 1)\n",
    "        loss_val = compute_accuracy(model, val_loader, loss)\n",
    "        \n",
    "        loss_train_history.append(float(ave_loss))\n",
    "        loss_val_history.append(loss_val)\n",
    "        \n",
    "        if scheduler != None:\n",
    "            scheduler.step()\n",
    "            \n",
    "\n",
    "        if loss_val < best_loss:\n",
    "            best_loss = loss_val\n",
    "            bet_model_name = f'{datetime.datetime.now().strftime(\"%d.%m.%Y_%H.%M.%S.%f\")}_epoch_{epoch}_loss_{round(best_loss, 4)}'\n",
    "            save(model, bet_model_name)\n",
    "            print(f\"saved {bet_model_name}\")\n",
    "\n",
    "            \n",
    "        print(\"Epoch: %i lr: %f; Train loss: %f, Val loss: %f, time: %i s\" % (epoch, get_lr(optimizer), ave_loss, loss_val,\n",
    "                                                                            round(time.time() - t1)))\n",
    "    return bet_model_name\n",
    "        \n",
    "    \n",
    "def compute_accuracy(model, loader, loss):\n",
    "    \"\"\"\n",
    "    Computes accuracy on the dataset wrapped in a loader    \n",
    "    Returns: accuracy as a float value between 0 and 1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss_accum = 0\n",
    "    for i_step, (x, y) in enumerate(loader):\n",
    "        prediction = model(x)\n",
    "        loss_value = loss(prediction, y)\n",
    "        loss_accum += loss_value\n",
    "\n",
    "    ave_loss = loss_accum / (i_step + 1)         \n",
    "    return float(ave_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "030563cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE_loss(prediction, target):\n",
    "    return torch.sqrt(nn.MSELoss()(prediction, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad310c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MfDotBias(120, len(user_lookup), len(movie_lookup), ratings_range=[-10, 10]).to(device)\n",
    "loss_train_history, loss_val_history = [], []\n",
    "\n",
    "best_model_name = '22.04.2023_19.04.16.683005_epoch_14_loss_5.6441'\n",
    "load2(best_model_name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5fc6fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.644097328186035\n",
      "saved 22.04.2023_19.57.49.619482_epoch_8_loss_5.5739\n",
      "Epoch: 8 lr: 0.001000; Train loss: 3.263098, Val loss: 5.573893, time: 325 s\n",
      "saved 22.04.2023_20.03.20.593287_epoch_9_loss_5.5515\n",
      "Epoch: 9 lr: 0.001000; Train loss: 3.125072, Val loss: 5.551464, time: 331 s\n",
      "saved 22.04.2023_20.08.39.395928_epoch_10_loss_5.5431\n",
      "Epoch: 10 lr: 0.001000; Train loss: 3.056410, Val loss: 5.543149, time: 319 s\n",
      "saved 22.04.2023_20.14.01.899971_epoch_11_loss_5.541\n",
      "Epoch: 11 lr: 0.001000; Train loss: 3.000063, Val loss: 5.541049, time: 323 s\n",
      "Epoch: 12 lr: 0.000800; Train loss: 2.947003, Val loss: 5.542380, time: 322 s\n",
      "Epoch: 13 lr: 0.000800; Train loss: 2.890984, Val loss: 5.544814, time: 324 s\n",
      "Epoch: 14 lr: 0.000800; Train loss: 2.849479, Val loss: 5.548423, time: 319 s\n",
      "Epoch: 15 lr: 0.000800; Train loss: 2.808391, Val loss: 5.552837, time: 335 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.015\u001b[39m)\n\u001b[0;32m      3\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m bet_model_name \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mRMSE_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_train_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_val_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(epoch_start, model, train_loader, val_loader, loss, optimizer, num_epochs, scheduler, loss_train_history, loss_val_history)\u001b[0m\n\u001b[0;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()   \n\u001b[0;32m     23\u001b[0m loss_accum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_step, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     25\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model(x)    \n\u001b[0;32m     26\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m loss(prediction, y)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torchvision\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torchvision\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torchvision\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torchvision\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mUserItemRatingDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     11\u001b[0m user_id \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_lookup[row\u001b[38;5;241m.\u001b[39mUID])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m movie_id \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmovie_lookup[row\u001b[38;5;241m.\u001b[39mJID])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 14\u001b[0m rating \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRating\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (user_id, movie_id), rating\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.015)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
    "\n",
    "bet_model_name = train_model(8,\n",
    "    model, \n",
    "    DataLoader(train_dataset, batch_size=10000),\n",
    "    DataLoader(valid_dataset, batch_size=10000),\n",
    "    RMSE_loss, optimizer, 100, scheduler, loss_train_history, loss_val_history)\n",
    "print('end!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b996be7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7da5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = UserItemRatingDataset(train_joke_df, movie_lookup, user_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c89565d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.103021144866943\n",
      "saved 22.04.2023_22.02.45.464659_epoch_21_loss_5.0474\n",
      "Epoch: 21 lr: 0.000100; Train loss: 4.107443, Val loss: 5.047354, time: 491 s\n",
      "saved 22.04.2023_22.11.02.175916_epoch_22_loss_4.9955\n",
      "Epoch: 22 lr: 0.000100; Train loss: 4.080122, Val loss: 4.995517, time: 497 s\n",
      "saved 22.04.2023_22.19.21.217493_epoch_23_loss_4.9479\n",
      "Epoch: 23 lr: 0.000100; Train loss: 4.056099, Val loss: 4.947924, time: 499 s\n",
      "saved 22.04.2023_22.27.42.082419_epoch_24_loss_4.9043\n",
      "Epoch: 24 lr: 0.000100; Train loss: 4.034567, Val loss: 4.904278, time: 501 s\n",
      "saved 22.04.2023_22.36.11.670000_epoch_25_loss_4.8642\n",
      "Epoch: 25 lr: 0.000100; Train loss: 4.015214, Val loss: 4.864242, time: 510 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m bet_model_name \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mRMSE_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_train_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_val_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(epoch_start, model, train_loader, val_loader, loss, optimizer, num_epochs, scheduler, loss_train_history, loss_val_history)\u001b[0m\n\u001b[0;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()   \n\u001b[0;32m     23\u001b[0m loss_accum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_step, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     25\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model(x)    \n\u001b[0;32m     26\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m loss(prediction, y)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torchvision\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torchvision\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torchvision\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torchvision\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mUserItemRatingDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m     10\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[index]\n\u001b[1;32m---> 11\u001b[0m     user_id \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_lookup\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUID\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     movie_id \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmovie_lookup[row\u001b[38;5;241m.\u001b[39mJID])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m     rating \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(row\u001b[38;5;241m.\u001b[39mRating, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "bet_model_name = train_model(21,\n",
    "    model, \n",
    "    DataLoader(dataset, batch_size=10000),\n",
    "    DataLoader(valid_dataset, batch_size=10000),\n",
    "    RMSE_loss, optimizer, 6, None, loss_train_history, loss_val_history)\n",
    "print('end!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1aae0675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 8))    \n",
    "plt.xlabel(\"#iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_train_history, label='train loss')\n",
    "plt.plot(loss_val_history, label='val loss')\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8cadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7927077",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2281558f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9878e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_joke_df_nofactrating = pd.read_csv(r'data\\recsys-in-practice\\test_joke_df_nofactrating.csv', index_col=0)\n",
    "\n",
    "test_joke_df_nofactrating['Rating'] = np.zeros((len(test_joke_df_nofactrating)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f9b38e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = UserItemRatingDataset(test_joke_df_nofactrating, movie_lookup, user_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5ad658e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.251912117004395\n",
      "4.338476181030273\n"
     ]
    }
   ],
   "source": [
    "best_model_name = '22.04.2023_22.36.11.670000_epoch_25_loss_4.8642'\n",
    "\n",
    "best_model = MfDotBias(120, len(user_lookup), len(movie_lookup), ratings_range=[-10, 10]).to(device)\n",
    "print(compute_accuracy(best_model, DataLoader(test_dataset, batch_size=5000), RMSE_loss))\n",
    "\n",
    "load2(best_model_name, best_model)\n",
    "print(compute_accuracy(best_model, DataLoader(test_dataset, batch_size=5000), RMSE_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d63fda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef49092e2ef840e8aaa4b66eb5b1cae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = []\n",
    "for x, y in tqdm(DataLoader(test_dataset, batch_size=5000)):\n",
    "    predict = model(x)\n",
    "    result.extend(predict.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "041e0435",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_joke_df_nofactrating['Rating'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "995f7b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InteractionID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.364809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.937026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.542684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.760567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.356848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Rating\n",
       "InteractionID          \n",
       "0              3.364809\n",
       "1             -3.937026\n",
       "2             -2.542684\n",
       "3              6.760567\n",
       "4              5.356848"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_joke_df_nofactrating['Rating'].to_frame().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a957b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_joke_df_nofactrating['Rating'].to_frame().to_csv('nn_embedding.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c17f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3d353",
   "metadata": {},
   "source": [
    "Comparing this to our baseline, we can see that there is an improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b89b19",
   "metadata": {},
   "source": [
    "## Sequential recommendations using a transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bb857b",
   "metadata": {},
   "source": [
    "Using matrix factorization, we are treating each rating as being independent from the ratings around it; however, incorporating information about other movies that a user recently rated could provide an additional signal that could boost performance. For example, suppose that a user is watching a trilogy of films; if they have rated the first two instalments highly, it is likely that they may do the same for the finale!\n",
    "\n",
    "One way that we can approach this is to use a transformer network, specifically the encoder portion, to encode additional context into the learned embeddings for each movie, and then using a fully connected neural network to make the rating predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c308a7",
   "metadata": {},
   "source": [
    "### Pre-processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b22435",
   "metadata": {},
   "source": [
    "The first step is to process our data so that we have a time-sorted list of movies for each user. Let's start by grouping all the ratings by user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7791058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_ratings = train_joke_df.groupby('UID').agg(tuple).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ca0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f7e51",
   "metadata": {},
   "source": [
    "Now that we have grouped by user, we can create an additional column so that we can see the number of events associated with each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbf2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_ratings['num_ratings'] = grouped_ratings['Rating'].apply(lambda row: len(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103612b7",
   "metadata": {},
   "source": [
    "Let's take a look at the new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0db536",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8559845",
   "metadata": {},
   "source": [
    "Now that we have grouped all the ratings for each user, let's divide these into smaller sequences. To make the most out of the data, we would like the model to have the opportunity to predict a rating for every movie in the training set. To do this, let's specify a sequence length s and use the previous s-1 ratings as our user history.\n",
    "\n",
    "As the model expects each sequence to be a fixed length, we will fill empty spaces with a padding token, so that sequences can be batched and passed to the model. Let's create a function to do this.\n",
    "\n",
    "We are going to arbitrarily choose a length of 10 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00164996",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09119db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(values, sequence_length):\n",
    "    sequences = []\n",
    "    for i, v in enumerate(values):\n",
    "        seq = values[:i+1]\n",
    "        if len(seq) > sequence_length:\n",
    "            seq = seq[i-sequence_length+1:i+1]\n",
    "        elif len(seq) < sequence_length:\n",
    "            seq =(*(['[PAD]'] * (sequence_length - len(seq))), *seq)\n",
    "       \n",
    "        sequences.append(seq)\n",
    "    return sequences\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9310980e",
   "metadata": {},
   "source": [
    "To visualize how this function works, let's apply it, with a sequence length of 3, to the first 10 movies rated by the first user. These movies are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f08327",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_ratings.iloc[0]['title'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1db720",
   "metadata": {},
   "source": [
    "Applying our function, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee50596",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sequences(grouped_ratings.iloc[0]['JID'][:10], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c93d0cf",
   "metadata": {},
   "source": [
    "As we can see, we have 10 sequences of length 3, where the final movie in the sequence is unchanged from the original list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c79f074",
   "metadata": {},
   "source": [
    "Now, let's apply this function to all of the features in our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed431a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_cols = ['title', 'rating', 'unix_timestamp', 'is_valid'] \n",
    "for col in grouped_cols:\n",
    "    grouped_ratings[col] = grouped_ratings[col].apply(lambda x: create_sequences(x, sequence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfcae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_ratings.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5fa619",
   "metadata": {},
   "source": [
    "Currently, we have one row that contains all the sequences for a certain user. However, during training, we would like to create batches made up of sequences from many different users. To do this, we will have to transform the data so that each sequence has its own row, while remaining associated with the user ID. We can use the pandas 'explode' function for each feature, and then aggregate these DataFrames together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d0523",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_ratings = grouped_ratings[['user_id', 'title']].explode('title', ignore_index=True)\n",
    "dfs = [grouped_ratings[[col]].explode(col, ignore_index=True) for col in grouped_cols[1:]]\n",
    "seq_df = pd.concat([exploded_ratings, *dfs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5af3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea5a39",
   "metadata": {},
   "source": [
    "Now, we can see that each sequence has its own row. However, for the is_valid column, we don't care about the whole sequence and only need the last value as this is the movie for which we will be trying to predict the rating. Let's create a function to extract this value and apply it to these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9279a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_entry(sequence):\n",
    "    return sequence[-1]\n",
    "\n",
    "seq_df['is_valid'] = seq_df['is_valid'].apply(get_last_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ce4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220cd447",
   "metadata": {},
   "source": [
    "Also, to make it easy to access the rating that we are trying to predict, let's separate this into its own column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff27e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_df['target_rating'] = seq_df['rating'].apply(get_last_entry)\n",
    "seq_df['previous_ratings'] = seq_df['rating'].apply(lambda seq: seq[:-1])\n",
    "seq_df.drop(columns=['rating'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9552504c",
   "metadata": {},
   "source": [
    "To prevent the model from including padding tokens when calculating attention scores, we can provide an attention mask to the transformer; the mask should be 'True' for a padding token and 'False' otherwise. Let's calculate this for each row, as well as creating a column to show the number of padding tokens present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcd5857",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_df['pad_mask'] = seq_df['title'].apply(lambda x: (np.array(x) == '[PAD]'))\n",
    "seq_df['num_pads'] = seq_df['pad_mask'].apply(sum)\n",
    "seq_df['pad_mask'] = seq_df['pad_mask'].apply(lambda x: x.tolist()) # in case we serialize later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99057816",
   "metadata": {},
   "source": [
    "Let's inspect the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5fead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893972ae",
   "metadata": {},
   "source": [
    "All looks as it should! Let's split this into training and validation sets and save this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_df = seq_df[seq_df.is_valid == False]\n",
    "valid_seq_df = seq_df[seq_df.is_valid == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69a8a2",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46b06f",
   "metadata": {},
   "source": [
    "As we saw previously, before we can feed this data into the model, we need to create lookup tables to encode our movies and users. However, this time, we need to include the padding token in our movie lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440df620",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_lookup = {v: i+1 for i, v in enumerate(ratings_df['user_id'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6c6b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_lookup(df, feature):\n",
    "    lookup = {v: i+1 for i, v in enumerate(df[feature].unique())}\n",
    "    lookup['[PAD]'] = 0\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc3a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_lookup = create_feature_lookup(ratings_df, 'title')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21228dbc",
   "metadata": {},
   "source": [
    "Now, we are dealing with sequences of ratings, rather than individual ones, so we will need to create a new dataset to wrap our processed DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0648f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieSequenceDataset(Dataset):\n",
    "    def __init__(self, df, movie_lookup, user_lookup):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.movie_lookup = movie_lookup\n",
    "        self.user_lookup = user_lookup\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.df.iloc[index]\n",
    "        user_id = torch.tensor(self.user_lookup[str(data.user_id)])\n",
    "        movie_ids = torch.tensor([self.movie_lookup[title] for title in data.title])\n",
    "\n",
    "        previous_ratings = torch.tensor(\n",
    "            [rating if rating != \"[PAD]\" else 0 for rating in data.previous_ratings]\n",
    "        )\n",
    "\n",
    "        attention_mask = torch.tensor(data.pad_mask).to(device)\n",
    "        target_rating = data.target_rating\n",
    "        encoded_features = {\n",
    "            \"user_id\": user_id.to(device),\n",
    "            \"movie_ids\": movie_ids.to(device),\n",
    "            \"ratings\": previous_ratings.to(device),\n",
    "        }\n",
    "\n",
    "        return (encoded_features, attention_mask), torch.tensor(target_rating, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41854545",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MovieSequenceDataset(train_seq_df, movie_lookup, user_lookup)\n",
    "valid_dataset = MovieSequenceDataset(valid_seq_df, movie_lookup, user_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cee81f6",
   "metadata": {},
   "source": [
    "Now, let's define our transformer model! As a start, given that the matrix factorization model can achieve good performance using only the user and movie ids, let's only include this information for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62bc2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BstTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        movies_num_unique,\n",
    "        users_num_unique,\n",
    "        sequence_length=10,\n",
    "        embedding_size=120,\n",
    "        num_transformer_layers=1,\n",
    "        ratings_range=(0.5, 5.5),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.y_range = ratings_range\n",
    "        self.movies_embeddings = nn.Embedding(movies_num_unique + 1, embedding_size, padding_idx=0)\n",
    "        self.user_embeddings = nn.Embedding(users_num_unique + 1, embedding_size)\n",
    "        self.position_embeddings = nn.Embedding(sequence_length, embedding_size)\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_size,\n",
    "                nhead=12,\n",
    "                dropout=0.1,\n",
    "                batch_first=True,\n",
    "                activation=\"gelu\",\n",
    "            ),\n",
    "            num_layers=num_transformer_layers,\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embedding_size + (embedding_size * sequence_length), 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Mish(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        features, mask = inputs\n",
    "\n",
    "        encoded_user_id = self.user_embeddings(features[\"user_id\"])\n",
    "\n",
    "        user_features = encoded_user_id\n",
    "\n",
    "        encoded_movies = self.movies_embeddings(features[\"movie_ids\"])\n",
    "\n",
    "        positions = torch.arange(0, self.sequence_length, 1, dtype=int, device=features[\"movie_ids\"].device)\n",
    "        positions = self.position_embeddings(positions)\n",
    "\n",
    "        transformer_features = encoded_movies + positions\n",
    "\n",
    "        transformer_output = self.encoder(transformer_features, src_key_padding_mask=mask)\n",
    "        transformer_output = torch.flatten(transformer_output, start_dim=1)\n",
    "\n",
    "        combined_output = torch.cat((transformer_output, user_features), dim=1)\n",
    "\n",
    "        rating = self.linear(combined_output)\n",
    "        rating = rating.squeeze()\n",
    "        if self.y_range is None:\n",
    "            return rating\n",
    "        else:\n",
    "            return rating * (self.y_range[1] - self.y_range[0]) + self.y_range[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26939213",
   "metadata": {},
   "source": [
    "We can see that, as a default, we feed our sequence of movie embeddings into a single transformer layer, before concatenating the output with the user features - here, just the user ID - and using this as the input to a fully connected network. Here, we are using only a simple positional encoding that is learned to represent the sequence in which the movies were rated; using a sine- and cosine-based approach provided no benefit during my experiments, but feel free to try it out if you are interested!\n",
    "\n",
    "Once again, let's define a training function for this model; except for the model initialization, this is identical to the one we used to train the matrix factorization model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef19722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279f207c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e006f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2143f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in tqdm(DataLoader(train_dataset, batch_size=8)):\n",
    "    #print(images[0].shape, texts[0], labels[0] , labels.shape)\n",
    "    break\n",
    "display(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc75dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c67ee7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f7c287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26164c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for xx in x:\n",
    "#    display(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b9719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[{key:xx[key] for key in xx} for xx in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed314da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs, scheduler, loss_train_history, loss_val_history):    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        loss_accum = 0\n",
    "        t1 = time.time()\n",
    "        for i_step, (x, y) in enumerate(train_loader):\n",
    "            prediction = model(x)    \n",
    "            loss_value = loss(prediction, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()            \n",
    "            loss_accum += loss_value\n",
    "\n",
    "        ave_loss = loss_accum / (i_step + 1)\n",
    "        loss_val = compute_accuracy(model, val_loader, loss)\n",
    "        \n",
    "        loss_train_history.append(float(ave_loss))\n",
    "        loss_val_history.append(loss_val)\n",
    "        \n",
    "        if scheduler != None:\n",
    "            scheduler.step()\n",
    "        print(\"Epoch: %i lr: %f; Train loss: %f, Val loss: %f, time: %i s\" % (epoch, get_lr(optimizer), ave_loss, loss_val,\n",
    "                                                                            round(time.time() - t1)))\n",
    "    return loss_train_history, loss_val_history\n",
    "        \n",
    "    \n",
    "def compute_accuracy(model, loader, loss):\n",
    "    \"\"\"\n",
    "    Computes accuracy on the dataset wrapped in a loader    \n",
    "    Returns: accuracy as a float value between 0 and 1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss_accum = 0\n",
    "    for i_step, (x, y) in enumerate(loader):\n",
    "        prediction = model(x)\n",
    "        loss_value = loss(prediction, y)\n",
    "        loss_accum += loss_value\n",
    "\n",
    "    ave_loss = loss_accum / (i_step + 1)         \n",
    "    return float(ave_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d5c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BstTransformer(len(movie_lookup), len(user_lookup), sequence_length, embedding_size=120).to(device)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
    "\n",
    "loss_train_history = []\n",
    "loss_val_history = []\n",
    "train_model(\n",
    "    model, \n",
    "    DataLoader(train_dataset, batch_size=10000),\n",
    "    DataLoader(valid_dataset, batch_size=10000),\n",
    "    loss_func, optimizer, 30, scheduler, loss_train_history, loss_val_history)\n",
    "print('end!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c7384",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))    \n",
    "plt.xlabel(\"#iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_train_history, label='train loss')\n",
    "plt.plot(loss_val_history, label='val loss')\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893bae86",
   "metadata": {},
   "source": [
    "We can see that this is a significant improvement over the matrix factorization approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ae00c3",
   "metadata": {},
   "source": [
    "### Adding additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2fab57",
   "metadata": {},
   "source": [
    "So far, we have only considered the user ID and a sequence of movie IDs to predict the rating; it seems likely that including information about the previous ratings made by the user would improve performance. Thankfully, this is easy to do, and the data is already being returned by our dataset. Let's tweak our architecture to include this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4425d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BstTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        movies_num_unique,\n",
    "        users_num_unique,\n",
    "        sequence_length=10,\n",
    "        embedding_size=120,\n",
    "        num_transformer_layers=1,\n",
    "        ratings_range=(0.5, 5.5),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.y_range = ratings_range\n",
    "        self.movies_embeddings = nn.Embedding(\n",
    "            movies_num_unique + 1, embedding_size, padding_idx=0\n",
    "        )\n",
    "        self.user_embeddings = nn.Embedding(users_num_unique + 1, embedding_size)\n",
    "        self.ratings_embeddings = nn.Embedding(6, embedding_size, padding_idx=0)\n",
    "        self.position_embeddings = nn.Embedding(sequence_length, embedding_size)\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_size,\n",
    "                nhead=12,\n",
    "                dropout=0.1,\n",
    "                batch_first=True,\n",
    "                activation=\"gelu\",\n",
    "            ),\n",
    "            num_layers=num_transformer_layers,\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                embedding_size + (embedding_size * sequence_length),\n",
    "                1024,\n",
    "            ),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Mish(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        features, mask = inputs\n",
    "\n",
    "        encoded_user_id = self.user_embeddings(features[\"user_id\"])\n",
    "\n",
    "        user_features = encoded_user_id\n",
    "\n",
    "        movie_history = features[\"movie_ids\"][:, :-1]\n",
    "        target_movie = features[\"movie_ids\"][:, -1]\n",
    "\n",
    "        ratings = self.ratings_embeddings(features[\"ratings\"])\n",
    "\n",
    "        encoded_movies = self.movies_embeddings(movie_history)\n",
    "        encoded_target_movie = self.movies_embeddings(target_movie)\n",
    "\n",
    "        positions = torch.arange(\n",
    "            0,\n",
    "            self.sequence_length - 1,\n",
    "            1,\n",
    "            dtype=int,\n",
    "            device=features[\"movie_ids\"].device,\n",
    "        )\n",
    "        positions = self.position_embeddings(positions)\n",
    "\n",
    "        encoded_sequence_movies_with_position_and_rating = (\n",
    "            encoded_movies + ratings + positions\n",
    "        )\n",
    "        encoded_target_movie = encoded_target_movie.unsqueeze(1)\n",
    "\n",
    "        transformer_features = torch.cat(\n",
    "            (encoded_sequence_movies_with_position_and_rating, encoded_target_movie),\n",
    "            dim=1,\n",
    "        )\n",
    "        transformer_output = self.encoder(\n",
    "            transformer_features, src_key_padding_mask=mask\n",
    "        )\n",
    "        transformer_output = torch.flatten(transformer_output, start_dim=1)\n",
    "\n",
    "        combined_output = torch.cat((transformer_output, user_features), dim=1)\n",
    "\n",
    "        rating = self.linear(combined_output)\n",
    "        rating = rating.squeeze()\n",
    "        if self.y_range is None:\n",
    "            return rating\n",
    "        else:\n",
    "            return rating * (self.y_range[1] - self.y_range[0]) + self.y_range[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdcbf1a",
   "metadata": {},
   "source": [
    "We can see that, to use the ratings data, we have added an additional embedding layer. For each previously rated movie, we then add together the movie embedding, the positional encoding and the rating embedding before feeding this sequence into the transformer. Alternatively, the rating data could be concatenated to, or multiplied with, the movie embedding, but adding them together worked the best out of the approaches that I tried.\n",
    "\n",
    "As Jupyter maintains a live state for each class definition, we don't need to update our training function; the new class will be used when we launch training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd1f73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = BstTransformer(len(movie_lookup), len(user_lookup), sequence_length, embedding_size=120).to(device)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
    "\n",
    "\n",
    "loss_train_history = []\n",
    "loss_val_history = []\n",
    "train_model(\n",
    "    model, \n",
    "    DataLoader(train_dataset, batch_size=10000),\n",
    "    DataLoader(valid_dataset, batch_size=10000),\n",
    "    loss_func, optimizer, 30, scheduler, loss_train_history, loss_val_history)\n",
    "print('end!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9bd685",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))    \n",
    "plt.xlabel(\"#iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_train_history, label='train loss')\n",
    "plt.plot(loss_val_history, label='val loss')\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04257221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56896157",
   "metadata": {},
   "source": [
    "We can see that incorporating the ratings data has improved our results slightly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96290024",
   "metadata": {},
   "source": [
    "### Adding user features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61193109",
   "metadata": {},
   "source": [
    "In addition to the ratings data, we also have more information about the users that we could add into the model. To remind ourselves, let's take a look at the users table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52ba262",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630b10c1",
   "metadata": {},
   "source": [
    "Let's try adding in the categorical variables representing the users' sex, age groups, and occupation to the model, and see if we see any improvement. While occupation looks like it is already sequentially numerically encoded, we must do the same for the sex and age_group columns. We can use the 'LabelEncoder' class from scikit-learn to do this for us, and append the encoded columns to the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed11ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355472a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1fda7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "users['sex_encoded'] = le.fit_transform(users.sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bef9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "users['age_group_encoded'] = le.fit_transform(users.age_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39baba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"user_id\"] = users[\"user_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b684dd4b",
   "metadata": {},
   "source": [
    "Now that we have all the features that we are going to use encoded, let's join the user features to our sequences DataFrame, and update our training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e83f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_with_user_features = pd.merge(seq_df, users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd6c4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = seq_with_user_features[seq_with_user_features.is_valid == False]\n",
    "valid_df = seq_with_user_features[seq_with_user_features.is_valid == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8100ddbc",
   "metadata": {},
   "source": [
    "Let's update our dataset to include these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b08c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieSequenceDataset(Dataset):\n",
    "    def __init__(self, df, movie_lookup, user_lookup):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.movie_lookup = movie_lookup\n",
    "        self.user_lookup = user_lookup\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.df.iloc[index]\n",
    "        user_id = torch.tensor(self.user_lookup[str(data.user_id)]).to(device)\n",
    "        movie_ids = torch.tensor([self.movie_lookup[title] for title in data.title]).to(device)\n",
    "\n",
    "        previous_ratings = torch.tensor(\n",
    "            [rating if rating != \"[PAD]\" else 0 for rating in data.previous_ratings]\n",
    "        ).to(device)\n",
    "\n",
    "        attention_mask = torch.tensor(data.pad_mask).to(device)\n",
    "        target_rating = data.target_rating\n",
    "        encoded_features = {\n",
    "            \"user_id\": user_id,\n",
    "            \"movie_ids\": movie_ids,\n",
    "            \"ratings\": previous_ratings,\n",
    "            \"age_group\": torch.tensor(data[\"age_group_encoded\"]).to(device),\n",
    "            \"sex\": torch.tensor(data[\"sex_encoded\"]).to(device),\n",
    "            \"occupation\": torch.tensor(data[\"occupation\"]).to(device),\n",
    "        }\n",
    "\n",
    "        return (encoded_features, attention_mask), torch.tensor(\n",
    "            target_rating, dtype=torch.float32\n",
    "        ).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d48d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MovieSequenceDataset(train_df, movie_lookup, user_lookup)\n",
    "valid_dataset = MovieSequenceDataset(valid_df, movie_lookup, user_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d0a744",
   "metadata": {},
   "source": [
    "We can now modify our architecture to include embeddings for these features and concatenate these embeddings to the output of the transformer; then we pass this into the feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c7f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BstTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        movies_num_unique,\n",
    "        users_num_unique,\n",
    "        sequence_length=10,\n",
    "        embedding_size=120,\n",
    "        num_transformer_layers=1,\n",
    "        ratings_range=(0.5, 5.5),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.y_range = ratings_range\n",
    "        self.movies_embeddings = nn.Embedding(\n",
    "            movies_num_unique + 1, embedding_size, padding_idx=0\n",
    "        )\n",
    "        self.user_embeddings = nn.Embedding(users_num_unique + 1, embedding_size)\n",
    "        self.ratings_embeddings = nn.Embedding(6, embedding_size, padding_idx=0)\n",
    "        self.position_embeddings = nn.Embedding(sequence_length, embedding_size)\n",
    "\n",
    "        self.sex_embeddings = nn.Embedding(\n",
    "            3,\n",
    "            2,\n",
    "        )\n",
    "        self.occupation_embeddings = nn.Embedding(\n",
    "            22,\n",
    "            11,\n",
    "        )\n",
    "        self.age_group_embeddings = nn.Embedding(\n",
    "            8,\n",
    "            4,\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_size,\n",
    "                nhead=12,\n",
    "                dropout=0.1,\n",
    "                batch_first=True,\n",
    "                activation=\"gelu\",\n",
    "            ),\n",
    "            num_layers=num_transformer_layers,\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                embedding_size + (embedding_size * sequence_length) + 4 + 11 + 2,\n",
    "                1024,\n",
    "            ),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Mish(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        features, mask = inputs\n",
    "\n",
    "        user_id = self.user_embeddings(features[\"user_id\"])\n",
    "\n",
    "        age_group = self.age_group_embeddings(features[\"age_group\"])\n",
    "        sex = self.sex_embeddings(features[\"sex\"])\n",
    "        occupation = self.occupation_embeddings(features[\"occupation\"])\n",
    "\n",
    "        user_features = user_features = torch.cat(\n",
    "            (user_id, sex, age_group, occupation), 1\n",
    "        )\n",
    "\n",
    "        movie_history = features[\"movie_ids\"][:, :-1]\n",
    "        target_movie = features[\"movie_ids\"][:, -1]\n",
    "\n",
    "        ratings = self.ratings_embeddings(features[\"ratings\"])\n",
    "\n",
    "        encoded_movies = self.movies_embeddings(movie_history)\n",
    "        encoded_target_movie = self.movies_embeddings(target_movie)\n",
    "\n",
    "        positions = torch.arange(\n",
    "            0,\n",
    "            self.sequence_length - 1,\n",
    "            1,\n",
    "            dtype=int,\n",
    "            device=features[\"movie_ids\"].device,\n",
    "        )\n",
    "        positions = self.position_embeddings(positions)\n",
    "\n",
    "        encoded_sequence_movies_with_position_and_rating = (\n",
    "            encoded_movies + ratings + positions\n",
    "        )\n",
    "        encoded_target_movie = encoded_target_movie.unsqueeze(1)\n",
    "\n",
    "        transformer_features = torch.cat(\n",
    "            (encoded_sequence_movies_with_position_and_rating, encoded_target_movie),\n",
    "            dim=1,\n",
    "        )\n",
    "        transformer_output = self.encoder(\n",
    "            transformer_features, src_key_padding_mask=mask\n",
    "        )\n",
    "        transformer_output = torch.flatten(transformer_output, start_dim=1)\n",
    "\n",
    "        combined_output = torch.cat((transformer_output, user_features), dim=1)\n",
    "\n",
    "        rating = self.linear(combined_output)\n",
    "        rating = rating.squeeze()\n",
    "        if self.y_range is None:\n",
    "            return rating\n",
    "        else:\n",
    "            return rating * (self.y_range[1] - self.y_range[0]) + self.y_range[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3f4f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7abc27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = BstTransformer(len(movie_lookup), len(user_lookup), sequence_length, embedding_size=120).to(device)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
    "\n",
    "\n",
    "loss_train_history = []\n",
    "loss_val_history = []\n",
    "train_model(\n",
    "    model, \n",
    "    DataLoader(train_dataset, batch_size=10000),\n",
    "    DataLoader(valid_dataset, batch_size=10000),\n",
    "    loss_func, optimizer, 30, scheduler, loss_train_history, loss_val_history)\n",
    "print('end!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d737f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))    \n",
    "plt.xlabel(\"#iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_train_history, label='train loss')\n",
    "plt.plot(loss_val_history, label='val loss')\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e17bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cab3e0f1",
   "metadata": {},
   "source": [
    "Here, we can see a slight decrease in the MAE, but a small increase in the MSE and RMSE, so it looks like these features made a negligible difference to the overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fcbe21",
   "metadata": {},
   "source": [
    "In writing this article, my main objective has been to try and illustrate how these approaches can be used, and so I've picked the hyperparameters somewhat arbitrarily; it's likely that with some hyperparameter tweaks, and different combinations of features, these metrics can probably be improved upon!\n",
    "\n",
    "Hopefully this has provided a good introduction to using both matrix factorization and transformer-based approaches in PyTorch, and how pytorch-accelerated can speed up our process when experimenting with different models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a86dfa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Comparing matrix factorixation with transformers using pytorch-accelerated blog post.ipynb",
    "public": true
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
